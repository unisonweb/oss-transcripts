type AccessToken
  = AccessToken Text

type Blob.ETag
  = ETag Text

type Blob.Key
  = Key Text

type Blob.Metadata
  = Metadata Key ETag Nat Instant

ability Blobs where
  bytes.tryList.impl :
    Database
    -> Optional Nat
    -> Text
    -> Text
    -> Optional PageToken
    ->{Blobs} Either Failure PrefixListResults
  typed.tryList.impl :
    Database
    -> Optional Nat
    -> Text
    -> Text
    -> Optional PageToken
    ->{Blobs} Either Failure PrefixListResults
  typed.tryRead :
    Database -> Key ->{Blobs} Either Failure (Optional (a, Metadata))
  typed.tryCreate : Database -> Key -> a ->{Blobs} Either Failure ETag
  typed.tryWrite : Database -> Key -> a ->{Blobs} Either Failure ETag
  bytes.tryWriteStreaming :
    Database -> Key -> '{Remote, Stream Bytes} () ->{Blobs} Either Failure ETag
  bytes.tryCreateStreaming :
    Database -> Key -> '{Remote, Stream Bytes} () ->{Blobs} Either Failure ETag
  bytes.tryCreate : Database -> Key -> Bytes ->{Blobs} Either Failure ETag
  bytes.tryWrite : Database -> Key -> Bytes ->{Blobs} Either Failure ETag
  bytes.tryDelete : Database -> Key ->{Blobs} Either Failure ()
  typed.tryDelete : Database -> Key ->{Blobs} Either Failure ()
  bytes.tryReadMetadata :
    Database -> Key ->{Blobs} Either Failure (Optional Metadata)
  typed.tryReadMetadata :
    Database -> Key ->{Blobs} Either Failure (Optional Metadata)
  bytes.tryPrefixQuery :
    Database
    -> Optional Nat
    -> Text
    -> Optional PageToken
    ->{Blobs} Either Failure PrefixQueryResults
  typed.tryPrefixQuery :
    Database
    -> Optional Nat
    -> Text
    -> Optional PageToken
    ->{Blobs} Either Failure PrefixQueryResults
  bytes.tryRead :
    Database -> Key ->{Blobs} Either Failure (Optional (Bytes, Metadata))
  bytes.tryReadRange :
    Database
    -> Key
    -> Nat
    -> Nat
    ->{Blobs} Either Failure (Optional (Bytes, ETag))

type Blobs.PageToken
  = PageToken Text

type Blobs.PrefixListResult
  = BlobResult Metadata
  | PrefixResult Text

type Blobs.PrefixListResults
  = PrefixListResults (Optional PageToken) [PrefixListResult]

type Blobs.PrefixQueryResults
  = PrefixQueryResults (Optional PageToken) [Metadata]

ability Cloud where
  ServiceName.unassign.impl : ServiceName a b ->{Cloud} Either Failure ()
  ServiceName.delete.impl : ServiceName a b ->{Cloud} Either Failure ()
  Database.delete.impl : Database ->{Cloud} Either Failure ()
  Environment.create.impl : Text ->{Cloud} Either Failure Environment
  deploy.impl :
    Environment -> (a ->{Remote} b) ->{Cloud} Either Failure (ServiceHash a b)
  Daemon.create.impl : Text ->{Cloud} Either Failure Daemon
  unexposeHttpWebSocket.impl :
    ServiceHash
      HttpRequest
      (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
    ->{Cloud} Either Failure ()
  Database.create.impl : Text ->{Cloud} Either Failure Database
  Environment.deleteValue.impl :
    Environment -> Text ->{Cloud} Either Failure ()
  DaemonHash.delete.impl : DaemonHash ->{Cloud} Either Failure ()
  Environment.setValue.impl :
    Environment -> Text -> Text ->{Cloud} Either Failure ()
  undeploy.impl : ServiceHash a b ->{Cloud} Either Failure ()
  exposeHttp.impl :
    ServiceHash HttpRequest HttpResponse ->{Cloud} Either Failure URI
  DaemonHash.create.impl :
    Environment -> '{Remote} () ->{Cloud} Either Failure DaemonHash
  logs.impl : QueryOptions ->{Cloud} Either Failure [Json]
  unexposeHttp.impl :
    ServiceHash HttpRequest HttpResponse ->{Cloud} Either Failure ()
  Environment.delete.impl : Environment ->{Cloud} Either Failure ()
  ServiceName.assign.impl :
    ServiceName a b -> ServiceHash a b ->{Cloud} Either Failure URI
  Environment.list.impl : {Cloud} (Either Failure [Environment])
  submit.impl : Environment -> '{Remote} a ->{Cloud} Either Failure a
  Daemon.assign.impl : Daemon -> DaemonHash ->{Cloud} Either Failure ()
  Daemon.delete.impl : Daemon ->{Cloud} Either Failure ()
  Daemon.unassign.impl : Daemon ->{Cloud} Either Failure ()
  exposeHttpWebSocket.impl :
    ServiceHash
      HttpRequest
      (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
    ->{Cloud} Either Failure URI
  Database.assign.impl : Database -> Environment ->{Cloud} Either Failure ()
  Database.unassign.impl : Database -> Environment ->{Cloud} Either Failure ()
  ServiceName.create.impl : Text ->{Cloud} Either Failure (ServiceName a b)

type Cloud.ClientConfig
  = { host : HostName,
      httpPort : Port,
      tcpPort : Port,
      token : AccessToken,
      tlsConfig : Optional Tls.ClientConfig,
      httpConfig : client.Config }

type Cloud.logs.Direction
  = Forward
  | Backward

type Cloud.logs.QueryOptions
  = { search : Optional Text,
      limit : Optional Nat,
      start : Optional Instant,
      end : Optional Instant,
      direction : Optional Direction }

type Cloud.run.local.LocalCloudConfig
  = { printBanners : Boolean, httpHost : Optional HostName, httpPort : Port }

type Daemon
  = Daemon Daemon.Id Text

type Daemon.Id
  = Id Text

type DaemonHash
  = DaemonHash Text

type Database
  = Database Database.Id Text

type Database.Id
  = Database.Id.Id Text

type DatabaseInfo
  = { id : Database.Id, name : Text, environments : [Environment.Id] }

type DeploymentInfo
  = { hash : ServiceHash.Untyped,
      deployedAt : Instant,
      undeployedAt : Optional Instant,
      deployedBy : UserInfo,
      exposedAt : Optional Instant,
      unexposedAt : Optional Instant }

structural type durable.Cell a
  = Cell Database a (Table () a)

type durable.GinIndex input key id data
  = internal.GinIndex
      Database
      (id -> id -> Ordering)
      (input -> [key])
      (OrderedTable key (OrderedTable id data))

type durable.GinIndex.Query key
  = And (durable.GinIndex.Query key) (durable.GinIndex.Query key)
  | Or (durable.GinIndex.Query key) (durable.GinIndex.Query key)
  | Minus (durable.GinIndex.Query key) (durable.GinIndex.Query key)
  | Exact key

type durable.Immutable
  = Immutable Database (Table Hash (Any, Nat))

type durable.Knn m v a
  = Knn Database (v -> m) (m -> v -> m) (m -> v -> Float) (Kit m v a)

structural type durable.Knn.internal.Kit m v a
  = Kit Nat (Cell ([(m, durable.Knn.internal.Kit m v a)], [(v, a)], [(v, a)]))

type durable.Knn.SampleWords txt
  = SampleWords Nat [txt]

type durable.LinearLog a
  = LinearLog (Cell Nat) (Table Nat a)

type durable.OrderedTable k v
  = internal.BTree
      Database
      (k -> k -> Ordering)
      Nat
      (Table internal.Id (internal.Node k))
      (Table k v)

type durable.OrderedTable.internal.Id
  = Id Bytes
  | Root

type durable.OrderedTable.internal.Node k
  = { id : internal.Id,
      keys : [k],
      pointers : [internal.Id],
      isLeaf : Boolean }

ability durable.OrderedTable.internal.test.MapGeneration where
  durable.BTree.internal.test.MapGeneration.insert :
    Nat -> Bytes ->{durable.OrderedTable.internal.test.MapGeneration} ()
  durable.BTree.internal.test.MapGeneration.delete :
    Nat ->{durable.OrderedTable.internal.test.MapGeneration} ()

type durable.OrderedTable.internal.test.Sample
  = { storageReads : Nat,
      reads : Nat,
      writes : Nat,
      deletes : Nat,
      keys : Nat }

type durable.OrderedTable.internal.test.Sample.Percentiles
  = { sampleSize : Nat,
      min : Sample,
      p50 : Sample,
      p95 : Sample,
      p99 : Sample,
      max : Sample }

type Environment
  = Environment Environment.Id Text

ability Environment.Config where
  lookup : Text ->{Environment.Config} Optional Text

type Environment.Config.failure.MissingKey
  =

type Environment.Id
  = Environment.Id.Id Text

type errors.Conflict
  =

type errors.CredentialsError
  =

type errors.ProtocolError
  =

type errors.SerializationError
  =

type errors.UnknownDaemon
  =

type errors.UnknownService
  =

type errors.UnknownTerm
  =

type errors.UnknownWebSocket
  =

type internal.CloudRequest
  = ForkRequest AccessToken Text Thunk
  | TermReply [(Link.Term, Bytes)]

type internal.CloudResponse
  = FailureReply Text
  | TermRequest [Link.Term]
  | TaskResult (Either Failure Bytes)
  | JobStarted JobId

type internal.Thunk
  = Thunk Bytes

type internal.XDG.MissingEnvVar
  =

type JobId
  = JobId Text

ability Log where
  debugLazyJson : 'Json ->{Log} ()
  lazyJson : 'Json ->{Log} ()

type Log.Level
  = Info
  | Warn
  | Error
  | Debug
  | Custom Text

ability Remote where
  Promise.tryWrite : Remote.Promise a -> a ->{Remote} Either Failure Boolean
  Ref.tryDelete : Remote.Ref a ->{Remote} Either Failure ()
  fail : Failure ->{Remote} x
  Promise.empty.detached! : {Remote} (Remote.Promise a)
  here! : {Remote} (Location {})
  region! : {Remote} (Location {})
  addFinalizer : (Outcome ->{Remote} ()) ->{Remote} ()
  Ref.new.detached : a ->{Remote} Remote.Ref a
  Promise.tryRead : Remote.Promise a ->{Remote} Either Failure a
  sleepMicroseconds : Nat ->{Remote} ()
  Ref.tryWrite : Remote.Ref a -> a ->{Remote} Either Failure ()
  tryScope : '{Remote} a ->{Remote} Either Failure a
  tryNear : Location g -> Location g2 ->{Remote} Either Failure (Location g)
  tryFar : Location g -> Location g2 ->{Remote} Either Failure (Location g)
  allowCancel! : {Remote} ()
  Ref.tryCas :
    Remote.Ref a -> Remote.Ref.Ticket a -> a ->{Remote} Either Failure Boolean
  time.monotonic! : {Remote} time.Duration
  tryCancel : Thread ->{Remote} Either Failure ()
  Promise.tryDelete : Remote.Promise a ->{Remote} Either Failure ()
  time.now! : {Remote} Instant
  Ref.tryReadForCas :
    Remote.Ref a ->{Remote} Either Failure (Remote.Ref.Ticket a, a)
  Promise.tryReadNow : Remote.Promise a ->{Remote} Either Failure (Optional a)
  tryDetachAt :
    Location g -> '{g, Exception, Remote} a ->{Remote} Either Failure Thread
  randomBytes : Nat ->{Remote} Bytes

type Remote.Duration
  = Duration Nat

type Remote.failure.Cancelled
  =

type Remote.failure.DeserializationError
  =

type Remote.failure.FailedToLoadTerms
  =

type Remote.failure.NoAvailableLocations
  =

type Remote.failure.PromiseDeleted
  =

type Remote.failure.RestrictedOperation
  =

type Remote.failure.Timeout
  =

type Remote.failure.UnknownHash
  =

type Remote.failure.UnknownLocation
  =

type Remote.failure.UnknownPromise
  =

type Remote.failure.UnknownRef
  =

type Remote.failure.UnknownTask
  =

type Remote.failure.ValueTooLarge
  =

type Remote.Location g
  = Near Location.Id Location.Id Location.Id [Text]
  | Far Location.Id Location.Id Location.Id [Text]
  | Location Location.Id [Text]

type Remote.Location.Id
  = LocationId UID

type Remote.Outcome
  = Completed
  | Cancelled
  | Failed Failure

type Remote.Promise a
  = Promise Promise.Id Location.Id

type Remote.Promise.Id
  = Id UID

type Remote.Ref a
  = Ref Ref.Id Location.Id

type Remote.Ref.Id
  = Id UID

type Remote.Ref.Ticket a
  = Ticket Nat

type Remote.run.Interrupt
  = Interrupt (concurrent.Promise ()) (concurrent.Promise ())

type Remote.run.pure.ThreadState s
  = ThreadState
      (ThreadStatus s)
      (Remote.Promise (Optional (Either Failure Any)))
      [Outcome ->{Remote, Scope s} ()]

type Remote.run.pure.ThreadStatus s
  = Scoped Thread.Id
  | Running
  | CancellationRequested
  | Blocking ('{Scope s} ())

type Remote.Task a
  = Task Thread (Remote.Promise (Either Failure a))

type Remote.test.assertions.Test a
  = Test Text ('{Remote} a) (Either Failure a ->{Exception} ())

ability Remote.test.TestLog where log : Text ->{Remote.test.TestLog} ()

type Remote.Thread
  = Thread Thread.Id Location.Id

type Remote.Thread.Id
  = Id UID

type Remote.UID
  = UID Bytes

structural type Remote.Value a
  = Value (∀ r. (a ->{Remote} r) ->{Remote} r)

type Remote.Value.memo.impl.MemoTagged a
  = MemoTagged a

type Remote.Value.memo.impl.SaveTagged a
  = SaveTagged a

ability Scratch where
  hashKey : k a ->{Scratch} Hashed a
  touch : Hashed a ->{Scratch} ()
  lookupHashed : Hashed a ->{Scratch} Optional a
  exists : Hashed a ->{Scratch} Boolean
  saveHashed : Hashed a -> a ->{Scratch} ()

type Scratch.Hashed a
  = Hashed Hash

type Scratch.Hashed.Hash
  = Hash Bytes

type ServiceAssignment
  = { hash : ServiceHash.Untyped,
      deployedBy : UserInfo,
      deployedAt : Instant,
      undeployedAt : Optional Instant,
      exposedAt : Optional Instant,
      unexposedAt : Optional Instant,
      tags : [Text],
      assignedBy : UserInfo,
      assignedAt : Instant,
      historyAt : Optional Instant }

type ServiceHash a b
  = ServiceHash Text

type ServiceHash.Untyped
  = Untyped Text

type ServiceName a b
  = ServiceName ServiceName.Id Text

type ServiceName.Id
  = Id Text

type ServiceName.Untyped
  = Untyped ServiceName.Id Text

type ServiceNameInfo
  = { service : ServiceName.Untyped,
      currentDeployment : Optional DeploymentInfo,
      owner : UserInfo }

ability Services where
  tryResolve : ServiceName a b ->{Services} Either Failure (ServiceHash a b)
  tryCallByName : ServiceName a b -> a ->{Services} Either Failure b
  tryCall : ServiceHash a b -> a ->{Services} Either Failure b

ability Storage where
  tryTransact :
    Database
    -> '{Transaction, Exception, Random, Batch} a
    ->{Storage} Either Failure a
  tryBatchRead : Database -> '{Exception, Batch} a ->{Storage} Either Failure a

ability Storage.Batch where
  forkRead : Table k v -> k ->{Storage.Batch} Read v
  tryAwaitRead : Read v ->{Storage.Batch} Optional v

type Storage.Batch.Read a
  = Read Read.Id Text Any

type Storage.Batch.Read.Id
  = Id Bytes

type Storage.Table k v
  = Table Text

ability Storage.Transaction where
  write.tx : Table k v -> k -> v ->{Storage.Transaction} ()
  tryRead.tx : Table k v -> k ->{Storage.Transaction} Optional v
  delete.tx : Table k v -> k ->{Storage.Transaction} ()

type Storage.Transaction.AbortTransaction
  =

type UserInfo
  = { avatarUrl : URI, email : Text, handl : Text, id : Text }

ability websockets.HttpWebSocket where
  tryWebSocket :
    HttpRequest
    ->{websockets.HttpWebSocket} Either Failure websockets.WebSocket

type websockets.WebSocket
  = WebSocket Location.Id WebSocket.Id

type websockets.WebSocket.Id
  = Id Bytes

ability websockets.WebSockets where
  tryReceive :
    websockets.WebSocket ->{websockets.WebSockets} Either Failure Message
  tryClose : websockets.WebSocket ->{websockets.WebSockets} Either Failure ()
  trySend :
    websockets.WebSocket -> Message ->{websockets.WebSockets} Either Failure ()

AccessToken.doc : Doc
AccessToken.doc =
  {{
  An access token is a string that is used to authenticate a user to the cloud.
  It is used to identify the user and to authorize them to perform certain
  actions. The access token is typically obtained automatically from a file
  called `credentials.json` that is created automatically when a user
  authenticates with the cloud using the `auth.login` command in the Unison
  Codebase Manager.

  # See also

    * {ClientConfig.token} gets the access token from a
      {type Cloud.ClientConfig}, see e.g. {Cloud.ClientConfig.default}.
  }}

Blob.ETag.doc : Doc
Blob.ETag.doc =
  {{
  An entity tag, as defined by the
  [HTTP ETag](https://en.wikipedia.org/wiki/HTTP_ETag) specification.

  An {type ETag} identifies a specific version of a blob, based on its content.
  If the value in the blob changes, the {etag} will change. This can be useful
  for caching purposes.
  }}

Blob.Key.doc : Doc
Blob.Key.doc =
  {{
  A unique (within a
  [namespace]({{ docLink (docEmbedTermLink do blobNamespaces) }})) identifier
  for a value in [blob]({type Blobs}) storage.
  }}

Blob.Key.toText : Key -> Text
Blob.Key.toText = cases Key t -> t

Blob.Metadata.byteCount : Metadata -> Nat
Blob.Metadata.byteCount = cases Metadata _ _ c _ -> c

Blob.Metadata.doc : Doc
Blob.Metadata.doc =
  {{
  Metadata about a stored blob, including:

  * {{ docLink (docEmbedTermLink do key) }}: the blob's {type Key}
  * {{ docLink (docEmbedTermLink do etag) }}: the blob's {type ETag}
  * {{ docLink (docEmbedTermLink do byteCount) }}: the size of the blob in
    bytes
  * {{ docLink (docEmbedTermLink do lastModified) }}: the time that the blob
    was last modified
  }}

Blob.Metadata.etag : Metadata -> ETag
Blob.Metadata.etag = cases Metadata _ v _ _ -> v

Blob.Metadata.key : Metadata -> Key
Blob.Metadata.key = cases Metadata k _ _ _ -> k

Blob.Metadata.lastModified : Metadata -> Instant
Blob.Metadata.lastModified = cases Metadata _ _ _ t -> t

Blobs.bytes.create : Database -> Key -> Bytes ->{Exception, Blobs} ETag
Blobs.bytes.create store key content =
  Either.toException (bytes.tryCreate store key content)

Blobs.bytes.create.doc : Doc
Blobs.bytes.create.doc =
  {{
  `` bytes.create database key content `` stores the bytes `content` at the
  specified {type Key}. If a blob already exists with the provided key, then an
  {type Exception} is raised.

  {{
  docCallout
    (Some {{ 🗜️ }})
    {{
    {bytes.create} does not apply any compression. If your content is not
    already compressed, you may want to run it through a compression algorithm
    such as {zlib.compress} before storing it. Just remember to
    {zlib.decompress} when you {bytes.read} it!
    }} }}

  # Related

    * {bytes.write} will replace the content if a blob already exists at the
      specified key.
    * {typed.create} creates a blob with an arbitrary value (as opposed to
      {type Bytes}) to the `typed` [namespace]({blobNamespaces}).
  }}

Blobs.bytes.createStreaming :
  Database -> Key -> '{Remote, Stream Bytes} () ->{Exception, Blobs} ETag
Blobs.bytes.createStreaming store key stream =
  Either.toException (tryCreateStreaming store key stream)

Blobs.bytes.delete : Database -> Key ->{Exception, Blobs} ()
Blobs.bytes.delete store key = Either.toException (bytes.tryDelete store key)

Blobs.bytes.delete.doc : Doc
Blobs.bytes.delete.doc =
  {{
  `` bytes.delete database key `` deletes the bytes blob at `key`.

  If there isn't a bytes blob with the specified key, this returns `()`. An
  {type Exception} is only raised if there is an unexpected server/network
  error.

  # Related

    * {bytes.tryDelete} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {typed.delete} deletes a typed blob (as opposed to a bytes blob).
  }}

Blobs.bytes.list :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Exception, Blobs} PrefixListResults
Blobs.bytes.list store maxResults prefix pageToken =
  Either.toException (bytes.tryList store maxResults prefix pageToken)

Blobs.bytes.list.doc : Doc
Blobs.bytes.list.doc =
  {{
  {{
  docExample 4 do
    database maxResults prefix pageToken ->
      bytes.list database maxResults prefix pageToken }} lists a single layer
  of the key hierarchy in `database`, starting with `prefix` and assuming `/`
  as a delimiter. It is similar to running `ls prefix` in a Unix shell.

  {{
  docCallout
    (Some {{ 🗃 }})
    {{
    This only returns results from the `bytes` [namespace]({blobNamespaces}).
    See {{ (docLink (docEmbedTermLink do typed.list)) }} to list results in the
    `typed` namespace.
    }} }}

  {{ blobListExample }}

  # Related

    * {{ docLink (docEmbedTermLink do typed.list) }} lists results in the
      `typed` namespace (as opposed to the `bytes` namespace).
    * {{ docLink (docEmbedTermLink do bytes.tryList) }} returns {type Either}
      instead of raising an {type Exception} if there is an error.
    * {bytes.tryList.impl} allows specifying a delimiter other than `/`.
  }}

Blobs.bytes.prefixQuery :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Exception, Blobs} PrefixQueryResults
Blobs.bytes.prefixQuery store maxResults prefix pageToken =
  Either.toException (bytes.tryPrefixQuery store maxResults prefix pageToken)

Blobs.bytes.prefixQuery.doc : Doc
Blobs.bytes.prefixQuery.doc =
  {{
  `` bytes.prefixQuery database maxResults prefix pageToken `` returns the
  metadata for blobs whose {type Key} starts with `prefix`.

  {{
  docCallout
    (Some {{ 🗃 }})
    {{
    This only returns results from the `bytes` [namespace]({blobNamespaces}).
    See {typed.prefixQuery} to list results in the `typed` namespace.
    }} }}

  {{ blobPrefixQueryExample }}

  # Related

    * {typed.prefixQuery} lists results in the `typed` namespace (as opposed to
      the `bytes` namespace).
    * {bytes.tryPrefixQuery} returns {type Either} instead of raising an
      {type Exception} if there is an error.
  }}

Blobs.bytes.read :
  Database -> Key ->{Exception, Blobs} Optional (Bytes, Metadata)
Blobs.bytes.read store key = Either.toException (bytes.tryRead store key)

Blobs.bytes.read.doc : Doc
Blobs.bytes.read.doc =
  {{
  `` bytes.read database key `` looks up the bytes blob at `key` and returns it
  along with its metadata (or returns {None} if the key doesn't exist).

  # Related

    * {bytes.tryRead} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {bytes.readMetadata} returns only the metadata and not the blob content
    * {typed.read} reads a typed blob (as opposed to a bytes blob).
    * {readRange} reads a range of a bytes blob (as opposed to the entire
      blob).
  }}

Blobs.bytes.readMetadata :
  Database -> Key ->{Exception, Blobs} Optional Metadata
Blobs.bytes.readMetadata store key =
  Either.toException (bytes.tryReadMetadata store key)

Blobs.bytes.readMetadata.doc : Doc
Blobs.bytes.readMetadata.doc =
  {{
  `` bytes.readMetadata database key `` looks up the bytes blob at `key` and
  returns metadata (or returns {None} if the key doesn't exist).

  # Related

    * {bytes.tryReadMetadata} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {bytes.read} reads both the metadata and the blob contents
  }}

Blobs.bytes.readRange :
  Database -> Key -> Nat -> Nat ->{Exception, Blobs} Optional (Bytes, ETag)
Blobs.bytes.readRange store key startInclusive endInclusive =
  Either.toException (tryReadRange store key startInclusive endInclusive)

Blobs.bytes.tryList :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Blobs} Either Failure PrefixListResults
Blobs.bytes.tryList store maxResults prefix pageToken =
  bytes.tryList.impl store maxResults "/" prefix pageToken

Blobs.bytes.tryList.doc : Doc
Blobs.bytes.tryList.doc =
  {{
  A variant of {{ docLink (docEmbedTermLink do bytes.list) }} that returns
  {type Either} instead of raising an {type Exception} in the case of an error.
  }}

Blobs.bytes.write : Database -> Key -> Bytes ->{Exception, Blobs} ETag
Blobs.bytes.write store key content =
  Either.toException (bytes.tryWrite store key content)

Blobs.bytes.write.doc : Doc
Blobs.bytes.write.doc =
  {{
  `` bytes.write database key content `` stores the bytes `content` at the
  specified {type Key}. If a blob already exists with the provided key, then
  its contents will be replaced.

  {{
  docCallout
    (Some {{ 🗜️ }})
    {{
    {bytes.write} does not apply any compression. If your content is not
    already compressed, you may want to run it through a compression algorithm
    such as {zlib.compress} before storing it. Just remember to
    {zlib.decompress} when you {bytes.read} it!
    }} }}

  # Related

    * {bytes.create} will fail if a blob already exists at the specified key.
    * {typed.write} writes an arbitrary value (as opposed to {type Bytes}) to
      the `typed` [namespace]({blobNamespaces}).
  }}

Blobs.bytes.writeStreaming :
  Database -> Key -> '{Remote, Stream Bytes} () ->{Exception, Blobs} ETag
Blobs.bytes.writeStreaming store key stream =
  Either.toException (tryWriteStreaming store key stream)

Blobs.doc : Doc
Blobs.doc =
  {{
  An API for unstructured object storage. This exposes operations for Binary
  Large OBject Storage (blobs) similar to [AWS S3](https://aws.amazon.com/s3/),
  [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs),
  or [MinIO](https://min.io/) but also supports typed value storage so users
  don't have to convert to/from {type Bytes} representations.

  # Supported Operations

    {{
    Doc.Table
      [ [{{ **Operation** }}, {{ **bytes** }}, {{ **typed** }}]
      , [ {{
          Write blob
          }}
        , {{
          {bytes.write}, {writeStreaming}
          }}
        , {{
          {typed.write}
          }}
        ]
      , [ {{
          Create blob (fail if it exists)
          }}
        , {{
          {bytes.create}, {createStreaming}
          }}
        , {{
          {typed.create}
          }}
        ]
      , [{{ Read blob }}, {{ {bytes.read}, {readRange} }}, {{ {typed.read} }}]
      , [ {{
          Read blob metadata
          }}
        , {{
          {bytes.readMetadata}
          }}
        , {{
          {typed.readMetadata}
          }}
        ]
      , [{{ Delete blob }}, {{ {bytes.delete} }}, {{ {typed.delete} }}]
      , [ {{
          Query blobs by prefix
          }}
        , {{
          {bytes.prefixQuery}
          }}
        , {{
          {typed.prefixQuery}
          }}
        ]
      , [ {{
          List blob hierarchy ( `ls` )
          }}
        , {{
          {bytes.list}
          }}
        , {{
          {typed.list}
          }}
        ]
      ] }}

  # Namespaces

    {{ docCallout (Some {{ 🗄 }}) blobNamespaces }}
  }}

Blobs.PageToken.doc : Doc
Blobs.PageToken.doc =
  {{ A token used to paginate through blob query/list results. }}

Blobs.PageToken.toText : PageToken -> Text
Blobs.PageToken.toText = cases PageToken t -> t

Blobs.PrefixListResults.nextPageToken : PrefixListResults -> Optional PageToken
Blobs.PrefixListResults.nextPageToken = cases PrefixListResults t _ -> t

Blobs.PrefixListResults.results : PrefixListResults -> [PrefixListResult]
Blobs.PrefixListResults.results = cases PrefixListResults _ l -> l

Blobs.PrefixQueryResult.blobs : PrefixQueryResults -> [Metadata]
Blobs.PrefixQueryResult.blobs = cases PrefixQueryResults _ l -> l

Blobs.PrefixQueryResult.nextPageToken :
  PrefixQueryResults -> Optional PageToken
Blobs.PrefixQueryResult.nextPageToken = cases PrefixQueryResults t _ -> t

Blobs.typed.create : Database -> Key -> a ->{Exception, Blobs} ETag
Blobs.typed.create store key content =
  Either.toException (typed.tryCreate store key content)

Blobs.typed.create.doc : Doc
Blobs.typed.create.doc =
  {{
  `` typed.create database key value `` stores the provided value at the
  specified {type Key}. If a blob already exists with the provided key, then an
  {type Exception} is raised.

  # Related

    * {typed.write} will replace the content if a blob already exists at the
      specified key.
    * {bytes.create} creates a blob with {type Bytes} content (as opposed to an
      arbitrary value) to the `bytes` [namespace]({blobNamespaces}).
  }}

Blobs.typed.delete : Database -> Key ->{Exception, Blobs} ()
Blobs.typed.delete store key = Either.toException (typed.tryDelete store key)

Blobs.typed.delete.doc : Doc
Blobs.typed.delete.doc =
  {{
  `` typed.delete database key `` deletes the typed blob at `key`.

  If there isn't a typed blob with the specified key, this returns `()`. An
  {type Exception} is only raised if there is an unexpected server/network
  error.

  # Related

    * {typed.tryDelete} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {bytes.delete} deletes a bytes blob (as opposed to a typed blob).
  }}

Blobs.typed.list :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Exception, Blobs} PrefixListResults
Blobs.typed.list store maxResults prefix pageToken =
  Either.toException (typed.tryList store maxResults prefix pageToken)

Blobs.typed.list.doc : Doc
Blobs.typed.list.doc =
  {{
  {{
  docExample 4 do
    database maxResults prefix pageToken ->
      typed.list database maxResults prefix pageToken }} lists a single layer
  of the key hierarchy in `database`, starting with `prefix` and assuming `/`
  as a delimiter. It is similar to running `ls prefix` in a Unix shell.

  {{
  docCallout
    (Some {{ 🗃 }})
    {{
    This only returns results from the `typed` [namespace]({blobNamespaces}).
    See {{ (docLink (docEmbedTermLink do bytes.list)) }} to list results in the
    `bytes` namespace.
    }} }}

  {{ blobListExample }}

  # Related

    * {{ docLink (docEmbedTermLink do bytes.list) }} lists results in the
      `bytes` namespace (as opposed to the `typed` namespace).
    * {{ docLink (docEmbedTermLink do typed.tryList) }} returns {type Either}
      instead of raising an {type Exception} if there is an error.
    * {typed.tryList.impl} allows specifying a delimiter other than `/`.
  }}

Blobs.typed.prefixQuery :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Exception, Blobs} PrefixQueryResults
Blobs.typed.prefixQuery store maxResults prefix pageToken =
  Either.toException (typed.tryPrefixQuery store maxResults prefix pageToken)

Blobs.typed.prefixQuery.doc : Doc
Blobs.typed.prefixQuery.doc =
  {{
  `` typed.prefixQuery database maxResults prefix pageToken `` returns the
  metadata for blobs whose {type Key} starts with `prefix`.

  {{
  docCallout
    (Some {{ 🗃 }})
    {{
    This only returns results from the `typed` [namespace]({blobNamespaces}).
    See {bytes.prefixQuery} to list results in the `bytes` namespace.
    }} }}

  {{ blobPrefixQueryExample }}

  # Related

    * {bytes.prefixQuery} lists results in the `bytes` namespace (as opposed to
      the `typed` namespace).
    * {typed.tryPrefixQuery} returns {type Either} instead of raising an
      {type Exception} if there is an error.
  }}

Blobs.typed.read : Database -> Key ->{Exception, Blobs} Optional (a, Metadata)
Blobs.typed.read store key = Either.toException (typed.tryRead store key)

Blobs.typed.read.doc : Doc
Blobs.typed.read.doc =
  {{
  `` typed.read database key `` looks up the typed blob at `key` and returns it
  along with its metadata (or returns {None} if the key doesn't exist).

  # Related

    * {typed.tryRead} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {typed.readMetadata} returns only the metadata and not the blob content
    * {bytes.read} reads a bytes blob (as opposed to a typed blob).
  }}

Blobs.typed.readMetadata :
  Database -> Key ->{Exception, Blobs} Optional Metadata
Blobs.typed.readMetadata store key =
  Either.toException (typed.tryReadMetadata store key)

Blobs.typed.readMetadata.doc : Doc
Blobs.typed.readMetadata.doc =
  {{
  `` typed.readMetadata database key `` looks up the typed blob at `key` and
  returns metadata (or returns {None} if the key doesn't exist).

  # Related

    * {typed.tryReadMetadata} returns {type Either} instead of raising an
      {type Exception} if there is an error.
    * {typed.read} reads both the metadata and the blob contents
  }}

Blobs.typed.tryList :
  Database
  -> Optional Nat
  -> Text
  -> Optional PageToken
  ->{Blobs} Either Failure PrefixListResults
Blobs.typed.tryList store maxResults prefix pageToken =
  typed.tryList.impl store maxResults "/" prefix pageToken

Blobs.typed.tryList.doc : Doc
Blobs.typed.tryList.doc =
  {{
  A variant of {{ docLink (docEmbedTermLink do typed.list) }} that returns
  {type Either} instead of raising an {type Exception} in the case of an error.
  }}

Blobs.typed.write : Database -> Key -> a ->{Exception, Blobs} ETag
Blobs.typed.write store key content =
  Either.toException (typed.tryWrite store key content)

Blobs.typed.write.doc : Doc
Blobs.typed.write.doc =
  {{
  `` typed.write database key value `` stores the provided value at the
  specified {type Key}. If a blob already exists with the provided key, then
  its contents will be replaced.

  # Related

    * {typed.create} will fail if a blob already exists at the specified key.
    * {bytes.write} writes raw {type Bytes} (as opposed to an arbitrary value)
      to the `bytes` [namespace]({blobNamespaces}).
  }}

Cloud.ClientConfig.authority : Cloud.ClientConfig -> Authority
Cloud.ClientConfig.authority config =
  Authority
    None (ClientConfig.host config) (Some (ClientConfig.httpPort config))

Cloud.ClientConfig.authority.doc : Doc
Cloud.ClientConfig.authority.doc =
  {{
  Gets the {type Authority} for the client configuration, which is the hostname
  and port number of the Unison Cloud service.
  }}

Cloud.ClientConfig.default : '{IO, Exception} Cloud.ClientConfig
Cloud.ClientConfig.default = do
  use getEnv impl
  host = defaultUnisonCloudHost()
  tlsConfig = match impl "UNISON_CLOUD_USE_TLS" with
    Right "false" -> None
    _             -> Some (defaultTlsConfig host)
  httpPort = defaultUnisonCloudHttpPort tlsConfig
  tcpPort = defaultUnisonCloudPort tlsConfig
  accessToken = match impl "UNISON_CLOUD_ACCESS_TOKEN" with
    Right t -> AccessToken t
    Left _  ->
      credentialsHost = match impl "UNISON_CLOUD_CREDENTIALS_HOST" with
        Right host -> HostName host
        Left _     -> HostName "api.unison-lang.org"
      accessTokenFromCredentialsFile credentialsHost
  ClientConfig host httpPort tcpPort accessToken tlsConfig Config.default

Cloud.ClientConfig.default.doc : Doc
Cloud.ClientConfig.default.doc =
  {{
  The default {type Cloud.ClientConfig} for the Unison Cloud client. The
  default configuration is suitable for most use cases. You can customize the
  default configuration by setting the fields to different values. For example,
  to customize the host and token, you can do:

  @typecheck ```
  config =
    Cloud.ClientConfig.default()
      |> ClientConfig.host.set (HostName "example.com")
      |> token.set (AccessToken "my-token")
  ```
  }}

Cloud.ClientConfig.defaultHttp.doc : Doc
Cloud.ClientConfig.defaultHttp.doc =
  {{ Returns the default client configuration for the Unison Cloud HTTP API. }}

Cloud.ClientConfig.defaults.defaultTlsConfig : HostName -> Tls.ClientConfig
Cloud.ClientConfig.defaults.defaultTlsConfig hostName =
  Tls.ClientConfig.default hostName ""

Cloud.ClientConfig.defaults.defaultTlsConfig.doc : Doc
Cloud.ClientConfig.defaults.defaultTlsConfig.doc =
  {{
  Gets a default {type Tls.ClientConfig} for the given {type HostName},
  suitable for most use cases.
  }}

Cloud.ClientConfig.defaults.defaultUnisonCloudHost : '{IO, Exception} HostName
Cloud.ClientConfig.defaults.defaultUnisonCloudHost =
  do match getEnv.impl "UNISON_CLOUD_HOST" with
    Right h -> HostName h
    Left _  -> HostName "api.unison.cloud"

Cloud.ClientConfig.defaults.defaultUnisonCloudHost.doc : Doc
Cloud.ClientConfig.defaults.defaultUnisonCloudHost.doc =
  {{
  Gets the default hostname for the Unison Cloud service. This is
  ``"api.unison.cloud"``, unless overridden by the environment variable
  `UNISON_CLOUD_HOST`.
  }}

Cloud.ClientConfig.defaults.defaultUnisonCloudHttpPort :
  Optional Tls.ClientConfig ->{IO, Exception} Port
Cloud.ClientConfig.defaults.defaultUnisonCloudHttpPort tls =
  match getEnv.impl "UNISON_CLOUD_HTTP_PORT" with
    Right p -> Port p
    Left _  ->
      match tls with
        None   -> Port "80"
        Some _ -> Port "443"

Cloud.ClientConfig.defaults.defaultUnisonCloudHttpPort.doc : Doc
Cloud.ClientConfig.defaults.defaultUnisonCloudHttpPort.doc =
  {{
  Gets the default HTTP port number for the Unison Cloud service given the
  {type Tls.ClientConfig}. Unless overridden by the environment variable
  `UNISON_CLOUD_HTTP_PORT`, this is port 443 if the TLS configuration is
  present, and port 80 otherwise.
  }}

Cloud.ClientConfig.defaults.defaultUnisonCloudPort :
  Optional Tls.ClientConfig ->{IO} Port
Cloud.ClientConfig.defaults.defaultUnisonCloudPort tls =
  match getEnv.impl "UNISON_CLOUD_TCP_PORT" with
    Right p -> Port p
    Left _  ->
      match tls with
        None   -> Port "29180"
        Some _ -> Port "29183"

Cloud.ClientConfig.defaults.defaultUnisonCloudPort.doc : Doc
Cloud.ClientConfig.defaults.defaultUnisonCloudPort.doc =
  {{
  Gets the default port number for the Unison Cloud service given the
  {type Tls.ClientConfig}. Unless overridden by the environment variable
  `UNISON_CLOUD_TCP_PORT`, this is port 29183 if the TLS configuration is
  present, and port 29180 otherwise.
  }}

Cloud.ClientConfig.doc : Doc
Cloud.ClientConfig.doc =
  use Cloud.ClientConfig default
  {{
  The configuration for the Unison Cloud client.

  # Creating a configuration

    For most use cases, you should not need to construct a configuration
    manually. {Cloud.run} and {Cloud.main} use the default configuration, which
    is sufficient for most use cases. However, you can customize the default
    configuration by setting the fields to different values. For example, to
    customize the host and token, you can do:

    @typecheck ```
    config =
      default()
        |> ClientConfig.host.set (HostName "example.com")
        |> token.set (AccessToken "my-token")
    ```

    Note that {default} is a thunk, so you need to force it with `!` to
    actually construct the default configuration.

    The configuration includes the following fields:

    * `host` - The hostname of the Unison Cloud service. A {type HostName}.
    * `httpPort` - The port number for HTTP connections to the Cloud service. A
      {type Port}.
    * `tcpPort` - The port number for TCP connections to the Cloud service. A
      {type Port}.
    * `token` - The access token to use for the Unison Cloud service. An
      {type AccessToken}.
    * `tlsConfig` - The TLS configuration for the client. An optional
      {type Tls.ClientConfig}.
    * `httpConfig` - The HTTP configuration for the client. An
      {type client.Config}.
  }}

Cloud.ClientConfig.host : Cloud.ClientConfig -> HostName
Cloud.ClientConfig.host = cases ClientConfig host _ _ _ _ _ -> host

Cloud.ClientConfig.host.doc : Doc
Cloud.ClientConfig.host.doc =
  {{
  Gets the {type HostName} of the Unison Cloud service from the client
  configuration.
  }}

Cloud.ClientConfig.host.modify :
  (HostName ->{g} HostName) -> Cloud.ClientConfig ->{g} Cloud.ClientConfig
Cloud.ClientConfig.host.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig (f host) httpPort tcpPort token tlsConfig httpConfig

Cloud.ClientConfig.host.modify.doc : Doc
Cloud.ClientConfig.host.modify.doc =
  {{
  Modifies the host name field of the {type Cloud.ClientConfig} using the given
  function, returning a new configuration with the updated {type HostName}.

  The value of this field is used to determine the hostname of the Unison Cloud
  service to connect to.

  # See also

    * {ClientConfig.host.set} - Sets the value of this field directly.
    * {defaultUnisonCloudHost} - Determines the default value of this field.
  }}

Cloud.ClientConfig.host.set :
  HostName -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.host.set host1 = cases
  ClientConfig _ httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host1 httpPort tcpPort token tlsConfig httpConfig

Cloud.ClientConfig.host.set.doc : Doc
Cloud.ClientConfig.host.set.doc =
  {{
  Sets the host name field of the {type Cloud.ClientConfig} to the given
  {type HostName}, returning a new configuration with the updated value.

  The value of this field is used to determine the hostname of the Unison Cloud
  service to connect to.

  # See also

    * {ClientConfig.host.modify} - Modifies the value of this field using a
      function.
    * {defaultUnisonCloudHost} - Determines the default value of this field.
  }}

Cloud.ClientConfig.httpConfig : Cloud.ClientConfig -> client.Config
Cloud.ClientConfig.httpConfig = cases
  ClientConfig _ _ _ _ _ httpConfig -> httpConfig

Cloud.ClientConfig.httpConfig.doc : Doc
Cloud.ClientConfig.httpConfig.doc =
  {{
  Gets the HTTP {type client.Config} for the Unison Cloud client from the
  {type Cloud.ClientConfig}.
  }}

Cloud.ClientConfig.httpConfig.modify :
  (client.Config ->{g} client.Config)
  -> Cloud.ClientConfig
  ->{g} Cloud.ClientConfig
Cloud.ClientConfig.httpConfig.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host httpPort tcpPort token tlsConfig (f httpConfig)

Cloud.ClientConfig.httpConfig.modify.doc : Doc
Cloud.ClientConfig.httpConfig.modify.doc =
  {{
  Modifies the HTTP configuration field of the {type Cloud.ClientConfig} using
  the given function, returning a new configuration with the updated
  {type client.Config}.

  The value of this field is used to configure the HTTP client used to connect
  to the Unison Cloud service.

  # See also

    * {httpConfig.set} - Sets the value of this field directly.
    * {Config.default} - The default HTTP client configuration.
  }}

Cloud.ClientConfig.httpConfig.set :
  client.Config -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.httpConfig.set httpConfig1 = cases
  ClientConfig host httpPort tcpPort token tlsConfig _ ->
    ClientConfig host httpPort tcpPort token tlsConfig httpConfig1

Cloud.ClientConfig.httpConfig.set.doc : Doc
Cloud.ClientConfig.httpConfig.set.doc =
  {{
  Sets the HTTP configuration field of the {type Cloud.ClientConfig} to the
  given {type client.Config}, returning a new configuration with the updated
  value.

  The value of this field is used to configure the HTTP client used to connect
  to the Unison Cloud service.

  # See also

    * {httpConfig.modify} - Modifies the value of this field using a function.
    * {Config.default} - The default HTTP client configuration.
  }}

Cloud.ClientConfig.httpPort : Cloud.ClientConfig -> Port
Cloud.ClientConfig.httpPort = cases ClientConfig _ httpPort _ _ _ _ -> httpPort

Cloud.ClientConfig.httpPort.doc : Doc
Cloud.ClientConfig.httpPort.doc =
  {{
  Gets the HTTP port number of the Unison Cloud service from the client
  configuration.
  }}

Cloud.ClientConfig.httpPort.modify :
  (Port ->{g} Port) -> Cloud.ClientConfig ->{g} Cloud.ClientConfig
Cloud.ClientConfig.httpPort.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host (f httpPort) tcpPort token tlsConfig httpConfig

Cloud.ClientConfig.httpPort.modify.doc : Doc
Cloud.ClientConfig.httpPort.modify.doc =
  {{
  Modifies the HTTP port field of the {type Cloud.ClientConfig} using the given
  function, returning a new configuration with the updated {type Port}.

  The value of this field is used to determine the port number for HTTP
  connections to the Unison Cloud service.

  # See also

    * {ClientConfig.httpPort.set} - Sets the value of this field directly.
    * {defaultUnisonCloudHttpPort} - Determines the default value of this
      field.
  }}

Cloud.ClientConfig.httpPort.set :
  Port -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.httpPort.set httpPort1 = cases
  ClientConfig host _ tcpPort token tlsConfig httpConfig ->
    ClientConfig host httpPort1 tcpPort token tlsConfig httpConfig

Cloud.ClientConfig.httpPort.set.doc : Doc
Cloud.ClientConfig.httpPort.set.doc =
  {{
  Sets the HTTP port field of the {type Cloud.ClientConfig} to the given
  {type Port}, returning a new configuration with the updated value.

  The value of this field is used to determine the port number for HTTP
  connections to the Unison Cloud service.

  # See also

    * {ClientConfig.httpPort.modify} - Modifies the value of this field using a
      function.
    * {defaultUnisonCloudHttpPort} - Determines the default value of this
      field.
  }}

Cloud.ClientConfig.httpUri : Cloud.ClientConfig -> Path -> RawQuery -> URI
Cloud.ClientConfig.httpUri config path query =
  scheme = match ClientConfig.tlsConfig config with
    None   -> Scheme.http
    Some _ -> Scheme.https
  URI scheme (Some (ClientConfig.authority config)) path query Fragment.empty

Cloud.ClientConfig.httpUri.doc : Doc
Cloud.ClientConfig.httpUri.doc =
  {{
  Constructs a {type URI} for the Unison Cloud client, given the
  {type Cloud.ClientConfig}, the {type Path}, and the {type RawQuery}
  parameters.
  }}

Cloud.ClientConfig.tcpPort : Cloud.ClientConfig -> Port
Cloud.ClientConfig.tcpPort = cases ClientConfig _ _ tcpPort _ _ _ -> tcpPort

Cloud.ClientConfig.tcpPort.doc : Doc
Cloud.ClientConfig.tcpPort.doc =
  {{
  Gets the TCP port number of the Unison Cloud service from the client
  configuration.
  }}

Cloud.ClientConfig.tcpPort.modify :
  (Port ->{g} Port) -> Cloud.ClientConfig ->{g} Cloud.ClientConfig
Cloud.ClientConfig.tcpPort.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host httpPort (f tcpPort) token tlsConfig httpConfig

Cloud.ClientConfig.tcpPort.modify.doc : Doc
Cloud.ClientConfig.tcpPort.modify.doc =
  {{
  Modifies the TCP port field of the {type Cloud.ClientConfig} using the given
  function, returning a new configuration with the updated {type Port}.

  The value of this field is used to determine the port number for TCP
  connections to the Unison Cloud service.

  # See also

    * {tcpPort.set} - Sets the value of this field directly.
    * {defaultUnisonCloudPort} - Determines the default value of this field.
  }}

Cloud.ClientConfig.tcpPort.set :
  Port -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.tcpPort.set tcpPort1 = cases
  ClientConfig host httpPort _ token tlsConfig httpConfig ->
    ClientConfig host httpPort tcpPort1 token tlsConfig httpConfig

Cloud.ClientConfig.tcpPort.set.doc : Doc
Cloud.ClientConfig.tcpPort.set.doc =
  {{
  Sets the TCP port field of the {type Cloud.ClientConfig} to the given
  {type Port}, returning a new configuration with the updated value.

  The value of this field is used to determine the port number for TCP
  connections to the Unison Cloud service.

  # See also

    * {tcpPort.modify} - Modifies the value of this field using a function.
    * {defaultUnisonCloudPort} - Determines the default value of this field.
  }}

Cloud.ClientConfig.tlsConfig : Cloud.ClientConfig -> Optional Tls.ClientConfig
Cloud.ClientConfig.tlsConfig = cases
  ClientConfig _ _ _ _ tlsConfig _ -> tlsConfig

Cloud.ClientConfig.tlsConfig.doc : Doc
Cloud.ClientConfig.tlsConfig.doc =
  {{
  Gets the TLS {type Tls.ClientConfig} for the Unison Cloud client from the
  {type Cloud.ClientConfig}.
  }}

Cloud.ClientConfig.tlsConfig.modify :
  (Optional Tls.ClientConfig ->{g} Optional Tls.ClientConfig)
  -> Cloud.ClientConfig
  ->{g} Cloud.ClientConfig
Cloud.ClientConfig.tlsConfig.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host httpPort tcpPort token (f tlsConfig) httpConfig

Cloud.ClientConfig.tlsConfig.modify.doc : Doc
Cloud.ClientConfig.tlsConfig.modify.doc =
  {{
  Modifies the TLS configuration field of the {type Cloud.ClientConfig} using
  the given function, returning a new configuration with the updated
  {type Tls.ClientConfig}.

  The value of this field is used to configure the TLS client used to connect
  to the Unison Cloud service.

  # See also

    * {ClientConfig.tlsConfig.set} - Sets the value of this field directly.
    * {defaultTlsConfig} - The default TLS client configuration for the Unison
      Cloud service.
  }}

Cloud.ClientConfig.tlsConfig.set :
  Optional Tls.ClientConfig -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.tlsConfig.set tlsConfig1 = cases
  ClientConfig host httpPort tcpPort token _ httpConfig ->
    ClientConfig host httpPort tcpPort token tlsConfig1 httpConfig

Cloud.ClientConfig.tlsConfig.set.doc : Doc
Cloud.ClientConfig.tlsConfig.set.doc =
  {{
  Sets the TLS configuration field of the {type Cloud.ClientConfig} to the
  given {type Tls.ClientConfig}, returning a new configuration with the updated
  value.

  The value of this field is used to configure the TLS client used to connect
  to the Unison Cloud service.

  # See also

    * {ClientConfig.tlsConfig.modify} - Modifies the value of this field using
      a function.
    * {defaultTlsConfig} - The default TLS client configuration for the Unison
      Cloud service.
  }}

Cloud.ClientConfig.token : Cloud.ClientConfig -> AccessToken
Cloud.ClientConfig.token = cases ClientConfig _ _ _ token _ _ -> token

Cloud.ClientConfig.token.doc : Doc
Cloud.ClientConfig.token.doc =
  {{ Gets the access token from the client configuration. }}

Cloud.ClientConfig.token.modify :
  (AccessToken ->{g} AccessToken)
  -> Cloud.ClientConfig
  ->{g} Cloud.ClientConfig
Cloud.ClientConfig.token.modify f = cases
  ClientConfig host httpPort tcpPort token tlsConfig httpConfig ->
    ClientConfig host httpPort tcpPort (f token) tlsConfig httpConfig

Cloud.ClientConfig.token.modify.doc : Doc
Cloud.ClientConfig.token.modify.doc =
  {{
  Modifies the token field of the {type Cloud.ClientConfig} using the given
  function, returning a new configuration with the updated {type AccessToken}.

  The value of this field is used to authenticate with the Unison Cloud
  service.

  # See also

    * {token.set} - Sets the value of this field directly.
  }}

Cloud.ClientConfig.token.set :
  AccessToken -> Cloud.ClientConfig -> Cloud.ClientConfig
Cloud.ClientConfig.token.set token1 = cases
  ClientConfig host httpPort tcpPort _ tlsConfig httpConfig ->
    ClientConfig host httpPort tcpPort token1 tlsConfig httpConfig

Cloud.ClientConfig.token.set.doc : Doc
Cloud.ClientConfig.token.set.doc =
  {{
  Sets the token field of the {type Cloud.ClientConfig} to the given
  {type AccessToken}, returning a new configuration with the updated value.

  The value of this field is used to authenticate with the Unison Cloud
  service.

  # See also

    * {token.modify} - Modifies the value of this field using a function.
  }}

Cloud.Daemon.assign : Daemon -> DaemonHash ->{Exception, Cloud} ()
Cloud.Daemon.assign daemon daemonHash =
  Either.toException (Daemon.assign.impl daemon daemonHash)

Cloud.Daemon.delete : Daemon ->{Exception, Cloud} ()
Cloud.Daemon.delete = Daemon.delete.impl >> Either.toException

Cloud.Daemon.deploy :
  Daemon
  -> Environment
  -> '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Random,
  Log,
  Scratch} ()
  ->{Exception, Cloud} DaemonHash
Cloud.Daemon.deploy daemon environment run =
  hash = DaemonHash.create environment run
  Daemon.assign daemon hash
  hash

Cloud.Daemon.deploy.doc : Doc
Cloud.Daemon.deploy.doc =
  {{
  `` Daemon.deploy daemon environment run `` deploys runs the provided `run`
  function in the specified `environment` with identifier `daemon`. If another
  version of code is currently assigned to `daemon`, the running code will be
  replaced by `run`.

  This is equivalent to calling {DaemonHash.create} followed by
  {Daemon.assign}.
  }}

Cloud.Daemon.logs.tail.console : Daemon ->{IO, Exception, Cloud} Void
Cloud.Daemon.logs.tail.console daemon =
  query =
    QueryOptions.default
      |> search.set (daemon |> Daemon.id |> Daemon.Id.toText |> Some)
  (do pageLogs query logs) |> Stream.foreach (Json.toText >> printLine)

Cloud.Daemon.named : Text ->{Exception, Cloud} Daemon
Cloud.Daemon.named = Daemon.create.impl >> Either.toException

Cloud.Daemon.unassign : Daemon ->{Exception, Cloud} ()
Cloud.Daemon.unassign = Daemon.unassign.impl >> Either.toException

Cloud.DaemonHash.create :
  Environment
  -> '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Random,
  Log,
  Scratch} ()
  ->{Exception, Cloud} DaemonHash
Cloud.DaemonHash.create env run = create.remote env (pool.wrap run)

Cloud.DaemonHash.create.remote :
  Environment -> '{Remote} () ->{Exception, Cloud} DaemonHash
Cloud.DaemonHash.create.remote env run =
  Either.toException (DaemonHash.create.impl env run)

Cloud.DaemonHash.delete : DaemonHash ->{Exception, Cloud} ()
Cloud.DaemonHash.delete = DaemonHash.delete.impl >> Either.toException

Cloud.DaemonHash.logs.tail.console : DaemonHash ->{IO, Exception, Cloud} Void
Cloud.DaemonHash.logs.tail.console daemonHash =
  query =
    QueryOptions.default
      |> search.set (daemonHash |> DaemonHash.toText |> Some)
  (do pageLogs query logs) |> Stream.foreach (Json.toText >> printLine)

Cloud.deploy :
  Environment
  -> (a
  ->{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  Random,
  Log,
  Scratch} b)
  ->{Exception, Cloud} ServiceHash a b
Cloud.deploy env handleRequest = deploy.remote env (pool.wrap handleRequest)

Cloud.deploy.doc : Doc
Cloud.deploy.doc =
  use Cloud deploy
  use Environment default
  use Nat * +
  {{
  `` deploy env f `` deploys a service to Unison Cloud, with access to
  {{ docExample 1 do env -> (env : Environment) }}.

  The service logic is just a function from `a` to `b` that can use various
  abilities.

  Here's a simple example of deploying two services, one which calls the other:

  @typecheck ```
  incService = deploy default() (n -> n + 1)
  deploy default() (n -> Services.call incService (n * n))
  ```
  }}

Cloud.deploy.doc.example : '{IO, Exception, Cloud} ServiceHash Text Bytes
Cloud.deploy.doc.example = do
  env = Environment.named "message-signing-staging"
  setValue env "signing-key" (getEnv "SIGNING_KEY")
  handleRequest : Text ->{Environment.Config, Exception} Bytes
  handleRequest message =
    use Text toUtf8
    signingKey = Config.expect "signing-key" |> toUtf8
    hmacBytes Sha2_256 signingKey (toUtf8 message)
  deploy.remote env (pool.wrap handleRequest)

Cloud.deploy.remote :
  Environment -> (a ->{Remote} b) ->{Exception, Cloud} ServiceHash a b
Cloud.deploy.remote env handleRequest =
  Either.toException (deploy.impl env handleRequest)

Cloud.deploy.remote.doc : Doc
Cloud.deploy.remote.doc =
  {{
  Deploys a service that uses only the {type Remote} ability. Takes an
  {type Environment} and the implementation of the service, and returns a
  {type ServiceHash} that can be used to refer to the deployed service.
  }}

Cloud.deployHttp :
  Environment
  -> (HttpRequest
  ->{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  Random,
  Log,
  Scratch} HttpResponse)
  ->{Exception, Cloud} ServiceHash HttpRequest HttpResponse
Cloud.deployHttp env handleRequest =
  deployHttp.remote env (pool.wrap handleRequest)

Cloud.deployHttp.doc : Doc
Cloud.deployHttp.doc =
  {{
  `` deployHttp environment httpService `` deploys a Unison Cloud HTTP service
  with access to the given environment. The service is a function from
  {type HttpRequest} to {type HttpResponse}.

  Example:

      @source{deployHttp.doc.example}
  }}

Cloud.deployHttp.doc.example :
  '{IO, Exception, Cloud} ServiceHash HttpRequest HttpResponse
Cloud.deployHttp.doc.example = do
  env = Environment.named "message-signing-staging"
  setValue env "signing-key" (getEnv "SIGNING_KEY")
  deployHttp env example.handleRequest

Cloud.deployHttp.remote :
  Environment
  -> (HttpRequest ->{Remote} HttpResponse)
  ->{Exception, Cloud} ServiceHash HttpRequest HttpResponse
Cloud.deployHttp.remote env handleRequest =
  serviceHash = deploy.remote env (pool.wrap handleRequest)
  unison_base_3_22_0.ignore (exposeHttp serviceHash)
  serviceHash

Cloud.deployHttp.remote.doc : Doc
Cloud.deployHttp.remote.doc =
  {{
  Deploys an HTTP service that uses only the {type Remote} ability. Takes an
  {type Environment}, the implementation of the service as a function from
  {type HttpRequest} to {type HttpResponse}, and returns a {type ServiceHash}
  that can be used to refer to the deployed service. Also prints the URL of the
  deployed HTTP service.
  }}

Cloud.deployHttpWebSocket :
  Environment
  -> (HttpRequest
  ->{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  Random,
  Log,
  Scratch} Either
    HttpResponse (websockets.WebSocket ->{Exception, Remote, WebSockets} ()))
  ->{Exception, Cloud} ServiceHash
    HttpRequest
    (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
Cloud.deployHttpWebSocket env handleRequest =
  deployHttpWebSocket.remote env (pool.wrap handleRequest)

Cloud.deployHttpWebSocket.doc : Doc
Cloud.deployHttpWebSocket.doc =
  {{
  Deploys a WebSocket service to the cloud. Takes an {type Environment}, the
  implementation of the service as a function, and returns a {type ServiceHash}
  that can be used to refer to the deployed service. Also prints the URL of the
  deployed WebSocket service.

  The function should take a {type HttpRequest} and return {type Either} a
  {type HttpResponse} or a computation that accepts a
  {type websockets.WebSocket} and interacts with it using the {type Remote} and
  {type WebSockets} abilities.

  # Example

    This deploys a WebSocket service that echoes messages back to the client:

    @typecheck ```
    echoServer request =
      Right
        (socket ->
          forever do
            message = WebSockets.receive socket
            WebSockets.send socket message)
    deployHttpWebSocket Environment.default() echoServer
    ```
  }}

Cloud.deployHttpWebSocket.remote :
  Environment
  -> (HttpRequest
  ->{Remote} Either
    HttpResponse (websockets.WebSocket ->{Exception, Remote, WebSockets} ()))
  ->{Exception, Cloud} ServiceHash
    HttpRequest
    (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
Cloud.deployHttpWebSocket.remote env handleRequest =
  serviceHash =
    deploy.remote
      env (pool.wrap
        (req -> (match handleRequest req with
          Left response -> Left response
          Right f       -> Right (ws -> reraise! do f ws))))
  unison_base_3_22_0.ignore (exposeHttpWebSocket serviceHash)
  serviceHash

Cloud.deployHttpWebSocket.remote.doc : Doc
Cloud.deployHttpWebSocket.remote.doc =
  {{
  A version of {deployHttpWebSocket} that is restricted to only using the
  {type Remote} ability during the establishment of the WebSocket connection.
  }}

Cloud.deployWebSocket :
  Environment
  -> (websockets.WebSocket ->{Exception, Remote, WebSockets} ())
  ->{Exception, Cloud} ServiceHash
    HttpRequest
    (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
Cloud.deployWebSocket env server =
  deployHttpWebSocket env (const (Right server))

Cloud.deployWebSocket.doc : Doc
Cloud.deployWebSocket.doc =
  {{
  Deploys a WebSocket service to the cloud. Takes an {type Environment}, the
  implementation of the service as a function, and returns a {type ServiceHash}
  that can be used to refer to the deployed service. Also prints the URL of the
  deployed WebSocket service.

  The function should take a {type websockets.WebSocket} and interact with it
  using the {type Remote} and {type WebSockets} abilities.

  # Example

    This deploys a WebSocket service that echoes messages back to the client:

    @typecheck ```
    echoServer socket = forever do
      message = WebSockets.receive socket
      WebSockets.send socket message
    deployWebSocket Environment.default() echoServer
    ```
  }}

Cloud.doc : Doc
Cloud.doc =
  use Cloud deploy run submit
  use Nat * +
  use limit set
  use logs service tail
  use service.tail console
  use unexposeHttpWebSocket impl
  {{
  The {type Cloud} ability provides the interface for the Unison Cloud client.
  It has a range of operations for interacting with the Cloud, including the
  ability to deploy and undeploy code, manage databases, store and retrieve
  data, and access cloud logs.

  Operations that interact with the cloud fall into these categories:

  * **Submitting code** : Submitting code to be run on the cloud.
  * **Service management** : Deploying and undeploying services, exposing and
    unexposing endpoints for deployed services, and managing service names.
  * **Environment management** : Creating and deleting environments, and
    setting and deleting environment values.
  * **Database management** : Creating and deleting databases, and assigning
    and unassigning databases to environments.
  * **Logging** : Retrieving logs for services.

  # Cloud and I/O

    The Cloud client ultimately runs in {type IO}, so it can perform I/O
    operations like reading from the file system, making network requests, and
    writing to the console. The functions to run the {type Cloud} ability in
    {type IO} are {run} and {Cloud.main}:

        @signatures{run, Cloud.main}

    The {run} function takes a computation that uses {type Cloud} and runs it
    in {type IO}, returning the result. The computation can also use any other
    ability, which still has to be handled separately.

    The {Cloud.main} function is like a specialized {run}, designed to be used
    as the entry point for your whole program. It takes a computation that's
    restricted to {type Cloud}, {type IO}, and {type Exception}. It turns the
    {type Cloud} operations into {type IO} operations.

    The typical usage pattern is to write a `main` function that begins with a
    call to {Cloud.main}:

    @typecheck ```
    main = Cloud.main do
      x = 2
      submit Environment.default() do x + 1
    ```

  # Submitting code

    The {submit} operation is used to submit code to be run on the cloud. It
    takes an {type Environment} and a computation, and returns the result of
    running the computation on the cloud:

        @signature{submit}

    For example, this submits a computation that adds two numbers:

    @typecheck ```
    submit Environment.default() do 1 + 2
    ```

    The argument to {submit} is allowed to use a number of abilities that are
    provided by the Cloud. Here's a brief overview of the abilities that are
    available:

    * {type Remote} - for forking background computations and waiting for their
      results, possibly at a different {type Location}. Also for access to
      mutable memory and clocks.
    * {type Http} - for making HTTP requests.
    * {type Log} - for logging messages, for example to record errors or the
      progress of a computation.
    * {type Scratch} - for storing and retrieving data in a temporary store
      (shared between computations running on the same node).
    * {type Environment.Config} - for reading configuration values from the
      {type Environment}.
    * {type Services} - for interacting with other services that are running on
      Unison Cloud.
    * {type Exception} - for raising exceptions.
    * {type Storage} - for reading and writing durable state that is shared
      between computations running in the same {type Environment}.
    * {type Random} - for generating random numbers.

  # Service management

    To deploy a service to the cloud, use {deploy}. This operation takes an
    {type Environment} and a computation, and returns a {type ServiceHash} that
    can be used to refer to the deployed service:

        @signature{deploy}

    For example, this deploys two services where one calls the other:

    @typecheck ```
    use Environment default
    incService = deploy default() (n -> n + 1)
    deploy default() (n -> Services.call incService (n * n))
    ```

    Note that services can use all the same abilities as computations submitted
    via {submit}.

    To undeploy a service, use {undeploy}:

        @signature{undeploy}

    To expose an HTTP endpoint for a deployed service, use {exposeHttp}:

        @signature{exposeHttp}

    You can directly deploy a function as an HTTP service using {deployHttp}:

        @signature{deployHttp}

    To unexpose an HTTP endpoint, use {unexposeHttp}:

        @signature{unexposeHttp}

    To expose a WebSocket endpoint for a deployed service, use
    {exposeHttpWebSocket}:

        @signature{exposeHttpWebSocket}

    To unexpose a WebSocket endpoint, use {impl}:

        @signature{impl}

    You can directly deploy a function as a WebSocket service using
    {deployHttpWebSocket}:

        @signature{deployHttpWebSocket}

  # Service names

    Service names are used to refer to services by a name, rather than by their
    {type ServiceHash}. This is useful for referring to persistent services
    across multiple deployments or versions. See the documentation for
    {type ServiceName} for more information on creating and deleting service
    names, and assigning and unassigning service names to service hashes.

  # Environment management

    See the documentation for {type Environment} for more information on
    creating and deleting environments, and setting and deleting environment
    values.

  # Database management

    See the documentation for {type Database} for more information on creating
    and deleting databases, and assigning and unassigning databases to
    environments.

  # Logging

    The {logs} operation is used to retrieve logs for services and jobs. It
    takes a {type QueryOptions} and returns a list of JSON objects:

        @signature{logs}

    For example, this retrieves the last 10 logs for all jobs and services:

    @typecheck ```
    logs (QueryOptions.default |> set (Some 10))
    ```

    The {type QueryOptions} type is used to specify the search criteria for the
    logs. It has fields for searching, limiting the number of logs, and
    specifying a time range.

    The {service} operation is used to retrieve logs for a specific service. It
    takes a {type ServiceHash} and a {type QueryOptions} and returns a list of
    JSON objects:

        @signature{service}

    For example, this retrieves the last 10 logs for a specific service:

    @typecheck ```
    echoService x = do
      info "Echoing" [("value", x)]
      x
    serviceHash = deploy Environment.default() echoService
    Remote.sleep (Remote.Duration.seconds 10)
    service serviceHash (QueryOptions.default |> set (Some 10))
    ```

    The {tail} function gives you a stream of logs for a service, updated in
    realtime. It takes a {type ServiceHash} and returns a stream of JSON
    objects:

        @signature{tail}

    Use {console} to print the logs to the console instead of getting a stream.
    This function takes a {type ServiceHash} and prints the logs to the console
    as they arrive:

        @signature{console}

    See the documentation for {type Log} for more information on logging in
    Unison Cloud.
  }}

Cloud.domains.createDomainMapping : HostName -> Text ->{IO, Exception} ()
Cloud.domains.createDomainMapping domain serviceName =
  createDomainMapping.withConfig
    Cloud.ClientConfig.default() domain serviceName

Cloud.domains.createDomainMapping.doc : Doc
Cloud.domains.createDomainMapping.doc =
  {{
  `` createDomainMapping myDomain myServiceName `` configures Unison Cloud to
  route requests for the domain `myDomain` your service with the name
  `myServiceName`. See [the Cloud domains documentation]({domains.doc}) for
  more information.

  # Related

    * {createDomainMapping.withConfig} accepts the provided
      {type Cloud.ClientConfig} instead of the default.
  }}

Cloud.domains.createDomainMapping.withConfig :
  Cloud.ClientConfig -> HostName -> Text ->{IO, Exception} ()
Cloud.domains.createDomainMapping.withConfig config domain serviceName =
  use Path /
  use Text ++
  domainText = HostName.toText domain
  path = root / "v2" / "domains" / domainText / serviceName
  uri = httpUri config path RawQuery.empty
  req =
    HttpRequest.post uri Body.empty
      |> addAuthHeader (ClientConfig.token config)
  response = handle request req with Http.configuredHandler (httpConfig config)
  context =
    "Registering custom domain "
      ++ domainText
      ++ " for service name "
      ++ serviceName
  expectApiSuccess context response

Cloud.domains.createDomainMapping.withConfig.doc : Doc
Cloud.domains.createDomainMapping.withConfig.doc =
  {{
  A variant of {createDomainMapping} that accepts a custom
  {type Cloud.ClientConfig} instead of using the default.
  }}

Cloud.domains.deleteDomainMapping : HostName ->{IO, Exception} ()
Cloud.domains.deleteDomainMapping domain =
  deleteDomainMapping.withConfig Cloud.ClientConfig.default() domain

Cloud.domains.deleteDomainMapping.doc : Doc
Cloud.domains.deleteDomainMapping.doc =
  {{
  `` deleteDomainMapping myDomain `` deletes the domain mapping configured for
  the domain `myDomain` within your Unison Cloud account. See
  [the Cloud domains documentation]({domains.doc}) for more information.

  # Related

    * {deleteDomainMapping.withConfig} accepts the provided
      {type Cloud.ClientConfig} instead of the default.
  }}

Cloud.domains.deleteDomainMapping.withConfig :
  Cloud.ClientConfig -> HostName ->{IO, Exception} ()
Cloud.domains.deleteDomainMapping.withConfig config domain =
  use Path /
  use Text ++
  domainText = HostName.toText domain
  path = root / "v2" / "domains" / domainText
  uri = httpUri config path RawQuery.empty
  req = HttpRequest.delete uri |> addAuthHeader (ClientConfig.token config)
  response = handle request req with Http.configuredHandler (httpConfig config)
  context = "getting custom domain mapping for domain " ++ domainText
  expectApiSuccess context response

Cloud.domains.deleteDomainMapping.withConfig.doc : Doc
Cloud.domains.deleteDomainMapping.withConfig.doc =
  {{
  A variant of {deleteDomainMapping} that accepts a custom
  {type Cloud.ClientConfig} instead of using the default.
  }}

Cloud.domains.doc : Doc
Cloud.domains.doc =
  {{
  # Unison Cloud services on custom domains

    When you deploy an HTTP service on Unison Cloud, it is assigned a URL of
    the form `https://my-username.unison-services.cloud/s/my-service/` where
    `my-username` is your Unison Cloud/Share username and `my-service` is the
    [service name]({type ServiceName}).

    If you are on a paid [Unison Cloud plan](https://www.unison.cloud/pricing/)
    then you can also expose services under your own custom domain. For
    example, if you own `my-excellent-domain.com`, then you can configure
    requests to `my-excellent-domain.com` (or
    `my-subdomain.my-excellent-domain.com`) to be served as though they were
    issued to `my-username.unison-services.cloud/s/my-service`.

  # Setting up a custom domain

    Below you will find step-by-step instructions.

    For the impatient, it looks something like this:

        @source{domains.example}

    ``` sh
    > dig +noall +answer my-subdomain.my-excellent-domain.com A
    my-subdomain.my-excellent-domain.com      1526    IN      A       54.189.213.233
    ```

    ## Step 1: Own a domain

       Unison Cloud can't really help you here, but there are plenty of domain
       registrars who can, such as [namecheap](https://www.namecheap.com/) or
       [porkbun](https://porkbun.com/).

    ## Step 2: Deploy a service

       Write your HTTP service and deploy it with {deployHttp} or
       {deployHttpWebSocket}.

    ## Step 3: Assign a service name

       Create a service name with {ServiceName.named} and then assign your
       service to it with {ServiceName.assign}.

    ## Step 4: Configure the custom domain mapping

       Call {createDomainMapping} to configure Unison Cloud to route requests
       for your domain to your service.

    ## Step 5: Point your DNS to Unison Cloud

       Use your DNS provider to add an `A` record that points your domain (or
       subdomain) to `54.189.213.233`, which is a stable IP address for Unison
       Cloud user services.

       An example DNS record might look like the following:

       ``` sh
       > dig +noall +answer my-subdomain.my-excellent-domain.com A
       my-subdomain.my-excellent-domain.com      1526    IN      A       54.189.213.233
       ```

       {{
       docCallout
         (Some {{ ⚠️ }})
         {{
         For security, do __not__ point a wildcard record such as
         `*.my-excellent-domain.com` to Unison Cloud. Create individual
         mappings for each subdomain that you wish to forward to Unison Cloud.
         }} }}

       That's it! You can now make a request to your service at
       `https://my-subdomain.my-excellent-domain.com` 🚀 ☁
  }}

Cloud.domains.example : '{IO, Exception} ()
Cloud.domains.example =
  myService : HttpRequest -> HttpResponse
  myService = cases
    HttpRequest GET _ (URI _ _ (Path ["hello"]) _ _) _ _ ->
      "hi!" |> Text.toUtf8 |> Body |> HttpResponse.ok
    _ -> HttpResponse.notFound
  do
    Cloud.run do
      service = ServiceName.named "my-service"
      serviceHash = deployHttp Environment.default() myService
      unisonCloudUrl = ServiceName.assign service serviceHash
      myDomain = HostName "my-subdomain.my-excellent-domain.com"
      createDomainMapping myDomain (ServiceName.name service)

Cloud.domains.getDomainMapping : HostName ->{IO, Exception} Text
Cloud.domains.getDomainMapping domain =
  getDomainMapping.withConfig Cloud.ClientConfig.default() domain

Cloud.domains.getDomainMapping.doc : Doc
Cloud.domains.getDomainMapping.doc =
  {{
  `` getDomainMapping myDomain `` returns the domain mapping configured for the
  domain `myDomain` within your Unison Cloud account. See
  [the Cloud domains documentation]({domains.doc}) for more information.

  # Related

    * {getDomainMapping.withConfig} accepts the provided
      {type Cloud.ClientConfig} instead of the default.
    * {listDomainMappings} lists all domain mappings for your account.
  }}

Cloud.domains.getDomainMapping.withConfig :
  Cloud.ClientConfig -> HostName ->{IO, Exception} Text
Cloud.domains.getDomainMapping.withConfig config domain =
  use Path /
  use Text ++
  domainText = HostName.toText domain
  path = root / "v2" / "domains" / domainText
  uri = httpUri config path RawQuery.empty
  req = HttpRequest.get uri |> addAuthHeader (ClientConfig.token config)
  response = handle request req with Http.configuredHandler (httpConfig config)
  context = "getting custom domain mapping for domain " ++ domainText
  expectApiSuccess context response
  response
    |> HttpResponse.body
    |> Body.toBytes
    |> fromUtf8
    |> Decoder.run (customDomainDetails >> at2)

Cloud.domains.getDomainMapping.withConfig.doc : Doc
Cloud.domains.getDomainMapping.withConfig.doc =
  {{
  A variant of {getDomainMapping} that accepts a custom
  {type Cloud.ClientConfig} instead of using the default.
  }}

Cloud.domains.listDomainMappings : '{IO, Exception} [(HostName, Text)]
Cloud.domains.listDomainMappings =
  listDomainMappings.withConfig << Cloud.ClientConfig.default

Cloud.domains.listDomainMappings.doc : Doc
Cloud.domains.listDomainMappings.doc =
  {{
  `` listDomainMappings `` lists the domain mappings configured for your Unison
  Cloud account. See [the Cloud domains documentation]({domains.doc}) for more
  information.

  # Related

    * {listDomainMappings.withConfig} accepts the provided
      {type Cloud.ClientConfig} instead of the default.
    * {getDomainMapping} returns the domain mapping for an individual domain.
  }}

Cloud.domains.listDomainMappings.withConfig :
  Cloud.ClientConfig ->{IO, Exception} [(HostName, Text)]
Cloud.domains.listDomainMappings.withConfig =
  use Path /
  decodeEntries = Decoder.array customDomainDetails
  config ->
    let
      path = root / "v2" / "domains"
      uri = httpUri config path RawQuery.empty
      req = HttpRequest.get uri |> addAuthHeader (ClientConfig.token config)
      response =
        handle request req with Http.configuredHandler (httpConfig config)
      context = "listing custom domains"
      expectApiSuccess context response
      response
        |> HttpResponse.body
        |> Body.toBytes
        |> fromUtf8
        |> Decoder.run decodeEntries

Cloud.domains.listDomainMappings.withConfig.doc : Doc
Cloud.domains.listDomainMappings.withConfig.doc =
  {{
  A variant of {listDomainMappings} that accepts a custom
  {type Cloud.ClientConfig} instead of using the default.
  }}

Cloud.exposeHttp :
  ServiceHash HttpRequest HttpResponse ->{Exception, Cloud} URI
Cloud.exposeHttp serviceHash = Either.toException (exposeHttp.impl serviceHash)

Cloud.exposeHttp.doc : Doc
Cloud.exposeHttp.doc =
  {{
  Expose HTTP endpoints for an existing Unison Cloud service. The service must
  have {type HttpRequest} as its input type and {type HttpResponse} as its
  output type. The service must already be deployed (see {deploy.remote}).

  Example:

      @source{exposeHttp.doc.example}
  }}

Cloud.exposeHttp.doc.example : '{IO, Exception, Cloud} URI
Cloud.exposeHttp.doc.example = do
  env = Environment.named "message-signing-staging"
  setValue env "signing-key" (getEnv "SIGNING_KEY")
  serviceHash = deploy.remote env (pool.wrap example.handleRequest)
  exposeHttp serviceHash

Cloud.exposeHttp.doc.example.handleRequest :
  HttpRequest ->{Environment.Config, Exception} HttpResponse
Cloud.exposeHttp.doc.example.handleRequest = cases
  HttpRequest POST _ (URI _ _ (Path ["sign"]) _ _) _ (Body message) ->
    signingKey = Config.expect "signing-key" |> Text.toUtf8
    signature = hmacBytes Sha2_256 signingKey message
    HttpResponse.ok (Body signature)
  _ -> HttpResponse.notFound

Cloud.exposeHttpWebSocket :
  ServiceHash
    HttpRequest
    (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
  ->{Exception, Cloud} URI
Cloud.exposeHttpWebSocket hash =
  Either.toException (exposeHttpWebSocket.impl hash)

Cloud.exposeHttpWebSocket.doc : Doc
Cloud.exposeHttpWebSocket.doc =
  {{
  Exposes a WebSocket endpoint for a deployed service. Takes a
  {type ServiceHash} and returns a {type URI} that can be used to connect to
  the WebSocket service.

  # Example

    This exposes a WebSocket endpoint for a deployed service:

    @typecheck ```
    echoServer request =
      Right
        (socket ->
          forever do
            message = WebSockets.receive socket
            WebSockets.send socket message)
    deployHttpWebSocket Environment.default() echoServer
    ```
  }}

Cloud.logs : QueryOptions ->{Exception, Cloud} [Json]
Cloud.logs options = Either.toException (logs.impl options)

Cloud.logs.Direction.doc : Doc
Cloud.logs.Direction.doc =
  {{
  The direction in which to search for logs. If {Forward}, the search will be
  performed from the start time to the end time. If {Backward}, the search will
  be performed from the end time to the start time.
  }}

Cloud.logs.Direction.toText : Direction -> Text
Cloud.logs.Direction.toText = cases
  Forward  -> "true"
  Backward -> "false"

Cloud.logs.Direction.toText.doc : Doc
Cloud.logs.Direction.toText.doc =
  use Direction toText
  {{
  Converts a {type Direction} to either "true" or "false", depending on whether
  the direction is {Forward} or {Backward}, respectively.

  # Examples

    ```
    toText Forward
    ```

    ```
    toText Backward
    ```
  }}

Cloud.logs.doc : Doc
Cloud.logs.doc =
  {{
  Retrieves logs for services and jobs on Unison Cloud.

  # Example

    This retrieves the last 10 logs that your jobs and services have produced:

    @typecheck ```
    logs (QueryOptions.default |> limit.set (Some 10))
    ```

    The {type QueryOptions} type is used to specify the search criteria for the
    logs. It has fields for searching, limiting the number of logs, and
    specifying a time range.

  # See also

    * {type QueryOptions} for specifying search criteria for logs
    * {type Log} for more information on logging in Unison Cloud
  }}

Cloud.logs.internal.pageLogs :
  QueryOptions
  -> (QueryOptions ->{g, IO} [Json])
  ->{g, IO, Exception, Stream Json} Void
Cloud.logs.internal.pageLogs initialQuery fetchLogs =
  use IO ref
  use Instant +
  use mutable Ref
  newQuery t0 =
    initialQuery
      |> start.set (Some t0)
      |> end.set (Some (t0 + time.Duration.hours +1))
      |> direction.set (Some Forward)
      |> limit.set (Some 500)
  query : Ref {IO} QueryOptions
  query = ref (newQuery realtime())
  seen : Ref {IO} (Set (Text, Instant))
  seen = ref Set.empty
  extract : Json -> (Text, Instant, Json)
  extract json =
    use Decoder text
    use object at!
    decoder =
      do
        ( at! "id" text
        , at! "time" text
            |> Instant.fromIso8601
            |> (getOrElse' do Decoder.fail "Cannot parse timestamp")
        , at! "userMsg" Decoder.value
        )
    json |> tryRun.parsed decoder |> Decoder.reraise
  go : '{g, IO, Exception, Stream Json} Void
  go =
    do
      use Instant ==
      use mutable.Ref read
      logLines = fetchLogs (read query)
      toStream! do
        line = each logLines
        let
          (id, time, userData) = extract line
          guard (read seen |> Set.contains (id, time) |> Boolean.not)
          modify.deprecated
            seen
            (Set.insert (id, time) >> (Set.filter cases (_, t) -> t == time))
          mutable.Ref.write query (newQuery time)
          userData
      concurrent.sleepMicroseconds 3000000
      go()
  go()

Cloud.logs.QueryOptions.default : QueryOptions
Cloud.logs.QueryOptions.default = QueryOptions None None None None None

Cloud.logs.QueryOptions.default.doc : Doc
Cloud.logs.QueryOptions.default.doc =
  {{
  The default options for searching logs with {logs}. The default options are:

  * {QueryOptions.search} is set to {None} meaning that all logs are returned.
  * {QueryOptions.limit} is set to {None} meaning that all logs are returned.
  * {QueryOptions.start} is set to {None} meaning that the search starts from
    the beginning.
  * {end} is set to {None} meaning that the search ends at the most recent log.
  * {direction} is set to {None} indicating no preference for the direction of
    the search.
  }}

Cloud.logs.QueryOptions.direction : QueryOptions -> Optional Direction
Cloud.logs.QueryOptions.direction = cases
  QueryOptions _ _ _ _ direction -> direction

Cloud.logs.QueryOptions.direction.doc : Doc
Cloud.logs.QueryOptions.direction.doc =
  {{
  Gets the {type Direction} of the search for logs from the
  {type QueryOptions}.
  }}

Cloud.logs.QueryOptions.direction.modify :
  (Optional Direction ->{g} Optional Direction)
  -> QueryOptions
  ->{g} QueryOptions
Cloud.logs.QueryOptions.direction.modify f = cases
  QueryOptions search limit start end direction ->
    QueryOptions search limit start end (f direction)

Cloud.logs.QueryOptions.direction.set :
  Optional Direction -> QueryOptions -> QueryOptions
Cloud.logs.QueryOptions.direction.set direction1 = cases
  QueryOptions search limit start end _ ->
    QueryOptions search limit start end direction1

Cloud.logs.QueryOptions.doc : Doc
Cloud.logs.QueryOptions.doc =
  use QueryOptions default search
  {{
  The search options for retrieving logs. The {type QueryOptions} type has
  fields for searching, limiting the number of logs, and specifying a time
  range.

  The {default} value is a default set of options for searching logs. You can
  customize these options by setting the fields to different values. For
  example, to search for logs with a specific search string, you can set the
  {search} field:

  @typecheck ```
  logs (default |> search.set (Some "error"))
  ```

  # Fields

    * {search} - The search string to match against the log messages.
    * {QueryOptions.limit} - The maximum number of logs to return.
    * {QueryOptions.start} - The start time for the search.
    * {end} - The end time for the search.
    * {direction} - The direction in which to search for logs, either {Forward}
      or {Backward}.
  }}

Cloud.logs.QueryOptions.end : QueryOptions -> Optional Instant
Cloud.logs.QueryOptions.end = cases QueryOptions _ _ _ end _ -> end

Cloud.logs.QueryOptions.end.doc : Doc
Cloud.logs.QueryOptions.end.doc =
  {{
  Gets the end time of the search for logs from the {type QueryOptions}, as an
  {type Optional} {type Instant}.
  }}

Cloud.logs.QueryOptions.end.modify :
  (Optional Instant ->{g} Optional Instant) -> QueryOptions ->{g} QueryOptions
Cloud.logs.QueryOptions.end.modify f = cases
  QueryOptions search limit start end direction ->
    QueryOptions search limit start (f end) direction

Cloud.logs.QueryOptions.end.set :
  Optional Instant -> QueryOptions -> QueryOptions
Cloud.logs.QueryOptions.end.set end1 = cases
  QueryOptions search limit start _ direction ->
    QueryOptions search limit start end1 direction

Cloud.logs.QueryOptions.limit : QueryOptions -> Optional Nat
Cloud.logs.QueryOptions.limit = cases QueryOptions _ limit _ _ _ -> limit

Cloud.logs.QueryOptions.limit.doc : Doc
Cloud.logs.QueryOptions.limit.doc =
  {{
  Gets the limit of the search for logs from the {type QueryOptions}, as an
  {type Optional} {type Nat}.
  }}

Cloud.logs.QueryOptions.limit.modify :
  (Optional Nat ->{g} Optional Nat) -> QueryOptions ->{g} QueryOptions
Cloud.logs.QueryOptions.limit.modify f = cases
  QueryOptions search limit start end direction ->
    QueryOptions search (f limit) start end direction

Cloud.logs.QueryOptions.limit.set :
  Optional Nat -> QueryOptions -> QueryOptions
Cloud.logs.QueryOptions.limit.set limit1 = cases
  QueryOptions search _ start end direction ->
    QueryOptions search limit1 start end direction

Cloud.logs.QueryOptions.search : QueryOptions -> Optional Text
Cloud.logs.QueryOptions.search = cases QueryOptions search _ _ _ _ -> search

Cloud.logs.QueryOptions.search.doc : Doc
Cloud.logs.QueryOptions.search.doc =
  {{
  Gets the search string of the search for logs from the {type QueryOptions},
  as an {type Optional} {type Text}.
  }}

Cloud.logs.QueryOptions.search.modify :
  (Optional Text ->{g} Optional Text) -> QueryOptions ->{g} QueryOptions
Cloud.logs.QueryOptions.search.modify f = cases
  QueryOptions search limit start end direction ->
    QueryOptions (f search) limit start end direction

Cloud.logs.QueryOptions.search.set :
  Optional Text -> QueryOptions -> QueryOptions
Cloud.logs.QueryOptions.search.set search1 = cases
  QueryOptions _ limit start end direction ->
    QueryOptions search1 limit start end direction

Cloud.logs.QueryOptions.start : QueryOptions -> Optional Instant
Cloud.logs.QueryOptions.start = cases QueryOptions _ _ start _ _ -> start

Cloud.logs.QueryOptions.start.doc : Doc
Cloud.logs.QueryOptions.start.doc =
  {{
  Gets the start time of the search for logs from the {type QueryOptions}, as
  an {type Optional} {type Instant}.
  }}

Cloud.logs.QueryOptions.start.modify :
  (Optional Instant ->{g} Optional Instant) -> QueryOptions ->{g} QueryOptions
Cloud.logs.QueryOptions.start.modify f = cases
  QueryOptions search limit start end direction ->
    QueryOptions search limit (f start) end direction

Cloud.logs.QueryOptions.start.set :
  Optional Instant -> QueryOptions -> QueryOptions
Cloud.logs.QueryOptions.start.set start1 = cases
  QueryOptions search limit _ end direction ->
    QueryOptions search limit start1 end direction

Cloud.logs.service : ServiceHash a b -> QueryOptions ->{IO, Exception} [Json]
Cloud.logs.service hash options =
  service.withConfig Cloud.ClientConfig.default() hash options

Cloud.logs.service.doc : Doc
Cloud.logs.service.doc =
  use Cloud deploy
  {{
  Gets the log entries for a service running on Unison Cloud, as a {type List}.
  Takes a {type ServiceHash} and a {type QueryOptions} and returns a list of
  log entries. The {type QueryOptions} can be used to specify the search
  criteria for the logs and limit the number of logs returned. The
  {type ServiceHash} is returned from e.g. {deploy}.

  The log entries are {type Json} objects as specified in {logs.spec}.

  # Example

    This retrieves the last 10 logs for a specific service:

    @typecheck ```
    echoService x = do
      info "Echoing" [("value", x)]
      x
    serviceHash = deploy Environment.default() echoService
    Remote.sleep (Remote.Duration.seconds 10)
    logs.service serviceHash (QueryOptions.default |> limit.set (Some 10))
    ```
  }}

Cloud.logs.service.tail : ServiceHash a b ->{IO, Exception, Stream Json} Void
Cloud.logs.service.tail service =
  service.tail.withConfig Cloud.ClientConfig.default() service

Cloud.logs.service.tail.console : ServiceHash a b ->{IO, Exception} Void
Cloud.logs.service.tail.console service =
  Cloud.run do
    (do service.tail service)
      |> Stream.map Json.toText
      |> Stream.foreach printLine

Cloud.logs.service.tail.console.doc : Doc
Cloud.logs.service.tail.console.doc =
  use service.tail console
  {{
  Prints the logs for a specific service to the console in real-time.

  The {console} operation takes a {type ServiceHash} and prints the logs to the
  console as they arrive. New logs are printed as they are produced by the
  service.

  The log entries are JSON, as specified in {logs.spec}.

  # Example

    This prints the logs for a specific service to the console in real-time:

    @typecheck ```
    echoService x = do
      info "Echoing" [("value", x)]
      x
    serviceHash = Cloud.deploy Environment.default() echoService
    console serviceHash
    ```
  }}

Cloud.logs.service.tail.doc : Doc
Cloud.logs.service.tail.doc =
  use Cloud deploy
  use Environment default
  use service tail
  {{
  Retrieves a {type Stream} of logs for a specific service, updated in
  realtime.

  The {tail} operation takes a {type ServiceHash} and returns a stream of JSON
  objects. The stream is updated in realtime as new logs are produced by the
  service.

  The log entries are {type Json} objects as specified in {logs.spec}.

  # Example

    This sends the logs for one service to another service in real-time:

    @typecheck ```
    echoService x = do
      info "Echoing" [("value", x)]
      x
    db = Database.named "logs-db"
    logService log =
      logTable = OrderedTable.named db "logs" Universal.ordering
      timestamp = object.at "time" Decoder.text |> run.parsed <| log
      OrderedTable.write logTable timestamp log
    _ = deploy default() logService
    serviceHash = deploy default() echoService
    Stream.foreach logService do tail serviceHash
    ```
  }}

Cloud.logs.service.tail.withConfig :
  Cloud.ClientConfig -> ServiceHash a b ->{IO, Exception, Stream Json} Void
Cloud.logs.service.tail.withConfig config service =
  pageLogs QueryOptions.default (service.withConfig config service)

Cloud.logs.service.tail.withConfig.doc : Doc
Cloud.logs.service.tail.withConfig.doc =
  {{
  A version of {service.tail} that takes a {type Cloud.ClientConfig} to use
  instead of the default configuration.
  }}

Cloud.logs.service.withConfig :
  Cloud.ClientConfig
  -> ServiceHash a b
  -> QueryOptions
  ->{IO, Exception} [Json]
Cloud.logs.service.withConfig config serviceHash options =
  decodeText res = res |> HttpResponse.body |> Body.toBytes |> fromUtf8
  toNanos = Nat.toText << Int.toRepresentation << nanosecondsSinceEpoch
  result =
    use Path /
    use Text ++
    addQuery which maybe acc = match maybe with
      None       -> acc
      Some value -> acc & (which, value)
    query =
      Query.empty
        |> addQuery "search" (options |> QueryOptions.search)
        |> addQuery
             "limit" (Optional.map Nat.toText (options |> QueryOptions.limit))
        |> addQuery
             "start" (Optional.map toNanos (options |> QueryOptions.start))
        |> addQuery "end" (Optional.map toNanos (options |> end))
        |> addQuery
             "direction" (Optional.map Direction.toText (options |> direction))
    path = root / "v2" / "logs" / "deployment" / ServiceHash.toText serviceHash
    uri = httpUri config path (fromQuery query)
    req = HttpRequest.get uri |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "fetching logs for service " ++ ServiceHash.toText serviceHash
    expectApiSuccess context res
    res
      |> decodeText
      |> Decoder.run (object.at "logs" (Decoder.array Decoder.text))
      |> List.map (s -> Json.fromText s)
  result

Cloud.logs.service.withConfig.doc : Doc
Cloud.logs.service.withConfig.doc =
  {{
  A version of {logs.service} that takes a {type Cloud.ClientConfig} to use
  instead of the default configuration.
  }}

Cloud.logs.spec : Doc
Cloud.logs.spec =
  {{
  # JSON Schema for Unison Cloud logs

    The schema for Unison Cloud logs is an array of log entries. Each log entry
    is an object with the following properties:

    ``` json
    {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "nodeId": { "type": "string" },
          "time": { "type": "string" },
          "level": { "type": "string" },
          "userId": { "type": "string" },
          "jobId": { "type": "string" },
          "meta": {
            "type": "object",
            "properties": {
              "serviceHash": { "type": "string" }
            },
            "required": ["serviceHash"]
          },
          "envId": { "type": "string" },
          "type": { "type": "string" },
          "id": { "type": "string" },
          "userMsg": {
            "type": "object",
            "properties": {
              "level": { "type": "string" },
              "message": { "type": "string" }
            },
            "required": ["level", "message"]
          }
        },
        "required": ["nodeId", "time", "level", "userId", "jobId", "meta", "envId", "type", "id", "userMsg"]
      }
    }
    ```

    The properties of a log entry are:

    * `nodeId` - The ID of the node that produced the log entry. A node is a
      physical or virtual machine that runs Unison Cloud services. A string.
    * `time` - The time the log entry was produced. A string in ISO 8601
      format.
    * `level` - The log level of the entry (for example, "info", "warn",
      "debug", or "error"). A string.
    * `userId` - The ID of the user who produced the log entry. A string.
    * `jobId` - The ID of the job that produced the log entry. A job is a
      computation that runs on Unison Cloud. A string.
    * `meta` - Additional metadata for the log entry. This will have at least a
      `serviceHash` property, which is the hash of the service that produced
      the log entry. An object.
    * `envId` - The ID of the environment in which the log entry was produced.
      An environment is a set of configurations for running Unison Cloud
      services. A string.
    * `type` - The name of the Unison type of the data associated with the log
      entry. A string.
    * `id` - The ID of the log entry. A string.
    * `userMsg` - The user-created message in the log entry. This will have at
      least a `level` property, which is the log level of the message, and a
      `message` property, which is the message itself. It can additionally have
      other properties set by the application that produced the log entry. An
      object.
  }}

Cloud.logs.tail : QueryOptions ->{IO, Exception, Stream Json} Void
Cloud.logs.tail query = logs.tail.withConfig Cloud.ClientConfig.default() query

Cloud.logs.tail.console.doc : Doc
Cloud.logs.tail.console.doc =
  {{
  Tails the logs of a given {type ServiceHash} and prints them to the console.
  }}

Cloud.logs.tail.doc : Doc
Cloud.logs.tail.doc =
  {{
  Tails the logs for a given [query]({type QueryOptions}). Also see
  {Cloud.logs.tail.console}.

  Note: This function is not supported in {Cloud.run.local} or
  {main.local.serve}, but note that they both tail logs to the console
  automatically.
  }}

Cloud.logs.tail.withConfig :
  Cloud.ClientConfig -> QueryOptions ->{IO, Exception, Stream Json} Void
Cloud.logs.tail.withConfig config query =
  run.withConfig config do pageLogs query logs

Cloud.logs.tail.withConfig.doc : Doc
Cloud.logs.tail.withConfig.doc =
  {{
  A version of {logs.tail} that takes a {type Cloud.ClientConfig} to use
  instead of the default configuration.
  }}

Cloud.main : '{IO, Exception, Cloud} a -> '{IO, Exception} a
Cloud.main c = do Cloud.run c

Cloud.main.doc : Doc
Cloud.main.doc =
  {{
  Helper to create a main function that uses the {type Cloud} ability:

      @source{myMain}

  You'll typically declare a top-level function like this, then run it via:

  ``` ucm
  myproj/main> run example.myMain
  ```

  **Also see:**

  * {main.local.serve} which is like this function but runs on your local
    machine
  * {Cloud.run}, which doesn't return a delayed computation
  }}

Cloud.main.example.myMain :
  '{IO, Exception} ServiceHash HttpRequest HttpResponse
Cloud.main.example.myMain = Cloud.main do
  logic req = HttpResponse.ok (Body (Text.toUtf8 "Hello, world!"))
  deployHttp Environment.default() logic

Cloud.main.local : '{IO, Exception, Cloud} t -> '{IO, Exception} t
Cloud.main.local c = do Cloud.run.local c

Cloud.main.local.doc : Doc
Cloud.main.local.doc =
  {{
  Just like {Cloud.main}, but produces a `main` function that runs locally
  rather than in the cloud.

  **Also see:**

  * {main.local.serve} to product a `main` function that runs until Enter is
    pressed, useful for interactive use cases
  * {Cloud.run.local} if you want a function that forces the thunk rather than
    returning it.
  }}

Cloud.main.local.serve : '{IO, Exception, Cloud} a -> '{IO, Exception} a
Cloud.main.local.serve p = do run.local.serve p

Cloud.main.local.serve.doc : Doc
Cloud.main.local.serve.doc =
  {{
  Runs a {type Cloud} computation locally, pausing to read a line before
  exiting. Typically used for launching services locally, since once your main
  function exits, all services will be shut down.

  **See also:**

  * {main.local} which doesn't pause to read a line before exiting
  * {Cloud.main} which interprets {type Cloud} in Unison Cloud, instead of
    locally
  }}

Cloud.run : '{g, Cloud} a ->{g, IO, Exception} a
Cloud.run p = run.withConfig Cloud.ClientConfig.default() p

Cloud.run.doc : Doc
Cloud.run.doc =
  use Cloud run
  use Nat +
  {{
  The {run} function takes a computation that uses {type Cloud} and runs it in
  {type IO}, returning the result. The computation can also use any other
  ability, which is not handled by {run}. This function communicates with the
  Unison Cloud client.

  # Example

    This runs a computation that submits a cloud job to add 1 to a number:

    @typecheck ```
    run do
      x = 2
      Cloud.submit Environment.default() do x + 1
    ```

  # See also

    * {Cloud.main} for running a computation that uses {type Cloud}, {type IO},
      and {type Exception} only.
    * {Cloud.run.local} for running a computation that uses {type Cloud}
      locally without communicating with the Unison Cloud client.
  }}

Cloud.run.local : '{IO, Exception, Cloud} a ->{IO, Exception} a
Cloud.run.local p =
  splitmix base.IO.randomNat() do
    local.impl LocalCloudConfig.default() Interrupt.new() p

Cloud.run.local.doc : Doc
Cloud.run.local.doc =
  use Cloud.run local
  use Nat +
  {{
  The {local} function takes a computation that uses {type Cloud} and runs it
  locally without communicating with the Unison Cloud client. This function is
  useful for testing cloud computations locally.

  # Example

    This runs a computation that submits a cloud job to add 1 to a number
    locally:

    @typecheck ```
    local do
      x = 2
      Cloud.submit Environment.default() do x + 1
    ```

  # Configuration

    {{ configuration }}

  # See also

    * {Cloud.run} for running a computation that uses {type Cloud} and
      communicates with the Unison Cloud client.
    * {run.local.serve} for a version that runs the {type Cloud} locally,
      pausing to read a line before exiting.
  }}

Cloud.run.local.doc.configuration : Doc
Cloud.run.local.doc.configuration =
  {{
  You can configure the local bind host and port by setting the following
  environment variables:

  * `UNISON_CLOUD_LOCAL_HTTP_HOST` - the local hostname to bind to.
    **Example:** `localhost`.
  * `UNISON_CLOUD_LOCAL_HTTP_PORT` - the local port to bind to. **Default:**
    `8080`.
  }}

Cloud.run.local.impl :
  LocalCloudConfig
  -> run.Interrupt
  -> '{IO, Exception, Cloud} a
  ->{IO, Exception, Random} a
Cloud.run.local.impl localConfig interrupt p =
  use Bytes toHex
  use Either flatMapRight mapRight
  use Exception raise
  use LocalCloudConfig httpPort
  use Map get values
  use Optional flatMap
  use Random bytes
  use Set filter
  use URI /
  use concurrent fork
  use concurrent.Promise write_
  use concurrent_1_1_0 Map.lookup
  use mutable Ref Ref.read
  use ref atomically
  use run Interrupt.Interrupt
  use run.Interrupt interruptAndAwaitFinalization interruptibly signalFinalization
  use unison_base_3_22_0 ignore
  use websockets WebSocket.WebSocket
  printBanners = LocalCloudConfig.printBanners localConfig
  httpBaseUri =
    host = Optional.getOrElse (HostName "localhost") (httpHost localConfig)
    authority = Authority None host (Some (httpPort localConfig))
    URI Scheme.http (Some authority) root RawQuery.empty Fragment.empty
  rng = io()
  newId : '{IO} Text
  newId = do rng do bytes 16 |> toHex
  services :
    systemfw_concurrent_2_2_0.Map
      (ServiceHash () ()) (Environment, (Any ->{Remote} Any))
  services = systemfw_concurrent_2_2_0.Map.empty()
  daemons :
    Ref
      {IO}
      ( data.Map DaemonHash (Environment, '{Remote} ()),
        data.Map Text Daemon.Id,
        data.Map Daemon.Id (Optional (DaemonHash, run.Interrupt)))
  daemons = IO.ref (data.Map.empty, data.Map.empty, data.Map.empty)
  serviceNames :
    Ref
      {IO}
      ( data.Map Text ServiceName.Id,
        data.Map ServiceName.Id (ServiceHash () ()))
  serviceNames = IO.ref (data.Map.empty, data.Map.empty)
  exposedv1 : Ref {IO} (Set (ServiceHash HttpRequest HttpResponse))
  exposedv1 = IO.ref Set.empty
  exposedv2 :
    Ref
      {IO}
      (Set
        (ServiceHash
          HttpRequest
          (Either HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))))
  exposedv2 = IO.ref Set.empty
  state : systemfw_concurrent_2_2_0.Map Database (data.Map (Text, Any) Any)
  state = systemfw_concurrent_2_2_0.Map.empty()
  byteBlobs :
    systemfw_concurrent_2_2_0.Map (Database.Id, Key) (Bytes, Metadata)
  byteBlobs = systemfw_concurrent_2_2_0.Map.empty()
  typedBlobs : systemfw_concurrent_2_2_0.Map (Database.Id, Key) (Any, Metadata)
  typedBlobs = systemfw_concurrent_2_2_0.Map.empty()
  controlPlane :
    Ref
      {IO}
      ( data.Map Text Environment,
        data.Map Text Database,
        data.Map Environment (data.Map Text Text),
        Set (Environment, Database))
  controlPlane =
    IO.ref (data.Map.empty, data.Map.empty, data.Map.empty, Set.empty)
  verifyDatabaseAccess : Environment -> Database -> Either Failure ()
  verifyDatabaseAccess env db =
    use Text ++
    dbPermissions = at4 (Ref.read controlPlane)
    if Set.contains (env, db) dbPermissions then Right()
    else
      Left
        (Failure
          (typeLink RestrictedOperation)
          ("The environment does not have access to the requested database: "
            ++ Database.toText db
            ++ ". Did you mean to call Database.assign?")
          (Any ((env : Environment), (db : Database))))
  logs : systemfw_concurrent_2_2_0.Map (ServiceHash () ()) [Json]
  logs = systemfw_concurrent_2_2_0.Map.empty()
  webSockets :
    systemfw_concurrent_2_2_0.Map
      WebSocket.Id unison_http_3_6_0.websockets.WebSocket
  webSockets = systemfw_concurrent_2_2_0.Map.empty()
  rememberWebSocket :
    unison_http_3_6_0.websockets.WebSocket ->{IO} WebSocket.Id
  rememberWebSocket ioWs =
    wsId = rng do WebSocket.Id.Id (bytes 16)
    systemfw_concurrent_2_2_0.Map.put webSockets wsId ioWs
    wsId
  forgetWebSocket wsId = systemfw_concurrent_2_2_0.Map.delete webSockets wsId
  stdout : Signal [Text]
  stdout = Signal.new []
  reporter : '{IO} ('{IO} ())
  reporter = do
    loop = do
      msgs = change stdout do
        logs = Changes.read
        if List.isEmpty logs then Changes.retry
        else
          Changes.write []
          logs
      unsafeRun! do msgs |> foreach.deprecated printLine
      loop()
    thread = fork loop
    do unsafeRun! do concurrent.kill thread
  writeLog : ServiceHash () () -> 'Json ->{IO} ()
  writeLog serviceHash msg =
    use List :+
    json = msg()
    concurrent_1_1_0.Map.alter logs serviceHash cases
      None      -> (Put [json], ())
      Some logs -> (Put (logs :+ json), ())
    Signal.modify stdout (logs -> logs :+ Json.toText json)
  handleScratch : ∀ g a. Environment.Id -> '{g, Scratch} a ->{g, IO} a
  handleScratch = local.handler()
  handleTransaction :
    ∀ a.
      data.Map (Text, Any) Any
      -> '{Transaction, Exception, Random, Batch} a
      -> (data.Map (Text, Any) Any, Either Failure a)
  handleTransaction state p = match catch do implTransaction state p with
    Right (state', a) -> (state', Right a)
    Left err          -> (state, Left err)
  handleBlobs :
    (Database ->{g} Either Failure ())
    -> (∀ a. '{g, Blobs} a ->{g, IO, Remote} a)
  handleBlobs verifyOwnership =
    use Bytes empty
    use Database Id
    use List last
    use Nat - == increment min
    use Stream toList
    use Value value
    use concurrent_1_1_0.Map delete lookup
    use mutable.Ref read
    use systemfw_concurrent_2_2_0.Map alter put
    etag : Bytes -> ETag
    etag = hashBytes Md5 >> toHex >> (t -> "\"" Text.++ t Text.++ "\"") >> ETag
    ignore do
      {{
      This roughly approximates how big a serialized value would be, so the {{
      docLink (docEmbedTermLink do byteCount) }} values can be realistic for
      the local interpreter.
      }}
    approxBytesForValue : reflection.Value ->{IO, Exception} Bytes
    approxBytesForValue v =
      use Bytes ++
      use Value serialize
      deps = dependenciesRecursive v
      serialize v ++ serialize (value deps) |> zlib.compress
    keysWithPrefix :
      ∀ a.
        data.Map (Id, Key) (a, Metadata)
        -> Database
        -> Text
        -> Optional PageToken
        -> '{Stream Metadata} ()
    keysWithPrefix all db prefix pageToken =
      m =
        Optional.fold
          (do all)
          (token ->
            Map.split (Database.id db, Key (PageToken.toText token)) all |> at2)
          pageToken
      m
        |> Map.toStream
        |> (concatMap cases
             ((_, k), (_, meta)) ->
               if startsWith prefix (Key.toText k) then [meta] else [])
    doPrefixQuery :
      ∀ a.
        data.Map (Id, Key) (a, Metadata)
        -> Database
        -> Optional Nat
        -> Text
        -> Optional PageToken
        -> PrefixQueryResults
    doPrefixQuery all db userMaxResults prefix pageToken =
      use List size
      maxResults = Optional.fold (do 1000) (min 1000) userMaxResults
      batchPlusOne =
        keysWithPrefix all db prefix pageToken
          |> Stream.take (increment maxResults)
          |> toList
      batch = List.take maxResults batchPlusOne
      nextPageToken =
        if size batch == size batchPlusOne then None
        else
          last batch
            |> Optional.map
              (meta -> PageToken (Key.toText (Metadata.key meta)))
      PrefixQueryResults nextPageToken batch
    doPrefixList :
      ∀ a.
        data.Map (Id, Key) (a, Metadata)
        -> Database
        -> Optional Nat
        -> Text
        -> Text
        -> Optional PageToken
        -> PrefixListResults
    doPrefixList all db userMaxResults delimiter prefix pageToken =
      use Optional fold
      keyToPrefix keyText =
        use Text ++
        afterPrefix = Text.drop (Text.size prefix) keyText
        Optional.map
          (i -> PrefixResult (prefix ++ Text.take i afterPrefix ++ delimiter))
          (Text.indexOf delimiter afterPrefix)
      toResult : Metadata -> PrefixListResult
      toResult meta =
        keyText = Key.toText (Metadata.key meta)
        keyToPrefix keyText |> (getOrElse' do BlobResult meta)
      prevCommonPrefixRes =
        flatMap keyToPrefix (Optional.map PageToken.toText pageToken)
      maxResults = fold (do 1000) (min 1000) userMaxResults
      batchPlusOne =
        keysWithPrefix all db prefix pageToken
          |> Stream.map (m -> toResult m)
          |> Stream.dropWhile (fold (do const false) (===) prevCommonPrefixRes)
          |> changes true
          |> Stream.take (increment maxResults)
          |> toList
      batch = List.take maxResults batchPlusOne
      nextPageToken =
        if List.size batch == List.size batchPlusOne then None
        else
          last batch |> (Optional.map cases
            PrefixResult p  -> PageToken p
            BlobResult meta -> PageToken (Key.toText (Metadata.key meta)))
      PrefixListResults nextPageToken batch
    go : ∀ a. '{g, Blobs, Remote} a ->{g, IO, Remote} a
    go thunk =
      handle thunk()
      with cases
        { a } -> a
        { bytes.tryList.impl db maxResults delimiter prefix pageToken -> k } ->
          byteBlobsSnapshot =
            (Map ref) = byteBlobs
            read ref
          res =
            mapRight
              (_ ->
                doPrefixList
                  byteBlobsSnapshot db maxResults delimiter prefix pageToken)
              (verifyOwnership db)
          go do k res
        { bytes.tryReadMetadata db key -> k } ->
          go do bytes.tryRead db key |> mapRight (Optional.map at2) |> k
        { typed.tryReadMetadata db key -> k } ->
          go do typed.tryRead db key |> mapRight (Optional.map at2) |> k
        { bytes.tryPrefixQuery db maxResults prefix pageToken -> k } ->
          byteBlobsSnapshot =
            (Map ref) = byteBlobs
            read ref
          res =
            mapRight
              (_ ->
                doPrefixQuery byteBlobsSnapshot db maxResults prefix pageToken)
              (verifyOwnership db)
          go do k res
        { typed.tryPrefixQuery db maxResults prefix pageToken -> k } ->
          typedBlobsSnapshot =
            (Map ref) = typedBlobs
            read ref
          res =
            mapRight
              (_ ->
                doPrefixQuery typedBlobsSnapshot db maxResults prefix pageToken)
              (verifyOwnership db)
          go do k res
        { typed.tryList.impl db maxResults delimiter prefix pageToken -> k } ->
          typedBlobsSnapshot =
            (Map ref) = typedBlobs
            read ref
          res =
            mapRight
              (_ ->
                doPrefixList
                  typedBlobsSnapshot db maxResults delimiter prefix pageToken)
              (verifyOwnership db)
          go do k res
        { tryWriteStreaming db key contentStream -> k } ->
          go do
            use Bytes ++
            content = Stream.fold (++) empty contentStream
            bytes.tryWrite db key content |> k
        { tryCreateStreaming db key contentStream -> k } ->
          go do
            use Bytes ++
            content = Stream.fold (++) empty contentStream
            bytes.tryCreate db key content |> k
        { bytes.tryRead db key -> k } ->
          run = do lookup byteBlobs (Database.id db, key)
          res = mapRight run (verifyOwnership db)
          go do k res
        { bytes.tryDelete db key -> k } ->
          res =
            mapRight
              (_ -> delete byteBlobs (Database.id db, key))
              (verifyOwnership db)
          go do k res
        { typed.tryDelete db key -> k } ->
          res =
            mapRight
              (_ -> delete typedBlobs (Database.id db, key))
              (verifyOwnership db)
          go do k res
        { typed.tryCreate db key userValue -> k } ->
          run =
            do
              use Text ++
              serialized = approxBytesForValue (value userValue)
              tag = etag serialized
              meta = Metadata key tag (Bytes.size serialized) now!
              res =
                alter typedBlobs (Database.id db, key) cases
                  None -> (Put (Any userValue, meta), Right tag)
                  Some _ ->
                    msg =
                      "Error creating typed blob: an object already exists at the provided key ("
                        ++ Key.toText key
                        ++ ")."
                    e = Failure (typeLink Conflict) msg (Any (key : Key))
                    (NoChange, Left e)
              Either.toException res
          res = flatMapRight (x -> catch do run x) (verifyOwnership db)
          go do k res
        { typed.tryWrite db key userValue -> k } ->
          run = do
            serialized = approxBytesForValue (value userValue)
            tag = etag serialized
            meta = Metadata key tag (Bytes.size serialized) now!
            put typedBlobs (Database.id db, key) (Any userValue, meta)
            tag
          res = flatMapRight (x -> catch do run x) (verifyOwnership db)
          go do k res
        { typed.tryRead db key -> k } ->
          run =
            do
              lookup typedBlobs (Database.id db, key)
                |> (Optional.map cases (any, meta) -> (unsafeExtract any, meta))
          res = mapRight run (verifyOwnership db)
          go do k res
        { tryReadRange db key start endInclusive -> k } ->
          go do
            bytes.tryRead db key
              |> mapRight
                   (Optional.map cases
                     (bs, meta) ->
                       ( Bytes.drop start bs
                           |> Bytes.take (increment (endInclusive - start))
                       , Metadata.etag meta
                       ))
              |> k
        { bytes.tryCreate db key content -> k } ->
          run =
            do
              use Text ++
              tag = etag content
              meta = Metadata key tag (Bytes.size content) now!
              alter byteBlobs (Database.id db, key) cases
                None -> (Put (content, meta), Right tag)
                Some _ ->
                  msg =
                    "Error creating bytes blob: an object already exists at the provided key ("
                      ++ Key.toText key
                      ++ ")."
                  e = Failure (typeLink Conflict) msg (Any (key : Key))
                  (NoChange, Left e)
          res = flatMapRight run (verifyOwnership db)
          go do k res
        { bytes.tryWrite db key content -> k } ->
          run = do
            tag = etag content
            meta = Metadata key tag (Bytes.size content) now!
            put byteBlobs (Database.id db, key) (content, meta)
            tag
          res = mapRight run (verifyOwnership db)
          go do k res
    go
  handleWebSockets :
    ∀ i g a. run.Interrupt -> (i ->{g, WebSockets} a) -> i ->{g, IO} a
  handleWebSockets interrupt =
    use Text ++
    use concurrent_1_1_0.Map lookup
    use websockets.WebSocket WebSocket
    unknownWebSocket = cases
      wsId@(WebSocket.Id.Id idBytes) ->
        Failure
          (typeLink UnknownWebSocket)
          ("unknown websocket: " ++ toHex idBytes)
          (Any wsId)
    go : ∀ i a. (i ->{g, WebSockets} a) -> i ->{g, IO} a
    go f i =
      handle f i
      with cases
        { a } -> a
        { tryReceive (WebSocket _ wsId) -> k } ->
          res =
            match lookup webSockets wsId with
              Some ws ->
                catchAll do
                  toDefault! (do raise cancelled) do
                    interruptibly interrupt do WebSocket.receive ws
              None -> Left (unknownWebSocket wsId)
          go k res
        { tryClose (WebSocket _ wsId) -> k } ->
          res = match lookup webSockets wsId with
            Some ws -> catchAll do WebSocket.close ws
            None    -> Right()
          go k res
        { trySend (WebSocket _ wsId) msg -> k } ->
          res = match lookup webSockets wsId with
            Some ws -> catchAll do WebSocket.send ws msg
            None    -> Left (unknownWebSocket wsId)
          go k res
    go
  handlePool :
    ∀ g a.
      (ServiceHash () (), Environment)
      -> run.Interrupt
      -> '{g,
      Environment.Config,
      Http,
      Blobs,
      Services,
      Storage,
      websockets.HttpWebSocket,
      WebSockets,
      Log,
      Scratch} a
      ->{g, IO, Exception, Abort, Remote} a
  handlePool ctx interrupt p =
    (serviceHash, env) = ctx
    handle
      (do handleScratch (Environment.id env) p)
        |> handleWebSockets interrupt
        |> handleBlobs (verifyDatabaseAccess env)
    with cases
      { a } -> a
      { Config.lookup k -> resume } ->
        (_, _, configs, _) = Ref.read controlPlane
        out = configs |> get env |> flatMap (get k)
        handlePool ctx interrupt do resume out
      { tryRequest req -> resume } ->
        response =
          interruptibly interrupt do handle tryRequest req with Http.handler
        handlePool ctx interrupt do resume response
      { websockets.HttpWebSocket.tryWebSocket req -> resume } ->
        response =
          interruptibly interrupt do
            (handle client.HttpWebSocket.tryWebSocket req
            with HttpWebSocket.handler)
              |> mapRight
                (rememberWebSocket
                  >> WebSocket.WebSocket (LocationId (UID 0xs)))
        handlePool ctx interrupt do resume response
      { debugLazyJson msg -> resume } ->
        writeLog serviceHash msg
        handlePool ctx interrupt resume
      { Log.lazyJson msg -> resume } ->
        writeLog serviceHash msg
        handlePool ctx interrupt resume
      { tryCall serviceHash in -> resume } ->
        untypedServiceHash = untyped serviceHash
        result =
          catch do
            (env, untypedService) =
              Map.lookup services untypedServiceHash
                |> (getOrElse' do raise (unknownServiceHash serviceHash))
            ctx' = (untypedServiceHash, env)
            runRemote :
              ∀ a.
                (ServiceHash () (), Environment)
                -> run.Interrupt
                -> '{Remote} a
                ->{IO, Exception} a
            runRemote =
              rng do
                threaded
                  (ctx interrupt -> coerceAbilities (handlePool ctx interrupt))
            runRemote ctx' interrupt do
              untypedService (Any in) |> unsafeExtract
        handlePool ctx interrupt do resume result
      { tryResolve service -> resume } ->
        out =
          Ref.read serviceNames
            |> at2
            |> get (ServiceName.id service)
            |> (cases
                 None ->
                   Left
                     (Failure
                       (typeLink UnknownService)
                       "This service is not associated with any serviceHash"
                       (Any service))
                 Some h -> Right (ServiceHash (ServiceHash.toText h)))
        handlePool ctx interrupt do resume out
      { tryCallByName service in -> resume } ->
        out =
          handlePool ctx interrupt do
            tryResolve service |> flatMapRight (h -> tryCall h in)
        handlePool ctx interrupt do resume out
      { tryTransact db transaction -> resume } ->
        out =
          (_, _, _, dbPermissions) = Ref.read controlPlane
          if dbPermissions |> Set.contains (env, db) then
            rng2 = rng do split!
            concurrent_1_1_0.Map.alter state db cases
              None -> (NoChange, failure "Database does not exist" db |> Left)
              Some dbState ->
                (dbState', out) = rng2 do handleTransaction dbState transaction
                (Put dbState', out)
          else
            Left
              (Failure
                (typeLink RestrictedOperation)
                "This environment cannot access the requested database"
                (Any (env, db)))
        handlePool ctx interrupt do resume out
      { tryBatchRead db p -> resume } ->
        out =
          (_, _, _, dbPermissions) = Ref.read controlPlane
          if dbPermissions |> Set.contains (env, db) then
            rng2 = rng do split!
            concurrent_1_1_0.Map.alter state db cases
              None -> (NoChange, failure "Database does not exist" db |> Left)
              Some dbState ->
                (_, out) = rng2 do handleTransaction dbState p
                (NoChange, out)
          else
            Left
              (Failure
                (typeLink RestrictedOperation)
                "This environment cannot access the requested database"
                (Any (env, db)))
        handlePool ctx interrupt do resume out
  getHash : ∀ a b. Environment -> (a ->{Remote} b) -> ServiceHash a b
  getHash service env =
    crypto.hash Blake2b_256 (service, env) |> toHex |> ServiceHash
  untyped : ∀ a b. ServiceHash a b -> ServiceHash () ()
  untyped = cases ServiceHash t -> ServiceHash t
  runRemote :
    ∀ a.
      (ServiceHash () (), Environment)
      -> run.Interrupt
      -> '{Remote} a
      ->{IO, Exception} a
  runRemote =
    threaded (ctx interrupt -> coerceAbilities (handlePool ctx interrupt))
  go : '{g, Cloud} a ->{g, IO, Exception} a
  go p =
    handle p()
    with cases
      { a } -> a
      { submit.impl env thunk -> resume } ->
        serviceHash = getHash env thunk |> untyped
        ctx = (serviceHash, env)
        out = catch do runRemote ctx Interrupt.new() thunk
        go do resume out
      { deploy.impl env service -> resume } ->
        serviceHash = getHash env service
        untypedServiceHash = serviceHash |> untyped
        untypedService : Any ->{Remote} Any
        untypedService input = Any (service (unsafeExtract input))
        concurrent_1_1_0.Map.put
          services untypedServiceHash (env, untypedService)
        go do resume (Right serviceHash)
      { undeploy.impl serviceHash -> resume } ->
        concurrent_1_1_0.Map.delete services (untyped serviceHash)
        go do resume Right()
      { exposeHttpWebSocket.impl serviceHash -> resume } ->
        use Text ++
        atomically exposedv2 cases
          exposed -> (exposed |> Set.insert serviceHash, ())
        uri = httpBaseUri / "h" / ServiceHash.toText serviceHash
        banner =
          "Service exposed successfully at:" ++ "\n  " ++ URI.toText uri ++ "/"
        when printBanners do printLine banner
        go do resume (Right uri)
      { exposeHttp.impl serviceHash -> resume } ->
        use Text ++
        atomically exposedv1 cases
          exposed -> (exposed |> Set.insert serviceHash, ())
        uri = httpBaseUri / "h" / ServiceHash.toText serviceHash
        banner =
          "Service exposed successfully at:" ++ "\n  " ++ URI.toText uri ++ "/"
        when printBanners do printLine banner
        go do resume (Right uri)
      { unexposeHttpWebSocket.impl serviceHash -> resume } ->
        atomically exposedv2 cases
          exposed -> (exposed |> Set.delete serviceHash, ())
        go do resume Right()
      { unexposeHttp.impl serviceHash -> resume } ->
        atomically exposedv1 cases
          exposed -> (exposed |> Set.delete serviceHash, ())
        go do resume Right()
      { logs.impl _ -> resume } ->
        use List ++
        out = concurrent.Map.foldLeft (++) [] logs
        go do resume (Right out)
      { Environment.create.impl name -> resume } ->
        use Map insert
        out =
          catch do
            validateName name
            freshId = Environment.Id.Id newId()
            atomically controlPlane cases
              s@(envNames, dbNames, configs, dbPermissions) ->
                match envNames |> get name with
                  Some env -> (s, env)
                  None ->
                    env = Environment freshId name
                    ( ( envNames |> insert name env
                      , dbNames
                      , configs |> insert env data.Map.empty
                      , dbPermissions
                      )
                    , env
                    )
        go do resume out
      { Environment.delete.impl env -> resume } ->
        (Environment _ name) = env
        use data.Map delete
        atomically controlPlane cases
          (envNames, dbNames, configs, dbPermissions) ->
            ( ( envNames |> delete name
              , dbNames
              , configs |> delete env
              , dbPermissions |> (filter cases (env', _) -> env' !== env)
              )
            , ()
            )
        go do resume Right()
      { list.impl -> resume } ->
        out = Ref.read controlPlane |> at1 |> values
        go do resume (Right out)
      { setValue.impl env k v -> resume } ->
        use Map insert
        out =
          atomically controlPlane cases
            s@(envNames, dbNames, configs, dbPermissions) ->
              match configs |> get env with
                None -> (s, Left (failure "Environment not found" env))
                Some config ->
                  ( ( envNames
                    , dbNames
                    , configs |> insert env (config |> insert k v)
                    , dbPermissions
                    )
                  , Right()
                  )
        go do resume out
      { deleteValue.impl env k -> resume } ->
        atomically controlPlane cases
          s@(envNames, dbNames, configs, dbPermissions) ->
            match configs |> get env with
              None -> (s, ())
              Some config ->
                ( ( envNames
                  , dbNames
                  , configs |> Map.insert env (config |> data.Map.delete k)
                  , dbPermissions
                  )
                , ()
                )
        go do resume Right()
      { Database.assign.impl db env -> resume } ->
        out =
          atomically controlPlane cases
            s@(envNames, dbNames, configs, dbPermissions) ->
              if dbNames |> Map.contains (Database.name db) then
                ( ( envNames
                  , dbNames
                  , configs
                  , dbPermissions |> Set.insert (env, db)
                  )
                , Right()
                )
              else (s, Left (failure "Database does not exist" db))
        go do resume out
      { Database.unassign.impl db env -> resume } ->
        atomically controlPlane cases
          (envNames, dbNames, configs, dbPermissions) ->
            ( ( envNames
              , dbNames
              , configs
              , dbPermissions |> Set.delete (env, db)
              )
            , ()
            )
        go do resume Right()
      { Database.create.impl name -> resume } ->
        out =
          catch do
            validateName name
            freshId = Database.Id.Id newId()
            db =
              atomically controlPlane cases
                s@(envNames, dbNames, configs, dbPermissions) ->
                  match dbNames |> get name with
                    Some db -> (s, db)
                    None ->
                      db = Database freshId name
                      ( ( envNames
                        , dbNames |> Map.insert name db
                        , configs
                        , dbPermissions
                        )
                      , db
                      )
            concurrent_1_1_0.Map.alter state db cases
              Some _ -> (NoChange, ())
              None   -> (Put data.Map.empty, ())
            db
        go do resume out
      { Database.delete.impl db -> resume } ->
        atomically controlPlane cases
          (envNames, dbNames, configs, dbPermissions) ->
            ( ( envNames
              , dbNames |> data.Map.delete (Database.name db)
              , configs
              , dbPermissions |> (filter cases (_, db') -> db !== db')
              )
            , ()
            )
        go do resume Right()
      { ServiceName.create.impl name -> resume } ->
        out =
          catch do
            validateName name
            freshId = ServiceName.Id.Id newId()
            atomically serviceNames cases
              s@(names, mappings) ->
                match names |> get name with
                  Some id -> (s, ServiceName id name)
                  None ->
                    ( (names |> Map.insert name freshId, mappings)
                    , ServiceName freshId name
                    )
        go do resume out
      { ServiceName.delete.impl service -> resume } ->
        use data.Map delete
        atomically serviceNames cases
          (names, mappings) ->
            ( ( names |> delete (ServiceName.name service)
              , mappings |> delete (ServiceName.id service)
              )
            , ()
            )
        go do resume Right()
      { ServiceName.assign.impl service hash -> resume } ->
        use Text ++
        deployedAt = httpBaseUri / "s" / ServiceName.name service
        out =
          match Map.lookup services (untyped hash) with
            None ->
              failure "The service hash is not currently deployed" hash |> Left
            Some _ ->
              atomically serviceNames cases
                (names, mappings) ->
                  ( ( names
                    , mappings
                        |> Map.insert (ServiceName.id service) (untyped hash)
                    )
                  , Right deployedAt
                  )
        banner =
          "Service with hash: "
            ++ ServiceHash.toText hash
            ++ " now available at: "
            ++ "\n  "
            ++ URI.toText deployedAt
            ++ "/"
        when printBanners do printLine banner
        go do resume out
      { ServiceName.unassign.impl service -> resume } ->
        atomically serviceNames cases
          (names, mappings) ->
            ((names, mappings |> data.Map.delete (ServiceName.id service)), ())
        go do resume Right()
      { Daemon.assign.impl daemon daemonHash -> resume } ->
        use Map insert
        use Text ++
        freshInterrupt = Interrupt.new()
        daemonInfo :
          Either
            Failure
            (Optional (Optional run.Interrupt, Environment, '{Remote} ()))
        daemonInfo =
          atomically daemons cases
            s@(hashes, names, ids) ->
              daemonId = Daemon.id daemon
              match get daemonHash hashes with
                None ->
                  ( s
                  , Left
                      (failure
                        "The provided daemon hash does not exist" daemonHash)
                  )
                Some (env, run) ->
                  match get daemonId ids with
                    None ->
                      ( s
                      , Left
                          (failure
                            "The provided daemon ID does not exist" daemon)
                      )
                    Some None ->
                      ( ( hashes
                        , names
                        , insert
                            daemonId (Some (daemonHash, freshInterrupt)) ids
                        )
                      , Right (Some (None, env, run))
                      )
                    Some (Some (oldHash, oldInterrupt)) ->
                      if oldHash === daemonHash then (s, Right None)
                      else
                        ( ( hashes
                          , names
                          , insert
                              daemonId (Some (daemonHash, freshInterrupt)) ids
                          )
                        , Right (Some (Some oldInterrupt, env, run))
                        )
        res =
          match daemonInfo with
            Left f -> Left f
            Right None -> Right()
            Right (Some (oldInterrupt, env, run)) ->
              ignore
                <| (fork do
                  Optional.fold
                    (do ()) interruptAndAwaitFinalization oldInterrupt
                  serviceHash =
                    (DaemonHash t) = daemonHash
                    ServiceHash t
                  match catch do
                    runRemote (serviceHash, env) freshInterrupt run with
                    Right () -> ()
                    Left e ->
                      if failureType e === typeLink Cancelled then ()
                      else
                        Debug.trace
                          ("Exception in daemon " ++ Daemon.name daemon) e
                  signalFinalization freshInterrupt)
              Right()
        go do resume res
      { Daemon.create.impl name -> resume } ->
        use Map insert
        out =
          catch do
            validateName name
            freshId = Daemon.Id.Id newId()
            atomically daemons cases
              s@(hashes, names, ids) ->
                match names |> get name with
                  Some id -> (s, Daemon id name)
                  None ->
                    ( ( hashes
                      , names |> insert name freshId
                      , ids |> insert freshId None
                      )
                    , Daemon freshId name
                    )
        go do resume out
      { Daemon.delete.impl daemon -> resume } ->
        use data.Map delete
        daemonId = Daemon.id daemon
        daemonInterrupt =
          atomically daemons cases
            (hashes, names, ids) ->
              daemonInterrupt = get daemonId ids |> flatMap (Optional.map at2)
              ( ( hashes
                , names |> delete (Daemon.name daemon)
                , ids |> delete daemonId
                )
              , daemonInterrupt
              )
        match daemonInterrupt with
          Some (Interrupt.Interrupt triggerInterruption _) ->
            write_ triggerInterruption ()
          None -> ()
        go do resume Right()
      { Daemon.unassign.impl daemon -> resume } ->
        daemonId = Daemon.id daemon
        daemonInterrupt = atomically daemons cases
          (hashes, names, ids) ->
            daemonInterrupt = get daemonId ids |> flatMap (Optional.map at2)
            ((hashes, names, ids |> Map.insert daemonId None), daemonInterrupt)
        match daemonInterrupt with
          Some (Interrupt.Interrupt triggerInterruption _) ->
            write_ triggerInterruption ()
          None -> ()
        go do resume Right()
      { DaemonHash.create.impl environment run -> resume } ->
        daemonHash =
          getHash environment run
            |> (cases ServiceHash hash -> DaemonHash hash)
        atomically daemons cases
          (hashes, names, ids) ->
            ((Map.insert daemonHash (environment, run) hashes, names, ids), ())
        go do resume (Right daemonHash)
      { DaemonHash.delete.impl daemonHash -> resume } ->
        use data.Map delete
        daemonInterrupt =
          atomically daemons cases
            (hashes, names, running) ->
              live =
                running
                  |> Map.toList
                  |> (findMap cases
                       (daemonId, Some (hash, interrupt))| hash === daemonHash  ->
                         Some (daemonId, interrupt)
                       _ -> None)
              let
                (daemonInterrupt, running') =
                  match live with
                    None -> (None, running)
                    Some (daemonId, interrupt) ->
                      (Some interrupt, delete daemonId running)
                ( (hashes |> delete daemonHash, names, running')
                , daemonInterrupt
                )
        match daemonInterrupt with
          Some (Interrupt.Interrupt triggerInterruption _) ->
            write_ triggerInterruption ()
          None -> ()
        go do resume Right()
  httpServer =
    do
      handler :
        HttpRequest ->{IO, Exception} Either HttpResponse WebSocketHandler
      handler =
        use HttpRequest setHeader
        use HttpResponse notFound
        use Text ++
        use mutable.Ref read
        serveHash :
          Text
          -> [Text]
          -> HttpRequest
          ->{IO, Exception} Either HttpResponse WebSocketHandler
        serveHash hash userPath r =
          use Interrupt new
          use Set contains
          normalisedPath = match userPath with
            [] -> [""]
            p  -> p
          request = r |> uri.modify (path.set (Path normalisedPath))
          serviceHashv1 : ServiceHash HttpRequest HttpResponse
          serviceHashv1 = ServiceHash hash
          serviceHashv2 :
            ServiceHash
              HttpRequest
              (Either
                HttpResponse (websockets.WebSocket ->{Remote, WebSockets} ()))
          serviceHashv2 = ServiceHash hash
          untypedServiceHash = untyped (ServiceHash hash)
          match Map.lookup services untypedServiceHash with
            Some (env, untypedService)
              | read exposedv1 |> contains serviceHashv1  ->
                service : HttpRequest ->{Remote} HttpResponse
                service req = untypedService (Any req) |> unsafeExtract
                (runRemote (untypedServiceHash, env) new() do service request)
                  |> Left
              | read exposedv2 |> contains serviceHashv2  ->
                service :
                  HttpRequest
                  ->{Remote} Either
                    HttpResponse
                    (websockets.WebSocket ->{Remote, WebSockets} ())
                service req = untypedService (Any req) |> unsafeExtract
                interrupt = new()
                match runRemote (untypedServiceHash, env) interrupt do
                  service request with
                  Left response       -> Left response
                  Right userWsHandler ->
                    serverWsHandler ioWs addFinalizer =
                      wsId = rememberWebSocket ioWs
                      addFinalizer (_ -> forgetWebSocket wsId)
                      runRemote (untypedServiceHash, env) interrupt do
                        locationId = Location.locationId here!
                        userWs = WebSocket.WebSocket locationId wsId
                        toRemote do userWsHandler userWs
                    WebSocketHandler serverWsHandler |> Right
            _ -> Left notFound
        cases
          origReq@(HttpRequest
            _ _ (URI _ _ (Path (["h", hash] ++ userPath)) _ _) _ _) ->
            req = origReq |> setHeader "Unison-Cloud-Base-Path" ["/h/" ++ hash]
            resp : Either HttpResponse WebSocketHandler
            resp = serveHash hash userPath req
            resp
          origReq@(HttpRequest
            _ _ (URI _ _ (Path (["s", serviceName] ++ userPath)) _ _) _ _) ->
            (names, hashes) = read serviceNames
            match get serviceName names with
              None -> Left notFound
              Some id ->
                match get id hashes with
                  None -> Left notFound
                  Some (ServiceHash hash) ->
                    req =
                      origReq
                        |> setHeader
                          "Unison-Cloud-Base-Path" ["/s/" ++ serviceName]
                    serveHash hash userPath req
          _ -> Left notFound
      forkServer
        (server.Config.Config None (httpPort localConfig) 1000 None) handler
  stopReporter = reporter()
  stopServer = httpServer()
  stopDaemons =
    do
      (_, _, activeDaemons) = Ref.read daemons
      activeDaemons
        |> values
        |> List.somes
        |> (List.foreach cases
             (_, interrupt) -> interruptAndAwaitFinalization interrupt)
  finalizer = do
    stopDaemons()
    stopReporter()
    stopServer()
    signalFinalization interrupt
  handle interruptibly interrupt do go p
  with cases
    { Abort.abort -> _ } ->
      finalizer()
      raise cancelled
    { a }                ->
      finalizer()
      a

Cloud.run.local.LocalCloudConfig.default : '{IO} LocalCloudConfig
Cloud.run.local.LocalCloudConfig.default = do
  use getEnv impl
  bindHost = match impl "UNISON_CLOUD_LOCAL_HTTP_HOST" with
    Right hostName -> Some (HostName hostName)
    Left _         -> None
  bindPort = match impl "UNISON_CLOUD_LOCAL_HTTP_PORT" with
    Right port -> Port port
    Left _     -> Port "8080"
  LocalCloudConfig false bindHost bindPort

Cloud.run.local.LocalCloudConfig.httpHost :
  LocalCloudConfig -> Optional HostName
Cloud.run.local.LocalCloudConfig.httpHost = cases
  LocalCloudConfig _ httpHost _ -> httpHost

Cloud.run.local.LocalCloudConfig.httpHost.modify :
  (Optional HostName ->{g} Optional HostName)
  -> LocalCloudConfig
  ->{g} LocalCloudConfig
Cloud.run.local.LocalCloudConfig.httpHost.modify f = cases
  LocalCloudConfig printBanners httpHost httpPort ->
    LocalCloudConfig printBanners (f httpHost) httpPort

Cloud.run.local.LocalCloudConfig.httpHost.set :
  Optional HostName -> LocalCloudConfig -> LocalCloudConfig
Cloud.run.local.LocalCloudConfig.httpHost.set httpHost1 = cases
  LocalCloudConfig printBanners _ httpPort ->
    LocalCloudConfig printBanners httpHost1 httpPort

Cloud.run.local.LocalCloudConfig.httpPort : LocalCloudConfig -> Port
Cloud.run.local.LocalCloudConfig.httpPort = cases
  LocalCloudConfig _ _ httpPort -> httpPort

Cloud.run.local.LocalCloudConfig.httpPort.modify :
  (Port ->{g} Port) -> LocalCloudConfig ->{g} LocalCloudConfig
Cloud.run.local.LocalCloudConfig.httpPort.modify f = cases
  LocalCloudConfig printBanners httpHost httpPort ->
    LocalCloudConfig printBanners httpHost (f httpPort)

Cloud.run.local.LocalCloudConfig.httpPort.set :
  Port -> LocalCloudConfig -> LocalCloudConfig
Cloud.run.local.LocalCloudConfig.httpPort.set httpPort1 = cases
  LocalCloudConfig printBanners httpHost _ ->
    LocalCloudConfig printBanners httpHost httpPort1

Cloud.run.local.LocalCloudConfig.printBanners : LocalCloudConfig -> Boolean
Cloud.run.local.LocalCloudConfig.printBanners = cases
  LocalCloudConfig printBanners _ _ -> printBanners

Cloud.run.local.LocalCloudConfig.printBanners.modify :
  (Boolean ->{g} Boolean) -> LocalCloudConfig ->{g} LocalCloudConfig
Cloud.run.local.LocalCloudConfig.printBanners.modify f = cases
  LocalCloudConfig printBanners httpHost httpPort ->
    LocalCloudConfig (f printBanners) httpHost httpPort

Cloud.run.local.LocalCloudConfig.printBanners.set :
  Boolean -> LocalCloudConfig -> LocalCloudConfig
Cloud.run.local.LocalCloudConfig.printBanners.set printBanners1 = cases
  LocalCloudConfig _ httpHost httpPort ->
    LocalCloudConfig printBanners1 httpHost httpPort

Cloud.run.local.serve : '{IO, Exception, Cloud} a ->{IO, Exception} a
Cloud.run.local.serve p =
  use concurrent.Promise read write_
  text =
    """
      Local Cloud computation started in the background, press Enter to stop it.

    """
  interrupt = Interrupt.new()
  result = Promise.new()
  _ =
    concurrent.fork do
      catchAll do
        p' =
          do
            res = p()
            write_ result (Some (Right res))
            completionMsg =
              """
                Local Cloud computation complete. press Enter to stop local server.

              """
            printLine completionMsg
            let
              (run.Interrupt.Interrupt int _) = interrupt
              read int
        config = LocalCloudConfig.default() |> printBanners.set true
        splitmix base.IO.randomNat() do
          match catch do local.impl config interrupt p' with
            Left (Failure t _ _)| t === typeLink Cancelled  ->
              write_ result None
            Left f -> write_ result (Some (Left f))
            Right () -> ()
  printLine text
  _ = readLine()
  run.Interrupt.interruptAndAwaitFinalization interrupt
  match read result with
    Some r -> Either.toException r
    None ->
      Exception.raise
        (Failure
          (typeLink Cancelled) "Local Cloud computation interrupted." (Any ()))

Cloud.run.local.serve.doc : Doc
Cloud.run.local.serve.doc =
  use Cloud.run local
  {{
  Runs a {type Cloud} computation locally, pausing to read a line before
  exiting. Typically used for launching services locally for interactive
  testing, since once the {local} function exits, all services will be shut
  down.

  # Configuration

    {{ configuration }}

    **See also:**

    * {local} which doesn't pause to read a line before exiting
    * {Cloud.run} which interprets {type Cloud} in Unison Cloud, instead of
      locally
  }}

Cloud.run.withConfig : Cloud.ClientConfig -> '{g, Cloud} a ->{g, IO} a
Cloud.run.withConfig config p =
  use ClientConfig token
  use Decoder array run text
  use Http configuredHandler
  use HttpRequest delete get post
  use Path /
  use Text ++
  use object at at!
  deploymentsPath = root / "v2" / "deployments"
  servicesPath = root / "v2" / "services"
  environmentsPath = root / "v2" / "environments"
  databasePath = root / "v2" / "storage"
  daemonsPath = root / "v2" / "daemons"
  daemonHashesPath = root / "v2" / "daemon-hashes"
  decodeText res = res |> HttpResponse.body |> Body.toBytes |> fromUtf8
  checkSandbox : Text -> reflection.Value ->{IO, Exception} ()
  checkSandbox =
    use Exception raise
    use Link Term
    allowedTerms = [termLink toDebugText.impl]
    msg userValue ->
      (match validateSandboxed allowedTerms userValue with
        Left missingDependencies ->
          raise
            (Failure
              (typeLink SerializationError)
              "While checking for restricted operations, the runtime requested the code for terms that it should have already known about. Please report this bug."
              (Any (missingDependencies : [Term])))
        Right [] -> ()
        Right sandboxViolations ->
          raise
            (Failure
              (typeLink RestrictedOperation)
              (msg ++ " Restricted operations include IO and Debug.trace.")
              (Any ("Restricted terms:", (sandboxViolations : [Term])))))
  go : ∀ g b. '{g, Cloud} b ->{g, IO} b
  go p =
    handle p()
    with cases
      { a } -> a
      { deploy.impl env service -> resume } ->
        result =
          catch do
            checkSandbox
              "Cannot deploy a service that performs restricted operations."
              (Value.value service)
            reqBody = Body (encode.toBytes do encodeService service)
            query =
              Query.empty
                & ( "environmentId"
                , env |> Environment.id |> Environment.Id.toText
                )
            path = deploymentsPath / "create"
            uri = httpUri config path (fromQuery query)
            req = post uri reqBody |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            context =
              "deploying a service to environment " ++ Environment.toText env
            expectApiSuccess context res
            res |> decodeText |> ServiceHash
        go do resume result
      { undeploy.impl serviceHash -> resume } ->
        use ServiceHash toText
        result = catch do
          path = deploymentsPath / toText serviceHash
          uri = httpUri config path RawQuery.empty
          req = delete uri |> addAuthHeader (token config)
          res = handle request req with configuredHandler (httpConfig config)
          context = "undeploying service with hash " ++ toText serviceHash
          expectApiSuccess context res
        go do resume result
      { exposeHttpWebSocket.impl serviceHash -> resume } ->
        result =
          catch do
            path = deploymentsPath / "expose" / ServiceHash.toText serviceHash
            query = Query.empty & ("httpServiceVersion", "1")
            uri = httpUri config path (fromQuery query)
            req = post uri Body.empty |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            context =
              "exposing HTTP/WebSocket service with hash "
                ++ ServiceHash.toText serviceHash
            expectApiSuccess context res
            deployedAt =
              res |> decodeText |> run (at "uri" uriStripTrailingSlash)
            banner =
              "Service exposed successfully at:"
                ++ "\n  "
                ++ URI.toText deployedAt
                ++ "/"
            printLine banner
            deployedAt
        go do resume result
      { exposeHttp.impl serviceHash -> resume } ->
        result =
          catch do
            path = deploymentsPath / "expose" / ServiceHash.toText serviceHash
            query = Query.empty & ("httpServiceVersion", "0")
            uri = httpUri config path (fromQuery query)
            req = post uri Body.empty |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            context =
              "exposing HTTP service with hash "
                ++ ServiceHash.toText serviceHash
            expectApiSuccess context res
            deployedAt =
              res |> decodeText |> run (at "uri" uriStripTrailingSlash)
            banner =
              "Service exposed successfully at:"
                ++ "\n  "
                ++ URI.toText deployedAt
                ++ "/"
            printLine banner
            deployedAt
        go do resume result
      { unexposeHttpWebSocket.impl serviceHash -> resume } ->
        use ServiceHash toText
        result =
          catch do
            path = deploymentsPath / "unexpose" / toText serviceHash
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            context =
              "unexposing HTTP/WebSocket service with hash "
                ++ toText serviceHash
            expectApiSuccess context res
        go do resume result
      { unexposeHttp.impl serviceHash -> resume } ->
        use ServiceHash toText
        result = catch do
          path = deploymentsPath / "unexpose" / toText serviceHash
          uri = httpUri config path RawQuery.empty
          req = delete uri |> addAuthHeader (token config)
          res = handle request req with configuredHandler (httpConfig config)
          context = "exposing HTTP service with hash " ++ toText serviceHash
          expectApiSuccess context res
        go do resume result
      { submit.impl env thunk -> resume } ->
        fetchLogs : QueryOptions ->{IO, Exception} [Json]
        fetchLogs options = go do logs options
        result =
          catch do
            bracket
              (do internal.connect config)
              Connection.close
              (connection ->
                runThunk (token config) fetchLogs env connection thunk)
        go do resume result
      { Environment.create.impl name -> resume } ->
        result =
          catch do
            path = environmentsPath / name
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "creating environment with name " ++ name
            expectApiSuccess context response
            envId = Environment.Id.Id (decodeText response)
            Environment envId name
        go do resume result
      { Environment.delete.impl env -> resume } ->
        result =
          catch do
            path =
              environmentsPath
                / (env |> Environment.id |> Environment.Id.toText)
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "deleting environment " ++ Environment.toText env
            expectApiSuccess context response
        go do resume result
      { list.impl -> resume } ->
        result =
          catch do
            path = environmentsPath
            req =
              get (httpUri config path RawQuery.empty)
                |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            res
              |> decodeText
              |> run
                   (array do
                     Environment
                       (Environment.Id.Id (at! "id" text)) (at! "name" text))
        go do resume result
      { setValue.impl env key value -> resume } ->
        result =
          catch do
            path =
              environmentsPath
                / (env |> Environment.id |> Environment.Id.toText)
                / key
            uri = httpUri config path RawQuery.empty
            req =
              post uri (Body (Text.toUtf8 value))
                |> headers.modify
                     (Headers.setHeader
                       "Content-Type" "text/plain;charset=UTF-8")
                |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "setting key '"
                ++ key
                ++ "' on environment "
                ++ Environment.toText env
            expectApiSuccess context response
        go do resume result
      { deleteValue.impl env key -> resume } ->
        result =
          catch do
            path =
              environmentsPath
                / (env |> Environment.id |> Environment.Id.toText)
                / key
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "deleting key '"
                ++ key
                ++ "' on environment "
                ++ Environment.toText env
            expectApiSuccess context response
        go do resume result
      { ServiceName.create.impl name -> resume } ->
        result =
          catch do
            path = servicesPath / name
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "creating service name " ++ name
            expectApiSuccess context response
            serviceId = ServiceName.Id.Id (decodeText response)
            ServiceName serviceId name
        go do resume result
      { ServiceName.delete.impl service -> resume } ->
        result =
          catch do
            path =
              servicesPath / ServiceName.Id.toText (ServiceName.id service)
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "deleting service name " ++ ServiceName.toText service
            expectApiSuccess context response
        go do resume result
      { ServiceName.assign.impl service serviceHash -> resume } ->
        result =
          catch do
            path =
              servicesPath
                / ServiceName.Id.toText (ServiceName.id service)
                / "assign"
                / ServiceHash.toText serviceHash
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "assigning service name "
                ++ ServiceName.toText service
                ++ " to service with hash "
                ++ ServiceHash.toText serviceHash
            expectApiSuccess context response
            deployedAt =
              response |> decodeText |> run (at "uri" uriStripTrailingSlash)
            banner =
              "Service with hash: "
                ++ ServiceHash.toText serviceHash
                ++ " now available at: "
                ++ "\n  "
                ++ URI.toText deployedAt
                ++ "/"
            printLine banner
            deployedAt
        go do resume result
      { ServiceName.unassign.impl service -> resume } ->
        result =
          catch do
            path =
              servicesPath
                / ServiceName.Id.toText (ServiceName.id service)
                / "unassign"
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "unassigning service name " ++ ServiceName.toText service
            expectApiSuccess context response
        go do resume result
      { Database.assign.impl db env -> resume } ->
        result =
          catch do
            path =
              databasePath
                / Database.Id.toText (Database.id db)
                / "assign"
                / Environment.Id.toText (Environment.id env)
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "assigning database "
                ++ Database.toText db
                ++ " to environment "
                ++ Environment.toText env
            expectApiSuccess context response
        go do resume result
      { Database.unassign.impl db env -> resume } ->
        result =
          catch do
            path =
              databasePath
                / Database.Id.toText (Database.id db)
                / "unassign"
                / Environment.Id.toText (Environment.id env)
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "unassigning database "
                ++ Database.toText db
                ++ " from environment "
                ++ Environment.toText env
            expectApiSuccess context response
        go do resume result
      { Database.create.impl name -> resume } ->
        result =
          catch do
            path = databasePath / "create" / name
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "creating database with name " ++ name
            expectApiSuccess context response
            dbId = Database.Id.Id (decodeText response)
            Database dbId name
        go do resume result
      { Database.delete.impl db -> resume } ->
        result =
          catch do
            path = databasePath / Database.Id.toText (Database.id db)
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "deleting database " ++ Database.toText db
            expectApiSuccess context response
        go do resume result
      { logs.impl options -> resume } ->
        toNanos = Nat.toText << Int.toRepresentation << nanosecondsSinceEpoch
        result =
          catch do
            addQuery which maybe acc = match maybe with
              None       -> acc
              Some value -> acc & (which, value)
            query =
              Query.empty
                |> addQuery "search" (options |> QueryOptions.search)
                |> addQuery
                     "limit"
                     (Optional.map Nat.toText (options |> QueryOptions.limit))
                |> addQuery
                     "start"
                     (Optional.map toNanos (options |> QueryOptions.start))
                |> addQuery "end" (Optional.map toNanos (options |> end))
                |> addQuery
                     "direction"
                     (Optional.map Direction.toText (options |> direction))
            path = root / "v2" / "logs"
            uri = httpUri config path (fromQuery query)
            req = get uri |> addAuthHeader (token config)
            res = handle request req with configuredHandler (httpConfig config)
            context = "fetching logs"
            expectApiSuccess context res
            res
              |> decodeText
              |> run (at "logs" (array text))
              |> List.map (s -> Json.fromText s)
        go do resume result
      { Daemon.assign.impl daemon hash -> k } ->
        use Daemon Id.toText
        result =
          catch do
            hashTxt = DaemonHash.toText hash
            path =
              daemonsPath / Id.toText (Daemon.id daemon) / "assign" / hashTxt
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context =
              "assigning daemon "
                ++ Id.toText (Daemon.id daemon)
                ++ " to service with hash "
                ++ hashTxt
            expectApiSuccess context response
        go do k result
      { Daemon.delete.impl daemon -> k } ->
        use Daemon.Id toText
        result =
          catch do
            path = daemonsPath / toText (Daemon.id daemon)
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "deleting daemon " ++ toText (Daemon.id daemon)
            expectApiSuccess context response
        go do k result
      { Daemon.create.impl name -> k } ->
        result =
          catch do
            path = daemonsPath / name
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "creating daemon with name " ++ name
            expectApiSuccess context response
            id = decodeText response |> Daemon.Id.Id
            Daemon id name
        go do k result
      { Daemon.unassign.impl daemon -> k } ->
        use Daemon.Id toText
        result =
          catch do
            path = daemonsPath / toText (Daemon.id daemon) / "unassign"
            uri = httpUri config path RawQuery.empty
            req = post uri Body.empty |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "unassigning daemon " ++ toText (Daemon.id daemon)
            expectApiSuccess context response
        go do k result
      { DaemonHash.create.impl env daemon -> k } ->
        result =
          catch do
            checkSandbox
              "Cannot deploy a daemon that performs restricted operations."
              (Value.value (daemon : '{Remote} ()))
            path = daemonHashesPath
            reqBody = Body (encode.toBytes do encodeService daemon)
            query =
              Query.empty
                & ( "environmentId"
                , env |> Environment.id |> Environment.Id.toText
                )
            uri = httpUri config path (fromQuery query)
            req = post uri reqBody |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "creating daemon hash"
            expectApiSuccess context response
            response |> decodeText |> DaemonHash
        go do k result
      { DaemonHash.delete.impl hash -> k } ->
        use DaemonHash toText
        result =
          catch do
            path = daemonHashesPath / toText hash
            uri = httpUri config path RawQuery.empty
            req = delete uri |> addAuthHeader (token config)
            response =
              handle request req with configuredHandler (httpConfig config)
            context = "deleting daemon hash " ++ toText hash
            expectApiSuccess context response
        go do k result
  go p

Cloud.run.withConfig.doc : Doc
Cloud.run.withConfig.doc =
  {{
  A version of {Cloud.run} that takes a {type Cloud.ClientConfig} to use
  instead of the default configuration.
  }}

Cloud.submit :
  Environment
  -> '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Random,
  Log,
  Scratch} a
  ->{Exception, Cloud} a
Cloud.submit environment thunk = submit.remote environment (pool.wrap thunk)

Cloud.submit.doc : Doc
Cloud.submit.doc =
  use Cloud submit
  {{
  `` submit environment computation `` submits a computation to run on the
  Unison Cloud and returns its result. The computation argument is a thunk
  which can use any of the abilities described in the signature of {submit}.

  {submit} is more commonly used for batch jobs whereas {Cloud.deploy} is used
  for long-running services.
  }}

Cloud.submit.remote : Environment -> '{Remote} a ->{Exception, Cloud} a
Cloud.submit.remote env thunk = Either.toException (submit.impl env thunk)

Cloud.submit.remote.doc : Doc
Cloud.submit.remote.doc =
  {{
  A version of {Cloud.submit} that is restricted to only using the
  {type Remote} ability.
  }}

Cloud.undeploy : ServiceHash a b ->{Exception, Cloud} ()
Cloud.undeploy serviceHash = Either.toException (undeploy.impl serviceHash)

Cloud.undeploy.doc : Doc
Cloud.undeploy.doc =
  {{
  Undeploy a service from Unison Cloud. After this, calls to the provided
  service will fail.
  }}

Cloud.unexposeHttp :
  ServiceHash HttpRequest HttpResponse ->{Exception, Cloud} ()
Cloud.unexposeHttp serviceHash =
  Either.toException (unexposeHttp.impl serviceHash)

Cloud.unexposeHttp.doc : Doc
Cloud.unexposeHttp.doc =
  {{
  Unexpose the HTTP routing for a service without undeploying the underlying
  Unison service itself.

  The provided service hash must be one that was previously exposed (such as
  with {exposeHttp}). Its input should be generally be of type
  {type HttpRequest}, though it could potentially be an older version of the
  HTTP request type.

  See {undeploy} to unexpose the HTTP routing __and__ remove the underlying
  Unison service.
  }}

Daemon.id : Daemon -> Daemon.Id
Daemon.id = cases Daemon id _ -> id

Daemon.Id.toText : Daemon.Id -> Text
Daemon.Id.toText = cases Daemon.Id.Id id -> id

Daemon.name : Daemon -> Text
Daemon.name = cases Daemon _ name -> name

DaemonHash.toText : DaemonHash -> Text
DaemonHash.toText = cases DaemonHash hash -> hash

Database.assign : Database -> Environment ->{Exception, Cloud} ()
Database.assign pool environment =
  Either.toException (Database.assign.impl pool environment)

Database.assign.doc : Doc
Database.assign.doc =
  {{
  Assigns a database to an environment. Takes a {type Database} and an
  {type Environment}.

  # Example

    @typecheck ```
    db = Database.named "my-db"
    env = Environment.named "my-env"
    Database.assign db env
    ```
  }}

Database.default : '{Exception, Cloud} Database
Database.default = do
  env = Environment.default()
  db = Database.named "default"
  Database.assign db env
  db

Database.default.doc : Doc
Database.default.doc =
  {{
  The {type Database} called `"default"`, which is accessible from
  {Environment.default}. The computation is idempotent so you can force it
  whenever you want.
  }}

Database.delete : Database ->{Exception, Cloud} ()
Database.delete database = Either.toException (Database.delete.impl database)

Database.delete.doc : Doc
Database.delete.doc =
  use Database named
  {{
  Takes a {type Database} and deletes it.

  # Example

    This example deletes a database named "my-db". Note that {named} is
    idempotent, so it gets the database if it already exists or creates a new
    one if it doesn't:

    @typecheck ```
    db = named "my-db"
    Database.delete db
    ```
  }}

Database.doc : Doc
Database.doc =
  {{
  A {type Database} is a collection of data that is stored in an
  {type Environment} in the {type Cloud}. It can be used to store and retrieve
  data, and to assign and unassign databases to environments.

  # Operations

    * {Database.default} - Get the default database for the default
      environment.
    * {Database.named} - Create a new database or get an existing one.
    * {Database.delete} - Delete a database.
    * {Database.assign} - Assign a database to an environment.
    * {Database.unassign} - Unassign a database from an environment.

    These operations use the {type Cloud} ability to interact with the cloud
    environment.

  # See also

    * {type Environment} - for creating and deleting environments, and setting
      and deleting environment values.
    * {type OrderedTable} - for storing and retrieving data in a durable B-tree
      in a {type Database}.
    * {type DatabaseInfo} - for retrieving information about a {type Database}.
  }}

Database.id : Database -> Database.Id
Database.id = cases Database id _ -> id

Database.id.doc : Doc
Database.id.doc = {{ Gets the {type Database.Id} of a {type Database}. }}

Database.Id.doc : Doc
Database.Id.doc =
  {{
  A {type Database.Id} is a unique identifier for a {type Database}. It is used
  to refer to the database in the {type Cloud}. The {type Database.Id} is
  typically obtained as part of a {type DatabaseInfo}, which is for example
  returned by the {Database.list} operation.
  }}

Database.Id.fromJson : '{Decoder} Database.Id
Database.Id.fromJson = do Database.Id.Id Decoder.text()

Database.Id.fromJson.doc : Doc
Database.Id.fromJson.doc =
  {{ A {type Decoder} for parsing JSON string into a {type Database.Id}. }}

Database.Id.toText : Database.Id -> Text
Database.Id.toText = cases Database.Id.Id t -> t

Database.Id.toText.doc : Doc
Database.Id.toText.doc =
  {{ Converts a {type Database.Id} to its {type Text} representation. }}

Database.list : '{IO, Exception} [DatabaseInfo]
Database.list = do Database.list.withConfig Cloud.ClientConfig.default()

Database.list.doc : Doc
Database.list.doc =
  {{
  Get the list of {type DatabaseInfo}s which have been assigned to any
  {type Environment}
  }}

Database.list.withConfig : Cloud.ClientConfig ->{IO, Exception} [DatabaseInfo]
Database.list.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "storage") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res : HttpResponse
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "listing databases"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array DatabaseInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

Database.list.withConfig.doc : Doc
Database.list.withConfig.doc =
  {{
  A version of {Database.list} that takes a {type Cloud.ClientConfig} to use
  instead of the default configuration.
  }}

Database.name : Database -> Text
Database.name = cases Database _ name -> name

Database.name.doc : Doc
Database.name.doc = {{ Gets the name of a {type Database}. }}

Database.named : Text ->{Exception, Cloud} Database
Database.named name = Either.toException (Database.create.impl name)

Database.named.doc : Doc
Database.named.doc =
  {{
  `` Database.named poolName `` creates a new {type Database} with the given
  human readable name. The creation of a storage pool is idempotent. If a
  storage pool with the given name already exists under the current account
  then the preexisting pool is returned.

  @typecheck ```
  myPool : Database
  myPool = Database.named "appPool"
  env : Environment
  env = Environment.named "myEnv"
  Database.assign myPool env
  ```
  }}

Database.toText : Database -> Text
Database.toText = cases
  Database id name ->
    Database.Id.toText id Text.++ " (" Text.++ name Text.++ ")"

Database.toText.doc : Doc
Database.toText.doc =
  {{
  Returns a human-readable {type Text} representation of a {type Database}.
  }}

Database.unassign : Database -> Environment ->{Exception, Cloud} ()
Database.unassign pool environment =
  Either.toException (Database.unassign.impl pool environment)

Database.unassign.doc : Doc
Database.unassign.doc =
  {{
  Unassigns a database from an environment. Takes a {type Database} and an
  {type Environment}. The database is not deleted, only unassigned from the
  environment.

  # Example

    @typecheck ```
    db = Database.named "my-db"
    env = Environment.named "my-env"
    Database.unassign db env
    ```

    This example unassigns the database "my-db" from the environment "my-env".
    Note that {Database.named} and {Environment.named} are idempotent, so they
    get the database and environment if they already exist or create new ones
    if they don't.
  }}

DatabaseInfo.doc : Doc
DatabaseInfo.doc =
  {{
  A {type DatabaseInfo} contains information about a {type Database}. It has
  fields for the database's {type Database.Id}, name, and the
  {type Environment.Id}s that it is assigned to.

  {type DatabaseInfo} is for example returned by {Database.list}.
  }}

DatabaseInfo.environments : DatabaseInfo -> [Environment.Id]
DatabaseInfo.environments = cases DatabaseInfo _ _ environments -> environments

DatabaseInfo.environments.doc : Doc
DatabaseInfo.environments.doc =
  {{
  Gets the environments to which the database is assigned from the
  {type DatabaseInfo}.
  }}

DatabaseInfo.environments.modify :
  ([Environment.Id] ->{g} [Environment.Id]) -> DatabaseInfo ->{g} DatabaseInfo
DatabaseInfo.environments.modify f = cases
  DatabaseInfo id name environments -> DatabaseInfo id name (f environments)

DatabaseInfo.environments.set :
  [Environment.Id] -> DatabaseInfo -> DatabaseInfo
DatabaseInfo.environments.set environments1 = cases
  DatabaseInfo id name _ -> DatabaseInfo id name environments1

DatabaseInfo.fromJson : '{Decoder} DatabaseInfo
DatabaseInfo.fromJson = do
  use object at!
  id = at! "id" Database.Id.fromJson
  name = at! "name" Decoder.text
  environments = at! "environments" (Decoder.array Environment.Id.fromJson)
  DatabaseInfo id name environments

DatabaseInfo.fromJson.doc : Doc
DatabaseInfo.fromJson.doc =
  {{
  A {type Decoder} for parsing JSON string into a {type DatabaseInfo}.

  This decoder expects a JSON object with the following properties:

  * `id` - The ID of the database. A string.
  * `name` - The name of the database. A string.
  * `environments` - The environments to which the database is assigned. An
    array of strings.
  }}

DatabaseInfo.id : DatabaseInfo -> Database.Id
DatabaseInfo.id = cases DatabaseInfo id _ _ -> id

DatabaseInfo.id.doc : Doc
DatabaseInfo.id.doc =
  {{ Gets the ID of the database from the {type DatabaseInfo}. }}

DatabaseInfo.id.modify :
  (Database.Id ->{g} Database.Id) -> DatabaseInfo ->{g} DatabaseInfo
DatabaseInfo.id.modify f = cases
  DatabaseInfo id name environments -> DatabaseInfo (f id) name environments

DatabaseInfo.id.set : Database.Id -> DatabaseInfo -> DatabaseInfo
DatabaseInfo.id.set id1 = cases
  DatabaseInfo _ name environments -> DatabaseInfo id1 name environments

DatabaseInfo.name : DatabaseInfo -> Text
DatabaseInfo.name = cases DatabaseInfo _ name _ -> name

DatabaseInfo.name.doc : Doc
DatabaseInfo.name.doc =
  {{ Gets the name of the database from the {type DatabaseInfo}. }}

DatabaseInfo.name.modify : (Text ->{g} Text) -> DatabaseInfo ->{g} DatabaseInfo
DatabaseInfo.name.modify f = cases
  DatabaseInfo id name environments -> DatabaseInfo id (f name) environments

DatabaseInfo.name.set : Text -> DatabaseInfo -> DatabaseInfo
DatabaseInfo.name.set name1 = cases
  DatabaseInfo id _ environments -> DatabaseInfo id name1 environments

DeploymentInfo.deployedAt : DeploymentInfo -> Instant
DeploymentInfo.deployedAt = cases
  DeploymentInfo _ deployedAt _ _ _ _ -> deployedAt

DeploymentInfo.deployedAt.doc : Doc
DeploymentInfo.deployedAt.doc =
  {{
  Gets the time the service or job was deployed from a {type DeploymentInfo},
  as an {type Instant}.
  }}

DeploymentInfo.deployedAt.modify :
  (Instant ->{g} Instant) -> DeploymentInfo ->{g} DeploymentInfo
DeploymentInfo.deployedAt.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash (f deployedAt) undeployedAt deployedBy exposedAt unexposedAt

DeploymentInfo.deployedAt.set : Instant -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.deployedAt.set deployedAt1 = cases
  DeploymentInfo hash _ undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt1 undeployedAt deployedBy exposedAt unexposedAt

DeploymentInfo.deployedBy : DeploymentInfo -> UserInfo
DeploymentInfo.deployedBy = cases
  DeploymentInfo _ _ _ deployedBy _ _ -> deployedBy

DeploymentInfo.deployedBy.doc : Doc
DeploymentInfo.deployedBy.doc =
  {{
  Gets the user who deployed the service or job from a {type DeploymentInfo},
  as a {type UserInfo}.
  }}

DeploymentInfo.deployedBy.modify :
  (UserInfo ->{g} UserInfo) -> DeploymentInfo ->{g} DeploymentInfo
DeploymentInfo.deployedBy.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt (f deployedBy) exposedAt unexposedAt

DeploymentInfo.deployedBy.set : UserInfo -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.deployedBy.set deployedBy1 = cases
  DeploymentInfo hash deployedAt undeployedAt _ exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt deployedBy1 exposedAt unexposedAt

DeploymentInfo.doc : Doc
DeploymentInfo.doc =
  {{
  A {type DeploymentInfo} contains information about a deployment of a service
  to the cloud.

  # Fields

    * {DeploymentInfo.hash} - The hash of the service that was deployed, as a
      {type ServiceHash.Untyped}.
    * {DeploymentInfo.deployedAt} - The time at which the service was deployed.
    * {DeploymentInfo.undeployedAt} - The time at which the service was
      undeployed, if it has been undeployed.
    * {DeploymentInfo.deployedBy} - Information about the user who deployed the
      service, as a {type UserInfo}.
    * {DeploymentInfo.exposedAt} - The time at which the service was exposed,
      if it has been exposed.
    * {DeploymentInfo.unexposedAt} - The time at which the service was
      unexposed, if it has been unexposed.
  }}

DeploymentInfo.exposedAt : DeploymentInfo -> Optional Instant
DeploymentInfo.exposedAt = cases
  DeploymentInfo _ _ _ _ exposedAt _ -> exposedAt

DeploymentInfo.exposedAt.doc : Doc
DeploymentInfo.exposedAt.doc =
  {{
  Gets the time the service was exposed from a {type DeploymentInfo}, as an
  {type Optional} {type Instant}.
  }}

DeploymentInfo.exposedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> DeploymentInfo
  ->{g} DeploymentInfo
DeploymentInfo.exposedAt.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt deployedBy (f exposedAt) unexposedAt

DeploymentInfo.exposedAt.set :
  Optional Instant -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.exposedAt.set exposedAt1 = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy _ unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt deployedBy exposedAt1 unexposedAt

DeploymentInfo.fromJson : '{Decoder} DeploymentInfo
DeploymentInfo.fromJson = do
  use Decoder optional
  use object at!
  hash = at! "hash" ServiceHash.fromJson
  deployedAt = at! "deployedAt" instant
  undeployedAt = at! "undeployedAt" (optional instant)
  deployedBy = at! "deployedBy" UserInfo.fromJson
  exposedAt = at! "exposedAt" (optional instant)
  unexposedAt = at! "unexposedAt" (optional instant)
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt

DeploymentInfo.fromJson.doc : Doc
DeploymentInfo.fromJson.doc =
  {{
  A {type Decoder} for parsing JSON string into a {type DeploymentInfo}.

  This decoder expects a JSON object with the following properties:

  * `hash` - The hash of the service or job. A string.
  * `deployedAt` - The time the service or job was deployed. A string in ISO
    8601 format.
  * `undeployedAt` (optional) - The time the service or job was undeployed. A
    string in ISO 8601 format.
  * `deployedBy` (optional) - A {type UserInfo} object representing the user
    who deployed the service or job. See {UserInfo.fromJson}.
  * `exposedAt` (optional) - The time the service was exposed. A string in ISO
    8601 format.
  * `unexposedAt` (optional) - The time the service was unexposed. A string in
    ISO 8601 format.
  }}

DeploymentInfo.hash : DeploymentInfo -> ServiceHash.Untyped
DeploymentInfo.hash = cases DeploymentInfo hash _ _ _ _ _ -> hash

DeploymentInfo.hash.doc : Doc
DeploymentInfo.hash.doc =
  {{
  Gets the hash of the service or job from a {type DeploymentInfo}, as a
  {type ServiceHash.Untyped}.
  }}

DeploymentInfo.hash.modify :
  (ServiceHash.Untyped ->{g} ServiceHash.Untyped)
  -> DeploymentInfo
  ->{g} DeploymentInfo
DeploymentInfo.hash.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      (f hash) deployedAt undeployedAt deployedBy exposedAt unexposedAt

DeploymentInfo.hash.set :
  ServiceHash.Untyped -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.hash.set hash1 = cases
  DeploymentInfo _ deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash1 deployedAt undeployedAt deployedBy exposedAt unexposedAt

DeploymentInfo.undeployedAt : DeploymentInfo -> Optional Instant
DeploymentInfo.undeployedAt = cases
  DeploymentInfo _ _ undeployedAt _ _ _ -> undeployedAt

DeploymentInfo.undeployedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> DeploymentInfo
  ->{g} DeploymentInfo
DeploymentInfo.undeployedAt.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt (f undeployedAt) deployedBy exposedAt unexposedAt

DeploymentInfo.undeployedAt.set :
  Optional Instant -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.undeployedAt.set undeployedAt1 = cases
  DeploymentInfo hash deployedAt _ deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt1 deployedBy exposedAt unexposedAt

DeploymentInfo.unexposedAt : DeploymentInfo -> Optional Instant
DeploymentInfo.unexposedAt = cases
  DeploymentInfo _ _ _ _ _ unexposedAt -> unexposedAt

DeploymentInfo.unexposedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> DeploymentInfo
  ->{g} DeploymentInfo
DeploymentInfo.unexposedAt.modify f = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt unexposedAt ->
    DeploymentInfo
      hash deployedAt undeployedAt deployedBy exposedAt (f unexposedAt)

DeploymentInfo.unexposedAt.set :
  Optional Instant -> DeploymentInfo -> DeploymentInfo
DeploymentInfo.unexposedAt.set unexposedAt1 = cases
  DeploymentInfo hash deployedAt undeployedAt deployedBy exposedAt _ ->
    DeploymentInfo
      hash deployedAt undeployedAt deployedBy exposedAt unexposedAt1

durable.Cell.database : Cell a -> Database
durable.Cell.database = cases Cell db _ _ -> db

durable.Cell.database.doc : Doc
durable.Cell.database.doc =
  {{ The {type Database}} where this {type Cell} is stored. }}

durable.Cell.delete : Cell a ->{Remote} ()
durable.Cell.delete = cases
  Cell db _ tbl -> toRemote do transact db do Storage.delete.tx tbl ()

durable.Cell.delete.doc : Doc
durable.Cell.delete.doc =
  {{
  Resets a {type Cell} back to its default value, deleting any associated
  durable storage.

  **Also see:** {Cell.delete.tx}
  }}

durable.Cell.delete.tx : Cell a ->{Transaction} ()
durable.Cell.delete.tx = cases Cell _ _ tbl -> Storage.delete.tx tbl ()

durable.Cell.delete.tx.doc : Doc
durable.Cell.delete.tx.doc =
  {{
  Resets a {type Cell} back to its default value, deleting any associated
  durable storage.

  **Also see:** {Cell.delete}
  }}

durable.Cell.doc : Doc
durable.Cell.doc =
  use Cell modify_ nested read.tx write.tx
  use List ++
  use Nat + -
  {{
  A single durable value.

  {{ final }}

  Here's a simple usage:

  @typecheck ```
  db = Database.named "my-slackbot"
  c = Cell.named db "recent-messages" []
  modify_ c (msgs -> msgs ++ ["hello, world!"])
  ```

  The core functions are:

      @signatures{Cell.modify, Cell.read, reset}

  There are also transactional variants of these, {read.tx}, {write.tx},
  {modify.tx}:

  @typecheck ```
  db = Database.named "banking-database"
  alice = Cell.named db "alice-balance" 10238
  bob = Cell.named db "bob-balance" 0
  transact db do
    do
      write.tx alice (read.tx alice - 100)
      write.tx bob (read.tx bob + 100)
  ```

  # Naming and nesting within other data structures

    If you have multiple {type Cell} and/or {type Table} values in your
    application, each must have a unique name. For top-level definitions, it's
    fine to pick a unique string manually, but for applications with lots of
    {type Cell} values, possibly nested within other data structures, you can
    use: * {nested}, which forms a unique identity by hashing some parent value
    and key.

    For example, here's a simple model of a blog with multiple posts, keyed by
    date, with each post represented by a {{
    docExample 1 do post -> (post : (Text, Cell [Text])) }} for the post text
    and a modifiable list of comments on that post.

    @typecheck ```
    db = Database.named "blog"
    posts = Cell.named db "posts" data.Map.empty
    date = "2023-08-25"
    post : Cell (Text, [Text])
    post = nested db (posts, date) ("A post", ["A comment"])
    modify_ posts (Map.insert date post)
    ```

    This doesn't use randomness. The {{
    docExample 2 do posts date -> (posts, date) }} is hashed to form the
    identity of the the `post` {type Cell}.
  }}

durable.Cell.docs.modificationFunctions : Doc
durable.Cell.docs.modificationFunctions =
  use Cell modify
  {{
  There are several variants of {modify}:

  {{
  docTable
    [ [{{ @inlineSignature{Cell.modify_} }}, {{ Returns `` () `` }}]
    , [ {{
        @inlineSignature{getModify}
        }}
      , {{
        Returns the original result before modification
        }}
      ]
    , [ {{
        @inlineSignature{Cell.modifyGet}
        }}
      , {{
        Returns the result after modification
        }}
      ]
    , [ {{
        @inlineSignature{modify}
        }}
      , {{
        Returns an arbitrary user-chosen value
        }}
      ]
    , [ {{
        @inlineSignature{modify.tx}
        }}
      , {{
        Returns an arbitrary user-chosen value within a {type Transaction}
        }}
      ]
    ] }}
  }}

durable.Cell.getModify :
  Cell a -> (a ->{Transaction, Exception} a) ->{Remote} a
durable.Cell.getModify c f =
  g a =
    r = f a
    (r, a)
  Cell.modify c g

durable.Cell.getModify.doc : Doc
durable.Cell.getModify.doc =
  use Nat +
  {{
  Modifies the value of a {type Cell}, returning the original value before
  modification.

  @typecheck ```
  db = Database.named "banking-database"
  alice = Cell.named db "alice-balance" 0
  Cell.modifyGet alice (x -> x + 1000000)
  ```

  This will return ``0``.

  {{ modificationFunctions }}
  }}

durable.Cell.modify :
  Cell a -> (a ->{Transaction, Exception} (a, b)) ->{Remote} b
durable.Cell.modify c f =
  toRemote do transact (Cell.database c) do modify.tx c f

durable.Cell.modify.doc : Doc
durable.Cell.modify.doc =
  use Cell modify
  use Nat +
  {{
  `` modify c f `` modifies the value of a {type Cell} using `f`, returning the
  second element of the pair produced by `f`.

  @typecheck ```
  db = Database.named "banking-database"
  alice = Cell.named db "alice-balance" 0
  modify alice (x -> (x + 1000000, "💰"))
  ```

  This will return ``"💰"``.

  {{ modificationFunctions }}
  }}

durable.Cell.modify.tx :
  Cell a -> (a ->{g, Transaction} (a, b)) ->{g, Transaction} b
durable.Cell.modify.tx c f =
  (a, b) = f (Cell.read.tx c)
  Cell.write.tx c a
  b

durable.Cell.modify.tx.doc : Doc
durable.Cell.modify.tx.doc =
  {{ Just like {Cell.modify} but returns the result within a transaction. }}

durable.Cell.modifyGet :
  Cell a -> (a ->{Transaction, Exception} a) ->{Remote} a
durable.Cell.modifyGet c f =
  g a =
    r = f a
    (r, r)
  Cell.modify c g

durable.Cell.modifyGet.doc : Doc
durable.Cell.modifyGet.doc =
  use Nat +
  {{
  Modifies the value of a {type Cell}, returning the modified result.

  @typecheck ```
  db = Database.named "banking-database"
  alice = Cell.named db "alice-balance" 0
  Cell.modifyGet alice (x -> x + 1000000)
  ```

  This will return ``1000000``.

  {{ modificationFunctions }}
  }}

durable.Cell.modifyGet.tx :
  Cell a -> (a ->{g} a) ->{g, Transaction, Exception} a
durable.Cell.modifyGet.tx c f =
  go a =
    a' = f a
    (a', a')
  modify.tx c go

durable.Cell.modifyGet.tx.doc : Doc
durable.Cell.modifyGet.tx.doc =
  {{
  Modifies the value of a {type Cell} and returns the new value.

  ```
  go db = transact db do
    c = Cell.named db "c" 0
    Cell.modifyGet.tx c Nat.increment
  exampleWithDb go
  ```

  **Also see:** {Cell.modifyGet}, {Cell.read.tx}, {Cell.write.tx}
  }}

durable.Cell.modify_ : Cell a -> (a ->{Transaction, Exception} a) ->{Remote} ()
durable.Cell.modify_ c f = Cell.modify c (a -> (f a, ()))

durable.Cell.modify_.doc : Doc
durable.Cell.modify_.doc =
  use Nat +
  {{
  Modifies the value of a {type Cell}, returning ``()``.

  @typecheck ```
  db = Database.named "banking-database"
  alice = Cell.named db "alice-balance" 0
  Cell.modify_ alice (x -> x + 1000000)
  ```

  This will return ``()``.

  {{ modificationFunctions }}
  }}

durable.Cell.named : Database -> Text -> a -> Cell a
durable.Cell.named db name a = Cell db a (Table.Table name)

durable.Cell.nested : Database -> parent -> a -> Cell a
durable.Cell.nested db parent a =
  name = blake2b_256 (durable.Cell.nested, parent) |> up.Bytes.toBase32Hex.text
  Cell db a (Table.Table name)

durable.Cell.read : Cell a ->{Remote} a
durable.Cell.read = cases
  Cell db default tbl ->
    toRemote do
      (transact db do Transaction.tryRead.tx tbl ())
        |> Optional.getOrElse default

durable.Cell.read.tx : Cell a ->{Transaction} a
durable.Cell.read.tx = cases
  Cell db default tbl ->
    Transaction.tryRead.tx tbl () |> Optional.getOrElse default

durable.Cell.reset : Cell a -> a ->{Exception, Storage} ()
durable.Cell.reset c a =
  (Cell db default tbl) = c
  transact db do Transaction.write.tx tbl () a

durable.Cell.write : Cell a -> a ->{Remote} ()
durable.Cell.write cell value =
  toRemote do transact (Cell.database cell) do Cell.write.tx cell value

durable.Cell.write.tx : Cell a -> a ->{Transaction} ()
durable.Cell.write.tx c a =
  (Cell db default tbl) = c
  Transaction.write.tx tbl () a

durable.docs.fragments.wip : Doc
durable.docs.fragments.wip =
  docCallout (Some {{ 🚧 }}) {{ This is a work in progress. }}

durable.docs.stability.draft : Doc
durable.docs.stability.draft =
  docCallout
    (Some {{ 🚧 }})
    {{
    This implementation is in draft form. Its interface may change
    substantially before being marked {stability.new}. It may have insufficient
    testing or bugs.
    }}

durable.docs.stability.final : Doc
durable.docs.stability.final =
  docCallout
    (Some {{ 🚢 }})
    {{
    This implementation is finalized. It contains adequate test coverage, and
    WILL NOT change in any backwards-incompatible way without a period of
    deprecation and a documented migration process. In the event of a
    backwards-incompatible change to a finalized type, the old version will be
    pulled into a separate library, for applications unable to migrate to newer
    versions yet.

    Finalized data structures are suitable for use in production applications.
    }}

durable.docs.stability.new : Doc
durable.docs.stability.new =
  docCallout
    (Some {{ 🧪 }})
    {{
    This implementation is new and experimental. It requires further testing
    and usage before being marked {final}. It MAY change in backwards
    incompatible ways before being marked {final}.

    Please let us know in the `#cloud` Discord if you give it a whirl!
    }}

durable.GinIndex.doc : Doc
durable.GinIndex.doc =
  {{
  A Generalised INverted Index, useful for search.

  {{ draft }}
  }}

durable.GinIndex.insert :
  GinIndex input key id data -> input -> id -> data ->{Remote} ()
durable.GinIndex.insert ginIndex input id data =
  use OrderedTable write.tx
  (GinIndex db ordering indexOn index) = ginIndex
  addEntry key = toRemote do
    transact db do
      postingList = match OrderedTable.tryRead.tx index key with
        Some postingList -> postingList
        None             ->
          postingList : OrderedTable id data
          postingList = OrderedTable.named db randomName() ordering
          write.tx index key postingList
          postingList
      write.tx postingList id data
  indexOn input
    |> List.distinct
    |> Remote.parMap addEntry
    |> unison_base_3_22_0.ignore

durable.GinIndex.internal.test.all : '{IO} [Result]
durable.GinIndex.internal.test.all =
  do
    test.verify GinIndex.internal.test.correctness
      List.++ test.verify GinIndex.internal.test.performance

durable.GinIndex.internal.test.correctness : '{IO, Exception} ()
durable.GinIndex.internal.test.correctness =
  do
    Cloud.run.local do
      use Debug trace
      use Text ++
      db = Database.default()
      Cloud.submit Environment.default() do
        index = GinIndex.named db "my-index" Universal.ordering util.Text.words
        write id v = GinIndex.insert index v id ("<" ++ v ++ ">")
        read query =
          GinIndex.query index query
            |> Orderator.toList
            |> List.map (at2 >> force)
        query =
          Minus
            (Or (And (Exact "foo") (Exact "bar")) (Exact "baz")) (Exact "bal")
        docs =
          [ "foo"
          , "hello"
          , "foo bar"
          , "bar"
          , "foo bar bal"
          , "baz"
          , "darkness"
          , "foo bar baz"
          ]
        expected = ["<foo bar>", "<baz>", "<foo bar baz>"]
        trace "indexing" ()
        _ =
          docs
            |> List.indexed
            |> (Remote.parMap cases (doc, id) -> write id doc)
        trace "indexing complete" ()
        trace "running the query" ()
        results = read query
        test.ensureEqual results expected
        trace "assertions satisfied" ()

durable.GinIndex.internal.test.performance : '{IO, Exception} ()
durable.GinIndex.internal.test.performance =
  do
    Cloud.run.local do
      use Debug trace
      use Text ++
      db = Database.default()
      Cloud.submit Environment.default() do
        index = GinIndex.named db "my-index" Universal.ordering util.Text.words
        write id v = GinIndex.insert index v id ("<" ++ v ++ ">")
        read query =
          GinIndex.query index query
            |> Orderator.toList
            |> List.map (at2 >> force)
        measure label thunk =
          use Duration -
          use Text join
          t0 = monotonic!
          r = thunk()
          t1 = monotonic!
          msg =
            join
              "\n"
              [ label
              , join
                  " "
                  [ "complete:"
                  , t1 - t0 |> asMilliseconds |> Float.toText
                  , "milliseconds"
                  ]
              , "results: "
              ]
          trace msg r
        _ =
          """

          query =
            Minus
             (Or (And (Exact "foo") (Exact "bar")) (Exact "baz")) (Exact "bal")
          """
        q0 = Exact "foo"
        q1 = And q0 (Exact "bar")
        q2 = And q1 (Exact "baz")
        q3 = And q2 (Exact "are")
        docs =
          [ "foo has 3 letters"
          , "hello darkness my old friend"
          , "foo and bar are used as variable names"
          , "let's go to a bar"
          , "foo and bar are often used as variable names but bal isn't"
          , "Another common variable name is baz"
          , "I've come to talk to you again"
          , "foo bar and baz are common variable names"
          ]
        trace "indexing: " docs
        measure "indexed" do
          _ =
            docs
              |> List.indexed
              |> (Remote.parMap cases (doc, name) -> write name doc)
          ()
        trace "querying" ()
        measure ("query: " ++ toDebugText q0) do read q0
        measure ("query: " ++ toDebugText q1) do read q1
        measure ("query: " ++ toDebugText q2) do read q2
        measure ("query: " ++ toDebugText q3) do read q3

durable.GinIndex.internal.test.printGinIndex :
  GinIndex input key id data ->{Remote} [(key, [(id, data)])]
durable.GinIndex.internal.test.printGinIndex = cases
  GinIndex _ _ _ index ->
    index
      |> OrderedTable.toStream
      |> Stream.toList
      |> (List.map cases
           (key, postingList) ->
             (key, postingList |> OrderedTable.toStream |> Stream.toList))

durable.GinIndex.named :
  Database
  -> Text
  -> (id -> id -> Ordering)
  -> (input -> [key])
  -> GinIndex input key id data
durable.GinIndex.named db name idOrdering indexOn =
  guid = "1ba5d1c428d247f20f97076f4babbb00798d935b219163a272a117960a47e6f6"
  index : OrderedTable key (OrderedTable id data)
  index =
    OrderedTable.named
      db (Text.join "-" [guid, name, "index"]) Universal.ordering
  GinIndex db idOrdering indexOn index

durable.GinIndex.query :
  GinIndex input key id data
  -> GinIndex.Query key
  ->{Remote} '{Remote, Orderator (id, '{Remote} data)} ()
durable.GinIndex.query ginIndex query =
  (GinIndex db idOrdering _ index) = ginIndex
  ordering = on idOrdering at1
  go : GinIndex.Query key -> '{Remote, Orderator (id, '{Remote} data)} ()
  go = cases
    Or q1 q2    -> unionBy ordering (go q1) (go q2)
    And q1 q2   -> intersectionBy ordering (go q1) (go q2)
    Minus q1 q2 -> subtractBy ordering (go q1) (go q2)
    Exact key   ->
      match OrderedTable.tryRead index key with
        None       -> Orderator.fromList []
        Some inner -> toOrderator inner
  go query

durable.Immutable.database : Immutable -> Database
durable.Immutable.database = cases Immutable db _ -> db

durable.Immutable.decrement.tx : Immutable -> Hashed a ->{Transaction} ()
durable.Immutable.decrement.tx c a =
  use Nat -
  (Immutable db tbl) = c
  (Hashed h) = a
  match Transaction.tryRead.tx tbl h with
    None          -> ()
    Some (any, 1) -> Transaction.delete.tx tbl h
    Some (any, n) -> Transaction.write.tx tbl h (any, n - 1)

durable.Immutable.doc : Doc
durable.Immutable.doc =
  {{
  A reference-counted immutable data-store, keyed by hash.

  {{ stability.new }}

  Use {Immutable.named} or {Immutable.nested} to create.

  Use {increment.tx} to store a value, incrementing its count. Use
  {decrement.tx} to decrement its count and delete if this causes the count to
  reach 0.

  @typecheck ```
  db = Database.named "db"
  blobs = Immutable.named db "store"
  transact db do
    h = increment.tx blobs 1
    one = Immutable.read.tx blobs h
    decrement.tx blobs h
    one
  ```
  }}

durable.Immutable.hashOf : Immutable -> a -> Hash
durable.Immutable.hashOf c a = Hash (blake2b_256 a)

durable.Immutable.increment.tx : Immutable -> a ->{Transaction} Hashed a
durable.Immutable.increment.tx c a =
  use Nat +
  use Transaction write.tx
  (Immutable db tbl) = c
  h = hashOf c a
  match Transaction.tryRead.tx tbl h with
    None          -> write.tx tbl h (Any a, 1)
    Some (any, n) -> write.tx tbl h (any, n + 1)
  Hashed h

durable.Immutable.named : Database -> Text -> Immutable
durable.Immutable.named db name = Immutable db (Table.Table name)

durable.Immutable.nested : Database -> a -> Immutable
durable.Immutable.nested db parent =
  Immutable db (nestedTable durable.Immutable.nested parent)

durable.Immutable.read : Immutable -> Hashed a ->{Remote} a
durable.Immutable.read store h = toRemote do read.cached store h

durable.Immutable.read.cached :
  Immutable -> Hashed a ->{Exception, Storage, Scratch} a
durable.Immutable.read.cached store h = match lookupHashed h with
  Some a -> a
  None   ->
    a = read.state store h
    saveHashed h a
    a

durable.Immutable.read.cached.doc : Doc
durable.Immutable.read.cached.doc =
  {{
  `` read.cached store h `` performs a cached read of `h`, first consulting
  {type Scratch} before falling back to a {type Storage} read from `store`.

  **Also see:** {Immutable.read} which does the same but in {type Remote}.
  }}

durable.Immutable.read.doc : Doc
durable.Immutable.read.doc =
  use Immutable read
  use Remote both
  use increment tx
  {{
  Performs a cached read of a {type Hashed} value, a building block for high
  performance queries over durable data structures.

  This first consults {type Scratch} on the current location; if not found, it
  calls {read.state} and adds the result to {type Scratch}.

  The operation requires {type Remote} and can be used with various operations
  like {Remote.parMap} or {both} for parallel processing.

  @typecheck ```
  db = Database.named "db"
  store = Immutable.named db "data"
  h1 = transact db do tx store [1, 2, 3]
  h2 = transact db do tx store [2, 3, 4, 5]
  both (do read store h1) do read store h2
  ```
  }}

durable.Immutable.read.state : Immutable -> Hashed a ->{Exception, Storage} a
durable.Immutable.read.state store h =
  transact (Immutable.database store) do Immutable.read.tx store h

durable.Immutable.read.tx : Immutable -> Hashed a ->{Transaction, Exception} a
durable.Immutable.read.tx ca = cases
  Hashed h -> unsafeExtract (at1 (Transaction.read (Immutable.table ca) h))

durable.Immutable.table : Immutable -> Table Hash (Any, Nat)
durable.Immutable.table = cases Immutable _ tbl -> tbl

durable.Immutable.tryRead.tx : Immutable -> Hashed a ->{Transaction} Optional a
durable.Immutable.tryRead.tx ca = cases
  Hashed h ->
    match Transaction.tryRead.tx (Immutable.table ca) h with
      None          -> None
      Some (any, _) -> Some (unsafeExtract any)

durable.Knn.add : Knn m v a -> (v, a) ->{Exception, Storage} ()
durable.Knn.add knn ma = transact (Knn.database knn) do Knn.add.tx knn ma

durable.Knn.add.tx : Knn m v a -> (v, a) ->{Transaction, Exception} ()
durable.Knn.add.tx c ma =
  use Abort abort
  use Cell database nested write.tx
  use Float < >
  use List ++ :+ drop size take unsafeAt
  use Nat + - >=
  inj = inject c
  dist = metric.distance c
  go : Kit m v a -> (v, a) ->{Transaction, Exception} ()
  go k ma =
    (Kit arity cell) = k
    (subs0, here, down) = Cell.read.tx cell
    use Cell.write tx
    pushDown subs ma =
      handle argMin.indexed (cases (m, a2) -> dist m (at1 ma)) subs
      with cases
        { Exception.raise e -> _ } ->
          msg =
            """
            Knn implementation bug: a node with elements to send down
            must have at least one child tree.

            This should be ensured because the only time we add to 'down'
            queue is when we have a full set of children. Otherwise we
            create new child nodes.
            """
          bug msg
        { ((m, child), i) }        ->
          go child ma
          replace i (metric.combine c m (at1 ma), child) subs
    down' = drop 2 down
    subs = match down with
      [] -> subs0
      _  ->
        r = List.foldLeft pushDown subs0 (take 2 down)
        tx cell (r, here, down')
        r
    match arity - size here with
      0
        | size subs >= arity  -> tx cell (subs, [], down' ++ here)
        | List.isEmpty subs   ->
          seed = unsafeAt 0 here
          sub = nested (database cell) cell ([], [], [])
          tx sub ([], [seed], [])
          tx cell (subs :+ (inj (at1 seed), Kit arity sub), drop 1 here, down')
        | otherwise           ->
          _ =
            """
            We seed the new child branch with the element
            that is furthest from any existing child branch.
            """
          let
            (seed, here') =
              go best bestInd i =
                if i >= size subs then bestInd
                else
                  handle
                    (m, _) = unsafeAt i here
                    f = cases
                      (m2, _) ->
                        d = dist m2 m
                        if d < best then abort else d
                    Float.minBy f subs
                  with cases
                    { abort -> _ } -> go best bestInd (i + 1)
                    { a } ->
                      if a > best then go a i (i + 1)
                      else go best bestInd (i + 1)
              i = go NegativeInfinity 0 0
              (unsafeAt i here, take i here ++ drop (i + 1) here)
            sub = nested (database cell) cell ([], [], [])
            tx sub ([], [seed], [])
            tx cell (subs :+ (inj (at1 seed), Kit arity sub), here', down')
      nonzero -> tx cell (subs, here :+ ma, down')
  go (kit c) ma

durable.Knn.arity : Knn m v a -> Nat
durable.Knn.arity = cases Knn _ _ _ _ (Kit arity _) -> arity

durable.Knn.database : Knn m v a -> Database
durable.Knn.database = cases Knn db _ _ _ _ -> db

durable.Knn.doc : Doc
durable.Knn.doc =
  use Knn add lookup
  use Knn.add tx
  use unison_base_3_22_0 ignore
  {{
  A durable data structure for approximate K-nearest neighbors search, useful
  for many domains:

  * Fuzzy document search (finds results similar to the query, without
    requiring exact matches)
  * Long-term memory for chatbots (for a given prompt, finds related fragments
    of text for refining the prompt, without requiring exact matches)
  * Spell correction (finds words similar to the query)

  {{ stability.new }}

  {type Knn} is generic and can be used with any type `m` as long as you
  provide a distance metric and a way of combining `m` to produce a summary.
  For instance, a very simple `m` type might be a set of words, combined using
  set union, and using the
  [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) as the
  distance function.

  For each item stored in the index, you can associate with it a value of type
  `a` (it might be a URL to the original source, say).

  Here's code which creates a fuzzy uses {type SampleWords}, a basic text
  summary type:

  @typecheck ```
  db = Database.named "db"
  fuzzySearch : Knn (SampleWords Text) (SampleWords Text) Text
  fuzzySearch =
    Knn.named db "myKnn" 32 Function.id SampleWords.merge SampleWords.distance
  ignore "Here's code to add to the index"
  docUrl = "/my/document/url"
  doc =
    """
    Imagine that this is a very long document
    with lots of words in it.
    """
  add fuzzySearch (fromWords 100 doc, docUrl)
  ignore "And here's example lookup code..."
  lookup fuzzySearch 10
  ```

  The {arity} (above it's `32`) defines the branching factor of the tree.
  Making it higher means fewer trips to storage and more accurate results, but
  also means that queries may fetch more data than necessary.

  # Common functions

    Create an index using {#28gbcn9os3}, {#spkdqqhsfq}, {Knn.named}, or
    {Knn.nested}. Then, to add to the index, use {add} or {tx}:

        @signatures{add, tx}

    And to do a lookup, use {lookup}:

        @signature{lookup}

    One caveat: the number of results returned from {lookup} is currently
    bounded by {arity}. You can repeat the {lookup} with the last element
    returned to expand the results further.
  }}

durable.Knn.kit : Knn m v a -> Kit m v a
durable.Knn.kit = cases Knn _ _ _ _ k -> k

durable.Knn.lookup : Knn m v a -> Nat -> v ->{Remote} [(Float, (v, a))]
durable.Knn.lookup knn k v0 =
  use List ++
  m = inject knn v0
  md = metric.distance knn
  dist : v -> Float
  dist v = md m v
  go : Kit m v a -> [(v, a)]
  go = cases
    Kit arity cell ->
      (subs, here, down) = Cell.read cell
      here
        ++ down
        ++ Optional.fold
             (do []) (at2 >> go) (tryArgMin (cases (m, _) -> md m v0) subs)
  go (kit knn)
    |> List.map (p -> (dist (at1 p), p))
    |> List.sortBy at1
    |> Stream.fromList
    |> (distinctAdjacentBy cases (d, (m, a)) -> a)
    |> Stream.take k
    |> Stream.toList

durable.Knn.metric.combine : Knn m v a -> m -> v -> m
durable.Knn.metric.combine = cases Knn _ _ s _ _ -> s

durable.Knn.metric.distance : Knn m v a -> m -> v -> Float
durable.Knn.metric.distance = cases Knn _ _ _ s _ -> s

durable.Knn.metric.inject : Knn m v a -> v -> m
durable.Knn.metric.inject = cases Knn _ inj _ _ _ -> inj

durable.Knn.named :
  Database
  -> Text
  -> Nat
  -> (v -> m)
  -> (m -> v -> m)
  -> (m -> v -> Float)
  -> Knn m v a
durable.Knn.named db name arity inj combine distance =
  use Nat <
  if arity < 2 then bug ("arity must be at least 2", arity) else ()
  kit = Kit arity (Cell.named db name ([], [], []))
  Knn db inj combine distance kit

durable.Knn.nested :
  Database
  -> parent
  -> Nat
  -> (v -> m)
  -> (m -> v -> m)
  -> (m -> v -> Float)
  -> Knn m v a
durable.Knn.nested db parent arity inj combine distance =
  name = blake2b_256 (durable.Knn.nested, parent) |> up.Bytes.toBase32Hex.text
  Knn.named db name arity inj combine distance

durable.Knn.SampleWords.add : txt -> SampleWords txt -> SampleWords txt
durable.Knn.SampleWords.add txt = cases
  SampleWords n samples ->
    SampleWords n (samples List.:+ txt) |> SampleWords.trim

durable.Knn.SampleWords.add.doc : Doc
durable.Knn.SampleWords.add.doc = {{ Add a word to this summary. }}

durable.Knn.SampleWords.bound : SampleWords txt -> Nat
durable.Knn.SampleWords.bound = cases SampleWords bound _ -> bound

durable.Knn.SampleWords.bound.doc : Doc
durable.Knn.SampleWords.bound.doc =
  {{ The number of words this {type SampleWords} is allowed to contain. }}

durable.Knn.SampleWords.compressedSize : SampleWords txt -> Nat
durable.Knn.SampleWords.compressedSize =
  Value.value >> Value.serialize >> gzip.compress >> Bytes.size

durable.Knn.SampleWords.distance : SampleWords txt -> SampleWords txt -> Float
durable.Knn.SampleWords.distance = normalizedGzipDistance

durable.Knn.SampleWords.distance.doc : Doc
durable.Knn.SampleWords.distance.doc =
  use SampleWords distance
  {{
  `` distance s1 s2 `` returns a value between 0 and 1 which will be:

  * close to `0.0` if `s1` and `s2` are similar
  * closer to `1.0` if they are dissimilar

  ```
  s1 = fromWords 10 "This is a sentence of several words"
  s2 = fromWords 10 "This is another sentence of several words"
  distance s1 s2
  ```

  It uses {normalizedGzipDistance} for its implementation.
  }}

durable.Knn.SampleWords.doc : Doc
durable.Knn.SampleWords.doc =
  use SampleWords distance merge
  {{
  A very simple type for text summarization. A {type SampleWords} can be
  created using functions like {fromWords}, {fromWords.alphanumeric} and
  combined using {merge}:

      @signatures{fromWords, merge}

  The {type Nat} parameter is the size bound on the {type SampleWords}. When a
  {type SampleWords} exceeds this size, random words are deleted from the
  summary until it's below the size bound.

  The type is equipped with a distance metric which returns a value between 0
  and 1 indicating the similarity of two summaries:

      @signature{distance}

  Here's an example:

  ```
  d = fromWords 5 "This is a sentence with multiple words"
  d2 = fromWords 5 "This is a another sentence with multiple words"
  d3 =
    fromWords
      5
      "Once upon a time, in a serene valley surrounded by towering mountains..."
  (distance d d, distance d d2, distance d d3)
  ```

  A value of 0 means the summaries are perfectly similar. A value of 1 means
  the summaries are as different as possible.

  # Example

    ```
    file =
      """
      Etiam eleifend tellus vitae nunc porttitor pretium.
      In suscipit scelerisque risus faucibus lobortis.
      Praesent porta ipsum vel nibh blandit, eget tristique erat tempor.

      Vestibulum sed porta massa.
      Sed mattis velit tellus, non pretium eros fringilla eu.
      Suspendisse sed neque eleifend, tempor turpis sed, finibus eros.
      Suspendisse sagittis ex quis neque sagittis facilisis.

      Suspendisse varius ex ex, a viverra magna malesuada nec.
      Fusce at vestibulum dui.
      Proin nec elementum ante.
      """
    pars = paragraphs file
    List.foldBalanced (fromWords 20) merge (SampleWords.empty 20) pars
    ```
  }}

durable.Knn.SampleWords.empty : Nat -> SampleWords txt
durable.Knn.SampleWords.empty n = SampleWords n []

durable.Knn.SampleWords.fromTokens : Nat -> [txt] -> SampleWords txt
durable.Knn.SampleWords.fromTokens n tokens =
  SampleWords n tokens |> SampleWords.trim

durable.Knn.SampleWords.fromTokens.doc : Doc
durable.Knn.SampleWords.fromTokens.doc =
  {{
  Create a {type SampleWords} from an arbitrary list of tokens.

  ```
  fromTokens 10 ["(", "1", "+", "1", ")"]
  ```

  The tokens need not be {type Text}, they can be of any type, so this type
  works just as well for a custom textual type (for instance, the tokens of a
  programming language)

  ```
  fromTokens 10 [3, 1, 2, 2, 4]
  ```

  **See also:** {fromWords}, {fromWords.alphanumeric}
  }}

durable.Knn.SampleWords.fromWords : Nat -> Text -> SampleWords Text
durable.Knn.SampleWords.fromWords n t = fromTokens n (util.Text.words t)

durable.Knn.SampleWords.fromWords.alphanumeric :
  Nat -> Text -> SampleWords Text
durable.Knn.SampleWords.fromWords.alphanumeric n t =
  fromTokens n (words.alphanumeric t)

durable.Knn.SampleWords.fromWords.alphanumeric.doc : Doc
durable.Knn.SampleWords.fromWords.alphanumeric.doc =
  {{
  Like {fromWords} but only keeps alphanumeric segments of the {type Text}.

  ```
  fromWords.alphanumeric 10 "This.is(a-sentence,with lots 'o punctuation."
  ```
  }}

durable.Knn.SampleWords.fromWords.doc : Doc
durable.Knn.SampleWords.fromWords.doc =
  {{
  Create a

  ```
  fromWords.alphanumeric 10 "This.is(a-sentence,with lots 'o punctuation."
  ```
  }}

durable.Knn.SampleWords.merge :
  SampleWords txt -> SampleWords txt -> SampleWords txt
durable.Knn.SampleWords.merge s1 s2 = mergeUntrimmed s1 s2 |> SampleWords.trim

durable.Knn.SampleWords.merge.doc : Doc
durable.Knn.SampleWords.merge.doc =
  {{
  `` SampleWords.merge s1 s2 `` combines two summaries, reducing the resulting
  size to {bound}.
  }}

durable.Knn.SampleWords.mergeUntrimmed :
  SampleWords txt -> SampleWords txt -> SampleWords txt
durable.Knn.SampleWords.mergeUntrimmed = cases
  SampleWords n1 s1, SampleWords n2 s2 ->
    SampleWords (Nat.min n1 n2) (s1 List.++ s2)

durable.Knn.SampleWords.mergeUntrimmed.doc : Doc
durable.Knn.SampleWords.mergeUntrimmed.doc =
  {{
  Like {SampleWords.merge}, but doesn't trim the result. Useful if supplying a
  custom trimming function.
  }}

durable.Knn.SampleWords.trim : SampleWords txt -> SampleWords txt
durable.Knn.SampleWords.trim = cases
  SampleWords n samples
    | List.size samples Nat.< n -> SampleWords n samples
    | otherwise                 ->
      use List ++ size
      use Nat - /
      seed = murmurHash (List.at (size samples / 2) samples)
      sample' = splitmix seed do
        i = Random.natIn 0 (size samples - 1)
        let
          (before, after) = List.splitAt i samples
          before ++ List.drop 1 after
      SampleWords n sample' |> durable.Knn.SampleWords.trim

durable.Knn.SampleWords.trim.doc : Doc
durable.Knn.SampleWords.trim.doc =
  {{
  Trims this sample to fit within {bound}. This is done automatically after
  every {SampleWords.merge} or {SampleWords.add}.

  It's idempotent. If the sample already fits, it's returned unchanged.
  }}

durable.LICENSE : License
durable.LICENSE = unison_base_3_22_0.LICENSE

durable.LinearLog.add.tx : LinearLog a -> a ->{Transaction} ()
durable.LinearLog.add.tx = cases
  LinearLog c tbl ->
    a -> let
      use Nat +
      n = modify.tx c (n -> (n + 1, n))
      Transaction.write.tx tbl n a

durable.LinearLog.at.tx : LinearLog a -> Nat ->{Transaction, Exception} a
durable.LinearLog.at.tx = cases LinearLog _ tbl -> Transaction.read tbl

durable.LinearLog.delete.tx : LinearLog a -> Nat ->{Transaction} ()
durable.LinearLog.delete.tx = cases
  LinearLog _ tbl -> Transaction.delete.tx tbl

durable.LinearLog.doc : Doc
durable.LinearLog.doc =
  use LinearLog add.tx
  {{
  A linear sequence of known size.

  {{ stability.new }}

  Use {LinearLog.named} or {LinearLog.nested} to create. {size.tx} tracks the
  size, {add.tx} adds to the end of the log, and {at.tx} access any position in
  the log.

  A short example:

  @typecheck ```
  db = Database.named "db"
  log = LinearLog.named db "myLog"
  transact db do
    add.tx log "event1"
    add.tx log "event2"
    add.tx log "event3"
    at.tx log 2
  ```

  **Common functions:**

      @signatures{LinearLog.named, add.tx, at.tx, size.tx}
  }}

durable.LinearLog.named : Database -> Text -> LinearLog a
durable.LinearLog.named db name =
  use Text ++
  LinearLog (Cell.named db (name ++ "-size") 0) (Table.Table name)

durable.LinearLog.nested : Database -> parent -> LinearLog v
durable.LinearLog.nested db parent =
  c = nestedTable durable.LinearLog.nested (parent, "values")
  LinearLog (Cell.nested db (parent, "size") 0) c

durable.LinearLog.size.tx : LinearLog a ->{Transaction} Nat
durable.LinearLog.size.tx = cases LinearLog c _ -> Cell.read.tx c

durable.OrderedTable.database : OrderedTable k v -> Database
durable.OrderedTable.database = cases BTree database _ _ _ _ -> database

durable.OrderedTable.database.doc : Doc
durable.OrderedTable.database.doc =
  {{
  Gets the {type Database} of a {type OrderedTable}. This is the database in
  which the {type OrderedTable} is stored.
  }}

durable.OrderedTable.delete : OrderedTable k v -> k ->{Remote} ()
durable.OrderedTable.delete tree k =
  toRemote do
    transact (OrderedTable.database tree) do OrderedTable.delete.tx tree k

durable.OrderedTable.delete.doc : Doc
durable.OrderedTable.delete.doc =
  {{
  Deletes a key from a {type OrderedTable}. Takes a {type OrderedTable} and a
  key, and deletes the key from the {type OrderedTable}. If the key does not
  exist, this operation has no effect.

  # Example

    This deletes the key "key" from a {type OrderedTable} named
    "my-orderedMap":

    @typecheck ```
    db = Database.named "my-db"
    orderMap = OrderedTable.named db "my-orderedMap" Universal.ordering
    OrderedTable.delete orderMap "key"
    ```
  }}

durable.OrderedTable.delete.tx : OrderedTable k v -> k ->{Transaction} ()
durable.OrderedTable.delete.tx tree k =
  use Boolean not
  use List firstIndexOf isEmpty
  use Nat - ==
  use Node id keys keys.modify
  use Transaction delete.tx tryRead.tx write.tx
  use internal index values
  underflows : internal.Node k -> Boolean
  underflows node =
    if isLeaf node then node |> keys |> isEmpty
    else node |> pointers |> isEmpty
  outerDescendant direction node =
    if isLeaf node then node
    else
      match direction with
        Left _  -> node |> childAt tree 0 |> outerDescendant direction
        Right _ ->
          last = (node |> pointers |> List.size) - 1
          node |> childAt tree last |> outerDescendant direction
  repairLinkedList node = cases
    [] -> ()
    parent +: ancestors ->
      i = findKeySlot tree k parent
      if i == 0 then repairLinkedList node ancestors
      else
        previousLeaf =
          parent |> childAt tree (i - 1) |> outerDescendant Right()
        previousLeaf' = previousLeaf |> pointers.set (pointers node)
        write.tx (index tree) (id previousLeaf') previousLeaf'
  propagateDeletion node ancestors =
    if not (underflows node) then write.tx (index tree) (id node) node
    else
      delete.tx (index tree) (id node)
      when (isLeaf node) do repairLinkedList node ancestors
      match ancestors with
        [] -> ()
        parent +: ancestors ->
          i =
            firstIndexOf (id node) (pointers parent)
              |> getOrBug "parent doesn't point to child"
          node' =
            parent
              |> pointers.modify (deleteAt i)
              |> keys.modify (deleteAt (i - 1))
          propagateDeletion node' ancestors
  replaceKeyInIndex =
    do
      use Nat +
      go node out =
        when (not (isLeaf node)) do
          match node |> keys |> firstIndexOf k with
            None ->
              branchToTake = node |> findKeySlot tree k
              node' = node |> childAt tree branchToTake
              go node' out
            Some i ->
              targetLeaf =
                node |> childAt tree (i + 1) |> outerDescendant Left()
              targetKey =
                targetLeaf |> keys |> List.head |> getOrBug "leaf is empty"
              node' = node |> keys.modify (replace i targetKey)
              write.tx (index tree) (id node') node'
      match tryRead.tx (index tree) Root with
        Some root -> go root None
        None      -> ()
  match tryRead.tx (values tree) k with
    None -> ()
    Some _ ->
      delete.tx (values tree) k
      match findLeaf tree k with
        [] -> ()
        leaf +: ancestors ->
          leaf' =
            leaf
              |> keys.modify
                (deleteFirst (k1 -> eqBy (internal.ordering tree) k k1))
          propagateDeletion leaf' ancestors
          replaceKeyInIndex()

durable.OrderedTable.delete.tx.doc : Doc
durable.OrderedTable.delete.tx.doc =
  use OrderedTable.delete tx
  use Universal ordering
  {{
  Deletes a key from a {type OrderedTable} within a {type Transaction}. Takes a
  {type OrderedTable} and a key and deletes the key from the
  {type OrderedTable} within the current transaction. If the key does not
  exist, this operation has no effect.

  # Example

    This deletes the key "key" from two {type OrderedTable} tables named
    "table1" and "table2", using a transaction to ensure that the deletion is
    atomic:

    @typecheck ```
    db = Database.named "my-db"
    table1 = OrderedTable.named db "table1" ordering
    table2 = OrderedTable.named db "table2" ordering
    transact db do
      tx table1 "key"
      tx table2 "key"
    ```

  # See also

    * {type Transaction} for more information on transactions.
  }}

durable.OrderedTable.doc : Doc
durable.OrderedTable.doc =
  use OrderedTable database delete delete.tx from max maxKey min minKey named rangeClosed read read.tx search toStream tryRead tryRead.tx write write.tx
  {{
  A data type that acts as a sorted map with CRUD operations and range query
  capabilities.

  {{ final }}

  # Creating an OrderedTable

    You can create a {type OrderedTable} with:

        @signature{named}

    which takes a {type Database}, a name for your {type OrderedTable}, and a
    function that defines an ordering for the keys. The ordering function is
    used both for implementation reasons and to specify the order of results in
    range query operations. Note that in most cases you can just pass
    {Universal.ordering} here, unless you need some custom order.

    {named} is pure and idempotent and will return the same {type OrderedTable}
    every time it is invoked with the same arguments, so you don't have to add
    the result of {named} to your codebase, you can just refer to it by name
    wherever you need access to it. The name you pass to {named} is scoped by
    the {type Database}, which acts as a namespace in addition of being the
    unit of access control.

    {{
    docCallout
      (Some {{ 🚨 }})
      {{
      Keep in mind that when you create a {type OrderedTable}, its name __and__
      key-value types are persistent, so you should treat them with the same
      care you use with a database schema, and can't just change them without a
      data migration.

      For example, if you have `users: OrderedTable UserId User`, add a field
      to `User` and then issue an `update`, UCM will happily update the `users`
      term, but then queries will fail at runtime since you have a different
      type in persistent storage, i.e. the old version of `User`. In the future
      we might enhance the language to make UCM aware of which types are
      persistent.
      }} }}

    ## Tree fanout

       {type OrderedTable} achieves very good asymptotics for all its
       operations, based on its `maxFanout` parameter, i.e. the maximum number
       of keys held in each node of the tree. You can look at
       {type internal.Node} for further details, but the gist is that a higher
       `maxFanout` parameter increases performance. However, if `maxFanout` is
       too big, a node in the tree might breach the 400kb size limit.

       The {type OrderedTable} constructors default to `maxFanout = 50`, you
       can change it by calling {maxFanout.set}, but note that this has to be
       decided at construction time and cannot be changed later.

  # CRUD operations

    {type OrderedTable} support transactional CRUD operations akin to the ones
    in {type Storage}:

        @signatures{tryRead.tx, read.tx, write.tx, delete.tx}

    Note that the {type OrderedTable} includes the {type Database} it's hosted
    in. This means that running a transaction on {type OrderedTable} will
    normally be in the form
    {{
    docExample 2 do
      yourOrderedTable yourTransaction ->
        transact (database yourOrderedTable) yourTransaction
    }}, where:

        @signatures{transact, database}

    There are also convenient shortcuts for transactions made of a single
    operation:

    {{
    docSignature
      [ docEmbedSignatureLink do tryRead
      , docEmbedSignatureLink do read
      , docEmbedSignatureLink do write
      , docEmbedSignatureLink do delete
      ] }}

  # Maximum and minimum

    You can get the key-value pair with maximum key via {max}, and with minimum
    key via {min}. You also have {minKey} and {maxKey} if you only care about
    the key. All these operations have transactional variants whose names end
    in `.tx` (e.g. {min.tx})

  # Query operations

    {type OrderedTable} also offers an api for iteration and querying based on
    the following operations:

    @inlineSignature{rangeClosed} {{ Linebreak }} Takes the lower and upper
    bound of the range, both inclusive.

    @inlineSignature{from} {{ Linebreak }} Takes the inclusive lower bound and
    iterates over all elements from there.

    @inlineSignature{toStream} {{ Linebreak }} Iterates over all the elements.

    @inlineSignature{search} {{ Linebreak }} Takes a `k -> Ordering` which can
    close over any value and compare it with the keys in the
    {type OrderedTable}. All other operations are defined in terms of {search},
    but there generally should be little need to use it directly.

    The functions above differ in the inputs specified for the query, but have
    similar behaviour for the output:

    * Results are ordered from smallest to highest.
    * Results are in the {type Stream} ability, since there is no bound on how
      many there could be.
    * Similarly, because there could be an unbounded number of items returned,
      they aren't fetched in a single transaction. Note that this means that
      results aren't read from a consistent snapshot.
    * Whenever possible, the implementation tries to fetch results
      concurrently.

    Every function has two additional variants, one for when you are only
    fetching the keys, and one for when you're fetching the keys in chunks
    corresponding to the leaves of the {type OrderedTable}, you can invoke them
    by appending `.keys` and `.keys.chunked` to the function name respectively
    (e.g. {rangeClosed.keys}, {from.keys.chunked}).

    Also, each function and each variant has a transactional version that you
    can obtain by appending `.tx` to the function name (e.g. {toStream.tx},
    {search.keys.tx}, {rangeClosed.keys.chunked.tx}). This is useful when you
    know the returned range is small and you want to read it transactionally,
    or when you want to change how to delimit the transaction boundaries in a
    large result set. {Stream.toList} and {Stream.uncons} are useful
    {type Stream} functions to be aware of for the former and latter case
    respectively.

  # Indexes and prefix queries

    If you are used to a relational setting, it might help you to think of
    {type OrderedTable} not as a table, but as an __index__ . Therefore, a
    common technique to support queries along multiple axes is to build
    __multiple__ {type OrderedTable} which are all updated in a single
    transaction.

    For example, let's say users have unique ids but can share an avatar:

        @source{UserInfo.UserInfo}

    we can manage users by their id with a `OrderedTable Text UserInfo`, and
    also keep track of how many users share the same avatar with a
    `OrderedTable URI Nat`, by making sure both indexes are kept in sync
    transactionally. For example, when adding a new user we would have to:

        @source{addUser}

    Let's say we want to generalise the example above so that instead of simply
    counting how many users have the same avatar, we want to actually query,
    i.e. fetch all the users with a specific avatar. We basically need to use
    transactions to maintain a reverse index, but we cannot simply have
    `OrderedTable Uri UserInfo` since multiple users share the same URI and
    would override each other. The solution is to use a tuple of
    `(avatar, userId)` as the key, where the last element of the tuple is a
    unique id: `OrderedTable (URI, Text) UserInfo `.

    However, we now have a problem with our query: to get all the users with a
    certain avatar we would have to know their id, whereas our query only knows
    the avatar {type URI}.

    To solve this problem, {type OrderedTable} offers variations of
    {rangeClosed} and {from} that perform __prefix queries__. The two most
    common versions are {rangeClosed.prefix} and {rangeClosed.prefix.tx}, but
    note that all the variations and semantics described in the section on
    Query Operations apply (i.e. you have {from.prefix.keys.tx},
    {rangeClosed.prefix.keys} etc.).

    Let's have a look at the type of {rangeClosed.prefix} to understand how to
    use it:

        @signature{rangeClosed.prefix}

    Just like {rangeClosed}, it takes the upper and lower bound of the search,
    but note that this time they don't have the same type as the keys of the
    {type OrderedTable}, so that when the key is a tuple, you can specify the
    bounds in terms of the first component of the tuple (in our example, the
    avatar {type URI}).

    You also have to pass an ordering function that can compare a prefix to the
    full tuple, but note that this ordering has to be consistent with the
    ordering passed on construction of the {type OrderedTable}. In practice,
    this means that you can only do __prefix__ searches rather than arbitrary
    tuple comparisons, so the order of elements in the tuple is important: we
    want to key our {type OrderedTable} over `(avatarUri, userId)`, and not
    `(userId, avatarUri)`.

    In the super common case in which you are using a tuple of two elements as
    your key, and want to do a prefix query on the first component using the
    universal ordering, you can use {{
    docLink (docEmbedTermLink do prefixOrdering) }} as the ordering function,
    which means that we can query all users with a given avatar by just passing
    that avatar as both the lower and upperbound, like:

        @source{getUsersByAvatar}

    Of course, you can also pass different upper and lower bounds to
    {rangeClosed.prefix}, for example you could give a start and end date to
    get all blogs published in a time period given a
    `OrderedTable (LocalDate, Blog.Id) Blog `

  # Full api & Performance notes

    The performance of {type OrderedTable} operations is defined in terms of
    `O(number of sequential accesses to storage)`, with:

    * `B = maxFanout / 2`
    * `N = number of items in the tree`
    * For query operations, `T = number of items in range`

    {{ docTable
      [[{{ **Operation** }}, {{ **O(..)** }}], [{{
          * {tryRead.tx}
          * {read.tx}
          * {write.tx} to existing key
          * {tryRead}
          * {read}
          * {{ docLink (docEmbedTermLink do write) }} to existing key
          }}, {{ `O(1)` }}], [{{
          * {write.tx} to new key
          * {delete.tx}
          * {{ docLink (docEmbedTermLink do write) }} to new key
          * {delete}
          * {min}
          * {min.tx}
          * {minKey}
          * {minKey.tx}
          * {max}
          * {max.tx}
          * {maxKey}
          * {maxKey.tx}
          }}, {{ `O(log_B(N))` }}], [{{
          * {rangeClosed}
          * {rangeClosed.keys}
          * {rangeClosed.keys.tx}
          * {rangeClosed.keys.chunked}
          * {rangeClosed.keys.chunked.tx}
          * {rangeClosed.prefix}
          * {rangeClosed.prefix.keys}
          * {rangeClosed.prefix.keys.tx}
          * {rangeClosed.prefix.keys.chunked}
          * {rangeClosed.prefix.keys.chunked.tx}
          * {from}
          * {from.keys}
          * {from.keys.tx}
          * {from.keys.chunked}
          * {from.keys.chunked.tx}
          * {from.prefix}
          * {from.prefix.keys}
          * {from.prefix.keys.tx}
          * {from.prefix.keys.chunked}
          * {from.prefix.keys.chunked.tx}
          * {toStream}
          * {toStream.keys}
          * {toStream.keys.tx}
          * {toStream.keys.chunked}
          * {toStream.keys.chunked.tx}
          * {search}
          * {search.keys}
          * {search.keys.tx}
          * {search.keys.chunked}
          * {search.keys.chunked.tx}
          }}, {{ `O(log_B(N) + T/B)` }}], [{{
          * {rangeClosed.tx}
          * {from.tx}
          * {toStream.tx}
          * {search.tx}
          }}, {{ `O(log_B(N) + T)` }}]] }}
  }}

durable.OrderedTable.doc.addUser :
  OrderedTable Text UserInfo
  -> OrderedTable URI Nat
  -> UserInfo
  ->{Transaction, Random} ()
durable.OrderedTable.doc.addUser users avatarCount user =
  use Nat +
  use OrderedTable write.tx
  write.tx users (UserInfo.id user) user
  avatar = avatarUrl user
  count = OrderedTable.tryRead.tx avatarCount avatar |> Optional.getOrElse 0
  write.tx avatarCount avatar (count + 1)

durable.OrderedTable.doc.getUsersByAvatar :
  OrderedTable (URI, Text) UserInfo -> URI -> '{Remote, Stream UserInfo} ()
durable.OrderedTable.doc.getUsersByAvatar usersByAvatar avatar =
  rangeClosed.prefix usersByAvatar prefixOrdering avatar avatar
    |> Stream.map at2

durable.OrderedTable.from :
  OrderedTable k v -> k -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.from tree start =
  from.prefix tree (internal.ordering tree) start

durable.OrderedTable.from.doc : Doc
durable.OrderedTable.from.doc =
  {{
  Returns a {type Stream} of key-value pairs from a {type OrderedTable} where
  the key is greater than or equal to the given key, according to the ordering
  of the {type OrderedTable}.

  # Example

    This gets all key-value pairs from a {type OrderedTable} named
    "my-ordered-map" that are greater than or equal to the key "key":

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" Universal.ordering
    OrderedTable.from orderedMap "key"
      |> (Stream.foreach cases (k, v) -> info "Key-value pair" [(k, v)])
    ```
  }}

durable.OrderedTable.from.keys :
  OrderedTable k v -> k -> '{Remote, Stream k} ()
durable.OrderedTable.from.keys tree start =
  from.prefix.keys tree (internal.ordering tree) start

durable.OrderedTable.from.keys.chunked :
  OrderedTable k v -> k -> '{Remote, Stream [k]} ()
durable.OrderedTable.from.keys.chunked tree start =
  from.prefix.keys.chunked tree (internal.ordering tree) start

durable.OrderedTable.from.keys.chunked.doc : Doc
durable.OrderedTable.from.keys.chunked.doc =
  {{
  Returns a {type Stream} of keys from a {type OrderedTable} that are greater
  than or equal to the given key, according to the ordering of the
  {type OrderedTable}. The keys are returned in chunks, as {type List}s of
  keys, with each chunk containing a number of keys according to the page size
  of the {type OrderedTable}.

  # Example

    This gets all keys from a {type OrderedTable} named "my-ordered-map" that
    are greater than or equal to the key "key", in chunks, and logs each chunk
    of keys:

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" Universal.ordering
    from.keys.chunked orderedMap "key"
      |> Stream.foreach
        (keys -> info "Keys" (List.map (Tuple.pair "key") keys))
    ```
  }}

durable.OrderedTable.from.keys.chunked.tx :
  OrderedTable k v -> k -> '{Transaction, Stream [k]} ()
durable.OrderedTable.from.keys.chunked.tx tree start =
  from.prefix.keys.chunked.tx tree (internal.ordering tree) start

durable.OrderedTable.from.keys.chunked.tx.doc : Doc
durable.OrderedTable.from.keys.chunked.tx.doc =
  {{
  A version of {from.keys.chunked} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.from.keys.doc : Doc
durable.OrderedTable.from.keys.doc =
  {{
  Returns a {type Stream} of keys from a {type OrderedTable} that are greater
  than or equal to the given key, according to the ordering of the
  {type OrderedTable}.

  # Example

    This gets all keys from a {type OrderedTable} named "my-ordered-map" that
    are greater than or equal to the key "key":

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" Universal.ordering
    from.keys orderedMap "key" |> Stream.foreach (k -> info "Key" [("key", k)])
    ```
  }}

durable.OrderedTable.from.keys.tx :
  OrderedTable k v -> k -> '{Transaction, Stream k} ()
durable.OrderedTable.from.keys.tx tree start =
  from.prefix.keys.tx tree (internal.ordering tree) start

durable.OrderedTable.from.keys.tx.doc : Doc
durable.OrderedTable.from.keys.tx.doc =
  {{
  A version of {from.keys} that operates within a {type Transaction}, ensuring
  that the operation is atomic.
  }}

durable.OrderedTable.from.prefix :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Remote, Stream (k, v)} ()
durable.OrderedTable.from.prefix tree ordering start =
  OrderedTable.search tree (ordering start)

durable.OrderedTable.from.prefix.doc : Doc
durable.OrderedTable.from.prefix.doc =
  use Universal ordering
  {{
  Returns a {type Stream} of key-value pairs from a {type OrderedTable}
  starting with the first key that has the given prefix. The key-value pairs
  are returned in order according to the key's ordering relative to the prefix
  using the given ordering function.

  # Example

    This gets a stream of key-value pairs from a {type OrderedTable} table
    named "my-ordered-map", starting with the first key that has the prefix
    "ora":

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" ordering
    from.prefix orderedMap ordering "ora"
      |> (Stream.foreach cases (k, v) -> info "Key-value pair" [(k, v)])
    ```
  }}

durable.OrderedTable.from.prefix.keys :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Remote, Stream k} ()
durable.OrderedTable.from.prefix.keys tree ordering start =
  search.keys tree (ordering start)

durable.OrderedTable.from.prefix.keys.chunked :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Remote, Stream [k]} ()
durable.OrderedTable.from.prefix.keys.chunked tree ordering start =
  search.keys.chunked tree (ordering start)

durable.OrderedTable.from.prefix.keys.chunked.doc : Doc
durable.OrderedTable.from.prefix.keys.chunked.doc =
  {{ A version of {from.prefix.keys} that returns the keys in chunks. }}

durable.OrderedTable.from.prefix.keys.chunked.tx :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Transaction, Stream [k]} ()
durable.OrderedTable.from.prefix.keys.chunked.tx tree ordering start =
  search.keys.chunked.tx tree (ordering start)

durable.OrderedTable.from.prefix.keys.chunked.tx.doc : Doc
durable.OrderedTable.from.prefix.keys.chunked.tx.doc =
  {{
  A version of {from.prefix.keys.chunked} that operates within a
  {type Transaction}, ensuring that the operation is atomic.
  }}

durable.OrderedTable.from.prefix.keys.doc : Doc
durable.OrderedTable.from.prefix.keys.doc =
  use Universal ordering
  {{
  Returns a {type Stream} of keys from a {type OrderedTable} starting with the
  first key that has the given prefix. The keys are returned in order according
  to their ordering relative to the prefix using the given ordering function.

  # Example

    This gets all keys from a {type OrderedTable} named "my-ordered-map",
    starting with the first key that has the prefix "ora":

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" ordering
    from.prefix.keys orderedMap ordering "ora"
      |> Stream.foreach (k -> info "Key" [("key", k)])
    ```
  }}

durable.OrderedTable.from.prefix.keys.tx :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Transaction, Stream k} ()
durable.OrderedTable.from.prefix.keys.tx tree ordering start =
  search.keys.tx tree (ordering start)

durable.OrderedTable.from.prefix.keys.tx.doc : Doc
durable.OrderedTable.from.prefix.keys.tx.doc =
  {{
  A version of {from.prefix.keys} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.from.prefix.tx :
  OrderedTable k v
  -> (prefix ->{g} k -> Ordering)
  -> prefix
  ->{g} '{Transaction, Exception, Batch, Stream (k, v)} ()
durable.OrderedTable.from.prefix.tx tree ordering start =
  search.tx tree (ordering start)

durable.OrderedTable.from.prefix.tx.doc : Doc
durable.OrderedTable.from.prefix.tx.doc =
  {{
  A version of {from.prefix} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.from.tx :
  OrderedTable k v -> k -> '{Transaction, Exception, Batch, Stream (k, v)} ()
durable.OrderedTable.from.tx tree start =
  from.prefix.tx tree (internal.ordering tree) start

durable.OrderedTable.from.tx.doc : Doc
durable.OrderedTable.from.tx.doc =
  {{
  A version of {OrderedTable.from} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.internal.bulkFetchValues :
  OrderedTable k v -> '{Remote, Stream [k]} () -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.internal.bulkFetchValues tree stream =
  stream
    |> (concatMap cases
      page ->
        toRemote do
          batchRead (OrderedTable.database tree) do
            page
              |> List.map (k -> (k, forkRead (internal.values tree) k))
              |> (List.map cases
                   (k, read) ->
                     tryAwaitRead read |> Optional.map (Tuple.pair k))
              |> List.somes)

durable.OrderedTable.internal.bulkFetchValues.tx :
  OrderedTable k v -> '{g, Stream [k]} () -> '{g, Batch, Stream (k, v)} ()
durable.OrderedTable.internal.bulkFetchValues.tx tree stream =
  stream
    |> (concatMap cases
      page ->
        page
          |> List.map (k -> (k, forkRead (internal.values tree) k))
          |> (List.map cases
               (k, read) -> tryAwaitRead read |> Optional.map (Tuple.pair k))
          |> List.somes)

durable.OrderedTable.internal.childAt :
  OrderedTable k v -> Nat -> internal.Node k ->{Transaction} internal.Node k
durable.OrderedTable.internal.childAt tree i node =
  if isLeaf node then bug "BTree: leaf pointers don't have children"
  else
    node |> pointers |> List.at i |> (cases
      None    -> bug "BTree: invalid children index"
      Some id -> fetch tree id)

durable.OrderedTable.internal.fetch :
  OrderedTable k v -> internal.Id ->{Transaction} internal.Node k
durable.OrderedTable.internal.fetch tree id =
  match Transaction.tryRead.tx (internal.index tree) id with
    None      -> bug "BTree: invalid node id"
    Some node -> node

durable.OrderedTable.internal.findKeySlot :
  OrderedTable k v -> k -> internal.Node k -> Nat
durable.OrderedTable.internal.findKeySlot tree target node =
  use Nat +
  go : Nat -> [k] -> Nat
  go i = cases
    [] -> i
    k +: keys ->
      if gtBy (internal.ordering tree) k target then i else go (i + 1) keys
  go 0 (Node.keys node)

durable.OrderedTable.internal.findLeaf :
  OrderedTable k v -> k ->{Transaction} [internal.Node k]
durable.OrderedTable.internal.findLeaf tree target =
  match Transaction.tryRead.tx (internal.index tree) Root with
    None      -> []
    Some root ->
      use internal Node
      go : Node k -> [Node k] -> [Node k]
      go node ancestors =
        use List +:
        ancestors' = node +: ancestors
        if isLeaf node then ancestors'
        else
          branchToTake = node |> findKeySlot tree target
          node' = node |> childAt tree branchToTake
          go node' ancestors'
      go root []

durable.OrderedTable.internal.index :
  OrderedTable k v -> Table internal.Id (internal.Node k)
durable.OrderedTable.internal.index = cases BTree _ _ _ index _ -> index

durable.OrderedTable.internal.newId : '{Random} internal.Id
durable.OrderedTable.internal.newId = do internal.Id.Id (Random.bytes 256)

durable.OrderedTable.internal.Node.doc : Doc
durable.OrderedTable.internal.Node.doc =
  {{
  {type OrderedTable} is a
  [B+ Tree](https://dl.acm.org/doi/pdf/10.1145/356770.356776): a balanced N-ary
  search tree with keys at the leaves, and leaves joined in a linked list.
  Search, range queries and insertion are standard, whereas for deletion we
  implement a variant of
  [Deletion without rebalance in multiway search trees](https://sidsen.azurewebsites.net/papers/relaxed-b-tree-tods.pdf)
  without periodic rebuilding. However, we also delete a key from the upper
  levels of the tree even though it's not necessary for correctness, to ensure
  we don't retain data that the user has decided to delete. Additionally, we
  keep the mapping from keys to values in a separate {type Table}, so we can
  serve simple reads and modifications of existing values without going through
  the tree at all.

  A B+ Tree has rules and invariants concerning three kinds of nodes:

  * the root
  * internal nodes
  * leaf nodes

  However, representing this distinction with a sum type makes the algorithms
  very convoluted, because several transformations are common to all three
  kinds of nodes. Moreover, the additional type safety of the sum type doesn't
  buy us any significant robustness, because several of the key invariants are
  not easily expressible in types.

  Instead, we represent all three kinds of nodes as:
      @source{type internal.Id}     @source{type internal.Node}

  # Assumptions about the {type OrderedTable}

    * has a `maxFanout` parameter which is `>= 3`.
    * knows how to order keys.
    * can retrieve, modify and delete nodes by their `id`.
    * can retrieve, modify and delete values by their key.

  # Invariants of all nodes

    * `id` is a globally unique identifier for a node.
    * all the ids in `pointers` point to valid nodes.
    * all the keys in `keys` point to valid values.
    * all the keys in `keys` are sorted in ascending order.

  # Invariants of leaf nodes

    * `isLeaf = true`
    * `0 < size keys <= maxFanout`.
    * `pointers` represent the sibling of the leaf:
      * `pointers` can have either 0 or 1 elements.
      * if `pointers` is not empty, `pointers[0]` contains the of the next leaf
        in the tree.

  # Invariants of internal nodes

    * `isLeaf = false`.
    * `pointers` represents the children of the internal node:
      * `0 < size pointers <= maxFanout`.
      * `size pointers == size keys + 1`.
      * Given `i ∈ [0, size keys)`, each key `k = keys[i]` has a pointer at
        `p = pointers[i + 1]`, where `p` is the `id` of a node containing keys
        `>= k`.
      * `pointers[0]` always contains the `id` of a node with keys smaller than
        any key in the current node.

  # Invariants of the root node

    `id` is `Root`. `isLeaf` can be either true or false, in which case the
    root behaves like a leaf node or an internal node respectively.

    If `isLeaf` is true, then the root is the only node in the tree.
  }}

durable.OrderedTable.internal.Node.id : internal.Node k -> internal.Id
durable.OrderedTable.internal.Node.id = cases internal.Node.Node id _ _ _ -> id

durable.OrderedTable.internal.Node.id.modify :
  (internal.Id ->{g} internal.Id) -> internal.Node k ->{g} internal.Node k
durable.OrderedTable.internal.Node.id.modify f = cases
  internal.Node.Node id keys pointers isLeaf ->
    internal.Node.Node (f id) keys pointers isLeaf

durable.OrderedTable.internal.Node.id.set :
  internal.Id -> internal.Node k -> internal.Node k
durable.OrderedTable.internal.Node.id.set id1 = cases
  internal.Node.Node _ keys pointers isLeaf ->
    internal.Node.Node id1 keys pointers isLeaf

durable.OrderedTable.internal.Node.isLeaf : internal.Node k -> Boolean
durable.OrderedTable.internal.Node.isLeaf = cases
  internal.Node.Node _ _ _ isLeaf -> isLeaf

durable.OrderedTable.internal.Node.isLeaf.modify :
  (Boolean ->{g} Boolean) -> internal.Node k ->{g} internal.Node k
durable.OrderedTable.internal.Node.isLeaf.modify f = cases
  internal.Node.Node id keys pointers isLeaf ->
    internal.Node.Node id keys pointers (f isLeaf)

durable.OrderedTable.internal.Node.isLeaf.set :
  Boolean -> internal.Node k -> internal.Node k
durable.OrderedTable.internal.Node.isLeaf.set isLeaf1 = cases
  internal.Node.Node id keys pointers _ ->
    internal.Node.Node id keys pointers isLeaf1

durable.OrderedTable.internal.Node.keys : internal.Node k -> [k]
durable.OrderedTable.internal.Node.keys = cases
  internal.Node.Node _ keys _ _ -> keys

durable.OrderedTable.internal.Node.keys.modify :
  ([k1] ->{g} [k]) -> internal.Node k1 ->{g} internal.Node k
durable.OrderedTable.internal.Node.keys.modify f = cases
  internal.Node.Node id keys pointers isLeaf ->
    internal.Node.Node id (f keys) pointers isLeaf

durable.OrderedTable.internal.Node.keys.set :
  [k] -> internal.Node k1 -> internal.Node k
durable.OrderedTable.internal.Node.keys.set keys1 = cases
  internal.Node.Node id _ pointers isLeaf ->
    internal.Node.Node id keys1 pointers isLeaf

durable.OrderedTable.internal.Node.pointers : internal.Node k -> [internal.Id]
durable.OrderedTable.internal.Node.pointers = cases
  internal.Node.Node _ _ pointers _ -> pointers

durable.OrderedTable.internal.Node.pointers.modify :
  ([internal.Id] ->{g} [internal.Id]) -> internal.Node k ->{g} internal.Node k
durable.OrderedTable.internal.Node.pointers.modify f = cases
  internal.Node.Node id keys pointers isLeaf ->
    internal.Node.Node id keys (f pointers) isLeaf

durable.OrderedTable.internal.Node.pointers.set :
  [internal.Id] -> internal.Node k -> internal.Node k
durable.OrderedTable.internal.Node.pointers.set pointers1 = cases
  internal.Node.Node id keys _ isLeaf ->
    internal.Node.Node id keys pointers1 isLeaf

durable.OrderedTable.internal.ordering : OrderedTable k v -> k -> k -> Ordering
durable.OrderedTable.internal.ordering = cases
  BTree _ ordering _ _ _ -> ordering

durable.OrderedTable.internal.test.bugmin : '{IO, Exception} ()
durable.OrderedTable.internal.test.bugmin =
  do
    use Nat toText
    use Text ++
    shorten : internal.Id -> Text
    shorten = cases
      Root                 -> "root"
      internal.Id.Id bytes -> Bytes.drop 248 bytes |> Bytes.toHex
    fromNode node =
      ( node |> Node.id |> shorten
      , node |> Node.keys
      , node |> pointers |> List.map shorten
      , node |> isLeaf
      )
    debug = true
    trace : Text -> a -> ()
    trace = if debug then Debug.trace else cases _, _ -> ()
    traceTransact : '{g, Transaction} a ->{g} a
    traceTransact t =
      use Map adjust get
      use Table Table
      use data Map
      use data.Map empty
      render storage =
        match Map.toList storage with
          [("index", index), ("root", root), ("values", values)] ->
            ( root
                |> Map.values
                |> List.head
                |> Optional.map (unsafeExtract >> shorten)
            , index |> Map.values |> List.map (unsafeExtract >> fromNode)
            )
          _ -> bug "Malformed storage map"
      go : Map Text (Map Any Any) -> '{g, Transaction} a ->{g} a
      go storage t =
        handle t()
        with cases
          { Transaction.tryRead.tx (Table table) k -> resume } ->
            out =
              storage
                |> get table
                |> Optional.flatMap (get (Any k))
                |> Optional.map unsafeExtract
            go storage do resume out
          { Transaction.write.tx (Table table) k v -> resume } ->
            storage' = storage |> adjust (Map.insert (Any k) (Any v)) table
            trace "written" (table, render storage')
            go storage' resume
          { Transaction.delete.tx (Table table) k -> resume } ->
            storage' = storage |> adjust (data.Map.delete (Any k)) table
            trace "deleted" (table, render storage')
            go storage' resume
          { a } -> a
      go
        (Map.fromList [("root", empty), ("index", empty), ("values", empty)]) t
    seed = base.IO.randomNat()
    printLine ("seed " ++ toText seed)
    splitmix seed do
      traceTransact do
        m = internal.test.testMap 3
        w n =
          OrderedTable.write.tx m n n
          trace (toText n ++ " written") ()
        d n =
          OrderedTable.delete.tx m n
          trace (toText n ++ " deleted") ()
        ()

durable.OrderedTable.internal.test.correctness :
  '{IO, Exception} [(Text, [(Text, Either Failure ())])]
durable.OrderedTable.internal.test.correctness =
  do
    use Nat * / toText
    use OrderedTable database
    use Text ++
    seed = base.IO.randomNat()
    size = 10000
    debug = false
    assertions prefix keys immutableMap tree =
      use Map get
      use Nat +
      use Tuple pair
      use test ensureEqual
      run : '{Transaction, Exception, Batch} a -> a
      run = transact (database tree)
      printLine "Generating expectations for iteration"
      immutableMapIteration = Map.toList immutableMap
      printLine "Generating expectations for reads"
      immutableMapReads = keys |> List.map (k -> immutableMap |> get k)
      printLine "Generating expectations for range queries"
      rangeSpan k = k + 25
      fetch k = get k immutableMap |> Optional.map (pair k)
      immutableMapRanges =
        keys
          |> List.map
            (k -> Nat.rangeClosed k (rangeSpan k) |> List.filterMap fetch)
      printLine "Generating results for iteration"
      treeIteration = do run (tree |> toStream.tx |> toDelayedList)
      iteration = catch do ensureEqual treeIteration() immutableMapIteration
      printLine "Generating results for reads"
      treeReads =
        do run do keys |> List.map (k -> OrderedTable.tryRead.tx tree k)
      reads = catch do ensureEqual treeReads() immutableMapReads
      printLine "Generating results for reads via search"
      treeSearches = do run do keys |> List.map (readViaSearch tree)
      searches = catch do ensureEqual treeSearches() immutableMapReads
      printLine "Generating results for range queries"
      treeRanges =
        do
          run do
            keys
              |> List.map
                (k -> rangeClosed.tx tree k (rangeSpan k) |> Stream.toList)
      ranges = catch do ensureEqual treeRanges() immutableMapRanges
      printLine "Checking for minimum value"
      minValue =
        run do catch do ensureEqual (Map.getMin immutableMap) (min.tx tree)
      printLine "Checking for maximum value"
      maxValue =
        run do catch do ensureEqual (Map.getMax immutableMap) (max.tx tree)
      printLine "Checking for orphan keys in the index"
      orphanKeys =
        go nodeId =
          node = internal.fetch tree nodeId
          guard (Boolean.not (isLeaf node))
          Each.append (do each (Node.keys node)) do each (pointers node) |> go
        run do
          catch do
            Each.run do
              k = go Root
              ensureWith k (readViaSearch tree k |> isSome)
      pair
        prefix
        [ ("iteration", iteration)
        , ("reads", reads)
        , ("searches", searches)
        , ("ranges", ranges)
        , ("min value", minValue)
        , ("max value", maxValue)
        , ("orphan keys", orphanKeys)
        ]
    test maxFanout =
      runner "BTree correctness" seed do
        banner = "Size: " ++ toText size ++ " fanout: " ++ toText maxFanout
        printLine banner
        keys = Nat.range 0 size
        gen = generate keys
        when debug do MapGeneration.log gen
        printLine "Generating immutable Map"
        immutableMap = toImmutableMap gen
        printLine "Generating BTree"
        tree = toBTree maxFanout gen
        results = assertions banner keys immutableMap tree
        _ =
          """
            We generate a lot of deletions to trigger all the conditions
            in the delete algorithm, since they happen rarely
          """
        printLine "Generating immutable Map with majority of deletions"
        let
          (deleted, remaining) =
            immutableMap
              |> Map.toList
              |> shuffle
              |> List.splitAt (data.Map.size immutableMap * 99 / 100)
          immutableMapAfterDeletions = remaining |> Map.fromList
          printLine "Generating tree with majority of deletions"
          transact (database tree) do
            deleted
              |> (foreach.deprecated cases
                (k, _) -> OrderedTable.delete.tx tree k)
          resultsAfterDeletion =
            assertions
              (banner ++ " after deletes") keys immutableMapAfterDeletions tree
          [results, resultsAfterDeletion]
    _ =
      """
        We want small fanouts to trigger plenty of restructuring ops so
        we can test all the corner cases, and we want both even and odd
        fanouts to catch floor and ceiling errors
      """
    [3, 4] |> List.flatMap test

durable.OrderedTable.internal.test.MapGeneration.generate :
  [Nat] ->{Random} '{MapGeneration} ()
durable.OrderedTable.internal.test.MapGeneration.generate keys =
  modifications = do
    use Float <=
    key = each keys
    reps = Random.natIn 0 3
    _ = Each.range 0 reps
    if Random.float() <= 0.7 then
      v = Random.bytes 9
      do MapGeneration.insert key v
    else do MapGeneration.delete key
  schedule = modifications |> Each.toList |> shuffle
  do schedule |> foreach.deprecated force

durable.OrderedTable.internal.test.MapGeneration.log :
  '{MapGeneration} a ->{IO, Exception} ()
durable.OrderedTable.internal.test.MapGeneration.log p =
  use Nat toText
  use Text ++
  use durable.OrderedTable.internal.test.MapGeneration log
  handle p()
  with cases
    { MapGeneration.insert k v -> resume } ->
      printLine ("inserting " ++ toText k ++ " " ++ toDebugText v)
      log resume
    { MapGeneration.delete k -> resume }   ->
      printLine ("deleting " ++ toText k)
      log resume
    { a }                                  -> ()

durable.OrderedTable.internal.test.MapGeneration.toBTree :
  Nat
  -> '{MapGeneration} a
  ->{Exception, Storage, Random} OrderedTable Nat Bytes
durable.OrderedTable.internal.test.MapGeneration.toBTree maxFanout p =
  use Nat <
  if maxFanout < 3 then bug "maxFanout should be at least 3"
  else
    m = internal.test.testMap maxFanout
    go p =
      handle p()
      with cases
        { MapGeneration.insert k v -> resume } ->
          OrderedTable.write.tx m k v
          go resume
        { MapGeneration.delete k -> resume }   ->
          OrderedTable.delete.tx m k
          go resume
        { a }                                  -> m
    transact (OrderedTable.database m) do go p

durable.OrderedTable.internal.test.MapGeneration.toImmutableMap :
  '{MapGeneration} a -> data.Map Nat Bytes
durable.OrderedTable.internal.test.MapGeneration.toImmutableMap p =
  use data Map
  go : Map Nat Bytes -> '{MapGeneration} a -> Map Nat Bytes
  go state p =
    handle p()
    with cases
      { MapGeneration.insert k v -> resume } ->
        go (state |> Map.insert k v) resume
      { MapGeneration.delete k -> resume } ->
        go (state |> data.Map.delete k) resume
      { a } -> state
  go data.Map.empty p

durable.OrderedTable.internal.test.perfmin : '{IO, Exception} ()
durable.OrderedTable.internal.test.perfmin =
  do
    size = 10000
    maxFanout = 100
    seed = base.IO.randomNat()
    runner "SortedMap performance" seed do
      tree = internal.test.testMap maxFanout
      run : '{Transaction, Exception, Random} a -> (a, Sample)
      run t = transact (OrderedTable.database tree) do Sample.sample t
      writesToNewKeys =
        Nat.range 0 size
          |> shuffle
          |> List.map
               (key -> (run do OrderedTable.write.tx tree key 42) |> at2)
      writesToNewKeys |> percentiles |> render "writes" |> printLine

durable.OrderedTable.internal.test.performance : '{IO, Exception} ()
durable.OrderedTable.internal.test.performance =
  do
    use List map
    use Nat + range toText
    use OrderedTable write.tx
    use Text ++
    size = 10000
    maxFanout = 100
    seed = base.IO.randomNat()
    runner "BTree performance" seed do
      printLine ("Size: " ++ toText size ++ " fanout: " ++ toText maxFanout)
      tree = internal.test.testMap maxFanout
      run : '{Transaction, Exception, Random} a -> (a, Sample)
      run t = transact (OrderedTable.database tree) do Sample.sample t
      printLine "Generating results for writes to new keys"
      writesToNewKeys =
        range 0 size
          |> map (key -> (run do write.tx tree key 42) |> at2)
          |> percentiles
      printLine "Generating results for reads via search"
      readsViaSearch =
        range 0 size
          |> map (key -> (run do readViaSearch tree key) |> at2)
          |> percentiles
      printLine "Generating results for direct reads"
      directReads =
        range 0 size
          |> map (key -> (run do OrderedTable.tryRead.tx tree key) |> at2)
          |> percentiles
      printLine "Generating results for writes to existing keys"
      writesToExistingKeys =
        range 0 size
          |> map (key -> (run do write.tx tree key 43) |> at2)
          |> percentiles
      printLine "Generating results for deletes"
      deletes =
        range 0 size
          |> List.filter Nat.isEven
          |> map (key -> (run do OrderedTable.delete.tx tree key) |> at2)
          |> percentiles
      rangeKeysOnly n =
        range 0 size
          |> map
               (i ->
                 (run do rangeClosed.keys.tx tree i (i + n) |> Stream.toList)
                   |> at2)
          |> percentiles
      printLine "Generating results for range queries, small batch"
      rangeSmall = rangeKeysOnly 10
      printLine "Generating results for range queries, medium batch"
      rangeMedium = rangeKeysOnly 50
      printLine "Generating results for range queries, large batch"
      rangeLarge = rangeKeysOnly 200
      output =
        Text.join
          "\n\n"
          [ render "writes to new keys" writesToNewKeys
          , render "reads via search" readsViaSearch
          , render "direct reads" directReads
          , render "writes to existing keys" writesToExistingKeys
          , render "deletes" deletes
          , render "range, keys only, batch of 10" rangeSmall
          , render "range, keys only, batch of 50" rangeMedium
          , render "range, keys only, batch of 200" rangeLarge
          ]
      printLine output

durable.OrderedTable.internal.test.prefixQueries : '{IO, Exception} ()
durable.OrderedTable.internal.test.prefixQueries =
  do
    runner "prefix query" 0 do
      use OrderedTable write.tx
      use test ensureEqual
      db = Database (Database.Id.Id "db") "db"
      tree : OrderedTable (Nat, Text) Text
      tree = OrderedTable.named db "tree" Universal.ordering
      transact db do
        write.tx tree (1, "e") "hello"
        write.tx tree (2, "a") "darkness"
        write.tx tree (3, "c") "my"
        write.tx tree (2, "d") "old"
        write.tx tree (4, "b") "friend"
        query hi lo =
          rangeClosed.prefix.tx tree prefixOrdering hi lo |> Stream.toList
        ensureEqual
          (query 2 4)
          [ ((2, "a"), "darkness")
          , ((2, "d"), "old")
          , ((3, "c"), "my")
          , ((4, "b"), "friend")
          ]
        ensureEqual (query 2 2) [((2, "a"), "darkness"), ((2, "d"), "old")]

durable.OrderedTable.internal.test.readViaSearch :
  OrderedTable k v -> k ->{Transaction} Optional v
durable.OrderedTable.internal.test.readViaSearch tree k =
  use Optional flatMap
  note =
    """
     Retrieval via tree search is used in the slow path of several
     operations, this helper facilitates testing it.
    """
  findLeaf tree k
    |> List.head
    |> flatMap
         (leaf -> List.find (eqBy (internal.ordering tree) k) (Node.keys leaf))
    |> flatMap (OrderedTable.tryRead.tx tree)

durable.OrderedTable.internal.test.runner :
  Text -> Nat -> '{g, Storage, Random} a ->{g, IO, Exception} a
durable.OrderedTable.internal.test.runner name seed test =
  use Text ++
  printLine (name ++ " test running with seed: " ++ Nat.toText seed)
  splitmix seed do Storage.run.local test

durable.OrderedTable.internal.test.Sample.deletes : Sample -> Nat
durable.OrderedTable.internal.test.Sample.deletes = cases
  Sample _ _ _ deletes _ -> deletes

durable.OrderedTable.internal.test.Sample.deletes.modify :
  (Nat ->{g} Nat) -> Sample ->{g} Sample
durable.OrderedTable.internal.test.Sample.deletes.modify f = cases
  Sample storageReads reads writes deletes keys ->
    Sample storageReads reads writes (f deletes) keys

durable.OrderedTable.internal.test.Sample.deletes.set : Nat -> Sample -> Sample
durable.OrderedTable.internal.test.Sample.deletes.set deletes1 = cases
  Sample storageReads reads writes _ keys ->
    Sample storageReads reads writes deletes1 keys

durable.OrderedTable.internal.test.Sample.empty : Sample
durable.OrderedTable.internal.test.Sample.empty = Sample 0 0 0 0 0

durable.OrderedTable.internal.test.Sample.keys : Sample -> Nat
durable.OrderedTable.internal.test.Sample.keys = cases
  Sample _ _ _ _ keys -> keys

durable.OrderedTable.internal.test.Sample.keys.modify :
  (Nat ->{g} Nat) -> Sample ->{g} Sample
durable.OrderedTable.internal.test.Sample.keys.modify f = cases
  Sample storageReads reads writes deletes keys ->
    Sample storageReads reads writes deletes (f keys)

durable.OrderedTable.internal.test.Sample.keys.set : Nat -> Sample -> Sample
durable.OrderedTable.internal.test.Sample.keys.set keys1 = cases
  Sample storageReads reads writes deletes _ ->
    Sample storageReads reads writes deletes keys1

durable.OrderedTable.internal.test.Sample.Percentiles.empty : Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.empty =
  use Sample empty
  Percentiles 0 empty empty empty empty empty

durable.OrderedTable.internal.test.Sample.Percentiles.max :
  Percentiles -> Sample
durable.OrderedTable.internal.test.Sample.Percentiles.max = cases
  Percentiles _ _ _ _ _ max -> max

durable.OrderedTable.internal.test.Sample.Percentiles.max.modify :
  (Sample ->{g} Sample) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.max.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles sampleSize min p50 p95 p99 (f max)

durable.OrderedTable.internal.test.Sample.Percentiles.max.set :
  Sample -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.max.set max1 = cases
  Percentiles sampleSize min p50 p95 p99 _ ->
    Percentiles sampleSize min p50 p95 p99 max1

durable.OrderedTable.internal.test.Sample.Percentiles.min :
  Percentiles -> Sample
durable.OrderedTable.internal.test.Sample.Percentiles.min = cases
  Percentiles _ min _ _ _ _ -> min

durable.OrderedTable.internal.test.Sample.Percentiles.min.modify :
  (Sample ->{g} Sample) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.min.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles sampleSize (f min) p50 p95 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.min.set :
  Sample -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.min.set min1 = cases
  Percentiles sampleSize _ p50 p95 p99 max ->
    Percentiles sampleSize min1 p50 p95 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.p50 :
  Percentiles -> Sample
durable.OrderedTable.internal.test.Sample.Percentiles.p50 = cases
  Percentiles _ _ p50 _ _ _ -> p50

durable.OrderedTable.internal.test.Sample.Percentiles.p50.modify :
  (Sample ->{g} Sample) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p50.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles sampleSize min (f p50) p95 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.p50.set :
  Sample -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p50.set p501 = cases
  Percentiles sampleSize min _ p95 p99 max ->
    Percentiles sampleSize min p501 p95 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.p95 :
  Percentiles -> Sample
durable.OrderedTable.internal.test.Sample.Percentiles.p95 = cases
  Percentiles _ _ _ p95 _ _ -> p95

durable.OrderedTable.internal.test.Sample.Percentiles.p95.modify :
  (Sample ->{g} Sample) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p95.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles sampleSize min p50 (f p95) p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.p95.set :
  Sample -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p95.set p951 = cases
  Percentiles sampleSize min p50 _ p99 max ->
    Percentiles sampleSize min p50 p951 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.p99 :
  Percentiles -> Sample
durable.OrderedTable.internal.test.Sample.Percentiles.p99 = cases
  Percentiles _ _ _ _ p99 _ -> p99

durable.OrderedTable.internal.test.Sample.Percentiles.p99.modify :
  (Sample ->{g} Sample) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p99.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles sampleSize min p50 p95 (f p99) max

durable.OrderedTable.internal.test.Sample.Percentiles.p99.set :
  Sample -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.p99.set p991 = cases
  Percentiles sampleSize min p50 p95 _ max ->
    Percentiles sampleSize min p50 p95 p991 max

durable.OrderedTable.internal.test.Sample.Percentiles.percentiles :
  [Sample] ->{Exception} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.percentiles samples =
  use Float * / fromNat
  use List map size sort
  use Nat -
  error = "percentiles expects a non empty list of samples"
  allStorageReads = samples |> map storageReads |> sort
  allReads = samples |> map reads |> sort
  allWrites = samples |> map writes |> sort
  allDeletes = samples |> map Sample.deletes |> sort
  allKeys = samples |> map Sample.keys |> sort
  buildSample f =
    f' x = f x |> Optional.toException error (typeLink Generic)
    Sample.empty
      |> storageReads.set (allStorageReads |> f')
      |> reads.set (allReads |> f')
      |> writes.set (allWrites |> f')
      |> deletes.set (allDeletes |> f')
      |> Sample.keys.set (allKeys |> f')
  percentile kth ns =
    (kth |> fromNat) / 100.0 * (ns |> size |> fromNat)
      |> Float.ceiling
      |> Float.toNat
      |> Optional.flatMap (i -> List.at (i - 1) ns)
  Percentiles.empty
    |> sampleSize.set (size samples)
    |> min.set (buildSample List.head)
    |> p50.set (buildSample (percentile 50))
    |> p95.set (buildSample (percentile 95))
    |> p99.set (buildSample (percentile 99))
    |> max.set (buildSample List.last)

durable.OrderedTable.internal.test.Sample.Percentiles.render :
  Text -> Percentiles -> Text
durable.OrderedTable.internal.test.Sample.Percentiles.render title percentiles =
  use Nat toText
  use Text ++ join
  indent n str = Text.repeat n " " ++ str
  lines = join "\n"
  line = join " "
  renderSample sample =
    line
      [ "storage reads: "
      , sample |> storageReads |> toText
      , "reads:"
      , sample |> reads |> toText
      , "writes:"
      , sample |> writes |> toText
      , "deletes:"
      , sample |> Sample.deletes |> toText
      , "keys:"
      , sample |> Sample.keys |> toText
      ]
      |> indent 2
  output =
    [ "samples:"
    , percentiles |> sampleSize |> toText |> indent 2
    , "min:"
    , percentiles |> Percentiles.min |> renderSample
    , "p50:"
    , percentiles |> p50 |> renderSample
    , "p95:"
    , percentiles |> p95 |> renderSample
    , "p99:"
    , percentiles |> p99 |> renderSample
    , "max:"
    , percentiles |> Percentiles.max |> renderSample
    ]
      |> List.map (indent 2)
      |> lines
  lines [Text.toUppercase title ++ ":", output]

durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize :
  Percentiles -> Nat
durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize = cases
  Percentiles sampleSize _ _ _ _ _ -> sampleSize

durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize.modify :
  (Nat ->{g} Nat) -> Percentiles ->{g} Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize.modify f = cases
  Percentiles sampleSize min p50 p95 p99 max ->
    Percentiles (f sampleSize) min p50 p95 p99 max

durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize.set :
  Nat -> Percentiles -> Percentiles
durable.OrderedTable.internal.test.Sample.Percentiles.sampleSize.set
  sampleSize1 = cases
  Percentiles _ min p50 p95 p99 max ->
    Percentiles sampleSize1 min p50 p95 p99 max

durable.OrderedTable.internal.test.Sample.reads : Sample -> Nat
durable.OrderedTable.internal.test.Sample.reads = cases
  Sample _ reads _ _ _ -> reads

durable.OrderedTable.internal.test.Sample.reads.modify :
  (Nat ->{g} Nat) -> Sample ->{g} Sample
durable.OrderedTable.internal.test.Sample.reads.modify f = cases
  Sample storageReads reads writes deletes keys ->
    Sample storageReads (f reads) writes deletes keys

durable.OrderedTable.internal.test.Sample.reads.set : Nat -> Sample -> Sample
durable.OrderedTable.internal.test.Sample.reads.set reads1 = cases
  Sample storageReads _ writes deletes keys ->
    Sample storageReads reads1 writes deletes keys

durable.OrderedTable.internal.test.Sample.sample :
  '{g, Transaction} a ->{g, Transaction} (a, Sample)
durable.OrderedTable.internal.test.Sample.sample t =
  use Nat increment
  use Set insert
  use Transaction delete.tx tryRead.tx write.tx
  go : Set Any -> Sample -> '{g, Transaction} a ->{g, Transaction} (a, Sample)
  go seen sample t =
    handle t()
    with cases
      { tryRead.tx table k -> resume } ->
        seen' = seen |> insert (Any k)
        sample' =
          sample
            |> reads.modify increment
            |> storageReads.modify
                 (if seen |> Set.contains (Any k) then Function.id
                 else increment)
        out = tryRead.tx table k
        go seen' sample' do resume out
      { write.tx table k v -> resume } ->
        seen' = seen |> insert (Any k)
        sample' = sample |> writes.modify increment
        write.tx table k v
        go seen' sample' resume
      { delete.tx table k -> resume } ->
        seen' = seen |> insert (Any k)
        sample' = sample |> deletes.modify increment
        delete.tx table k
        go seen' sample' resume
      { a } ->
        sample' = sample |> Sample.keys.set (Set.size seen)
        (a, sample')
  go Set.empty Sample.empty t

durable.OrderedTable.internal.test.Sample.storageReads : Sample -> Nat
durable.OrderedTable.internal.test.Sample.storageReads = cases
  Sample storageReads _ _ _ _ -> storageReads

durable.OrderedTable.internal.test.Sample.storageReads.modify :
  (Nat ->{g} Nat) -> Sample ->{g} Sample
durable.OrderedTable.internal.test.Sample.storageReads.modify f = cases
  Sample storageReads reads writes deletes keys ->
    Sample (f storageReads) reads writes deletes keys

durable.OrderedTable.internal.test.Sample.storageReads.set :
  Nat -> Sample -> Sample
durable.OrderedTable.internal.test.Sample.storageReads.set storageReads1 = cases
  Sample _ reads writes deletes keys ->
    Sample storageReads1 reads writes deletes keys

durable.OrderedTable.internal.test.Sample.writes : Sample -> Nat
durable.OrderedTable.internal.test.Sample.writes = cases
  Sample _ _ writes _ _ -> writes

durable.OrderedTable.internal.test.Sample.writes.modify :
  (Nat ->{g} Nat) -> Sample ->{g} Sample
durable.OrderedTable.internal.test.Sample.writes.modify f = cases
  Sample storageReads reads writes deletes keys ->
    Sample storageReads reads (f writes) deletes keys

durable.OrderedTable.internal.test.Sample.writes.set : Nat -> Sample -> Sample
durable.OrderedTable.internal.test.Sample.writes.set writes1 = cases
  Sample storageReads reads _ deletes keys ->
    Sample storageReads reads writes1 deletes keys

durable.OrderedTable.internal.test.testMap : Nat -> OrderedTable k v
durable.OrderedTable.internal.test.testMap maxFanout =
  use Table Table
  BTree
    (Database (Database.Id.Id "testDb") "testDb")
    Universal.ordering
    maxFanout
    (Table "index")
    (Table "values")

durable.OrderedTable.internal.values : OrderedTable k v -> Table k v
durable.OrderedTable.internal.values = cases BTree _ _ _ _ values -> values

durable.OrderedTable.max : OrderedTable k v ->{Remote} Optional (k, v)
durable.OrderedTable.max tree =
  toRemote do transact (OrderedTable.database tree) do max.tx tree

durable.OrderedTable.max.doc : Doc
durable.OrderedTable.max.doc =
  {{
  Returns the entry with the maximum key in a {type OrderedTable}, if it
  exists. If the {type OrderedTable} is empty, this operation returns {None}.
  }}

durable.OrderedTable.max.tx :
  OrderedTable k v ->{Transaction, Exception} Optional (k, v)
durable.OrderedTable.max.tx tree =
  maxKey.tx tree |> Optional.map (k -> (k, OrderedTable.read.tx tree k))

durable.OrderedTable.max.tx.doc : Doc
durable.OrderedTable.max.tx.doc =
  {{
  Returns the entry with the maximum key in a {type OrderedTable}, if it
  exists. If the {type OrderedTable} is empty, this operation returns {None}.
  This operation is performed within a {type Transaction}, ensuring that the
  operation is atomic.
  }}

durable.OrderedTable.maxFanout : OrderedTable k v -> Nat
durable.OrderedTable.maxFanout = cases BTree _ _ maxFanout _ _ -> maxFanout

durable.OrderedTable.maxFanout.doc : Doc
durable.OrderedTable.maxFanout.doc =
  {{
  Gets the maximum fanout of a {type OrderedTable}. The fanout is the maximum
  number of children that a node in the {type OrderedTable} can have.
  }}

durable.OrderedTable.maxFanout.modify :
  (Nat ->{g} Nat) -> OrderedTable k v ->{g} OrderedTable k v
durable.OrderedTable.maxFanout.modify f = cases
  BTree database ordering maxFanout index values ->
    BTree database ordering (f maxFanout) index values

durable.OrderedTable.maxFanout.set :
  Nat -> OrderedTable k v -> OrderedTable k v
durable.OrderedTable.maxFanout.set maxFanout1 = cases
  BTree database ordering _ index values ->
    BTree database ordering maxFanout1 index values

durable.OrderedTable.maxKey : OrderedTable k v ->{Remote} Optional k
durable.OrderedTable.maxKey tree =
  toRemote do transact (OrderedTable.database tree) do maxKey.tx tree

durable.OrderedTable.maxKey.doc : Doc
durable.OrderedTable.maxKey.doc =
  {{
  Returns the maximum key in a {type OrderedTable}, if it exists. If the
  {type OrderedTable} is empty, this operation returns {None}.
  }}

durable.OrderedTable.maxKey.tx : OrderedTable k v ->{Transaction} Optional k
durable.OrderedTable.maxKey.tx tree =
  match Transaction.tryRead.tx (internal.index tree) Root with
    None -> None
    Some root ->
      use List last
      goToLeaf node =
        if isLeaf node then node
        else
          node
            |> pointers
            |> last
            |> getOrBug "OrderedMap: invalid node"
            |> fetch tree
            |> goToLeaf
      leaf = goToLeaf root
      leaf |> Node.keys |> last

durable.OrderedTable.maxKey.tx.doc : Doc
durable.OrderedTable.maxKey.tx.doc =
  {{
  Returns the maximum key in a {type OrderedTable}, if it exists. If the
  {type OrderedTable} is empty, this operation returns {None}. This operation
  is performed within a {type Transaction}, ensuring that the operation is
  atomic.
  }}

durable.OrderedTable.min : OrderedTable k v ->{Remote} Optional (k, v)
durable.OrderedTable.min tree =
  toRemote do transact (OrderedTable.database tree) do min.tx tree

durable.OrderedTable.min.doc : Doc
durable.OrderedTable.min.doc =
  {{
  Returns the entry with the minimum key in a {type OrderedTable}, if it
  exists. If the {type OrderedTable} is empty, this operation returns {None}.
  }}

durable.OrderedTable.min.tx :
  OrderedTable k v ->{Transaction, Exception} Optional (k, v)
durable.OrderedTable.min.tx tree =
  minKey.tx tree |> Optional.map (k -> (k, OrderedTable.read.tx tree k))

durable.OrderedTable.min.tx.doc : Doc
durable.OrderedTable.min.tx.doc =
  {{
  Returns the entry with the minimum key in a {type OrderedTable}, if it
  exists. If the {type OrderedTable} is empty, this operation returns {None}.
  This operation is performed within a {type Transaction}, ensuring that the
  operation is atomic.
  }}

durable.OrderedTable.minKey : OrderedTable k v ->{Remote} Optional k
durable.OrderedTable.minKey tree =
  toRemote do transact (OrderedTable.database tree) do minKey.tx tree

durable.OrderedTable.minKey.doc : Doc
durable.OrderedTable.minKey.doc =
  {{
  Returns the minimum key in a {type OrderedTable}, if it exists. If the
  {type OrderedTable} is empty, this operation returns {None}.
  }}

durable.OrderedTable.minKey.tx : OrderedTable k v ->{Transaction} Optional k
durable.OrderedTable.minKey.tx tree =
  match Stream.uncons (toStream.keys.tx tree) with
    Left _       -> None
    Right (k, _) -> Some k

durable.OrderedTable.minKey.tx.doc : Doc
durable.OrderedTable.minKey.tx.doc =
  {{
  Returns the minimum key in a {type OrderedTable}, if it exists. If the
  {type OrderedTable} is empty, this operation returns {None}. This operation
  is performed within a {type Transaction}, ensuring that the operation is
  atomic.
  }}

durable.OrderedTable.named :
  Database -> Text -> (k -> k -> Ordering) -> OrderedTable k v
durable.OrderedTable.named db name ordering =
  use Text ++
  newTable : ∀ k v. Text -> Table k v
  newTable suffix = nestedTable durable.OrderedTable.named (name ++ suffix)
  BTree db ordering 100 (newTable "index") (newTable "values")

durable.OrderedTable.named.doc : Doc
durable.OrderedTable.named.doc =
  {{
  Gets a named {type OrderedTable} from a {type Database}. The
  {type OrderedTable} is identified by the given name and uses the given
  ordering function to order the keys. The {type OrderedTable} is created if it
  does not already exist.

  # Example

    This gets a named {type OrderedTable} named "mytable" from a database named
    "my-db":

    @typecheck ```
    db = Database.named "my-db"
    OrderedTable.named db "mytable" Universal.ordering
    ```
  }}

durable.OrderedTable.nested :
  Database -> parent -> (k -> k -> Ordering) -> OrderedTable k v
durable.OrderedTable.nested db parent ord =
  name =
    blake2b_256 (durable.OrderedTable.nested, parent)
      |> up.Bytes.toBase32Hex.text
  OrderedTable.named db name ord

durable.OrderedTable.nested.doc : Doc
durable.OrderedTable.nested.doc =
  {{
  Gets or creates a nested {type OrderedTable} given a {type Database} and a
  parent name. The nested {type OrderedTable} is identified by the given name
  and uses the given ordering function to order the keys. The
  {type OrderedTable} is created if it does not already exist.
  }}

durable.OrderedTable.prefixOrdering : k1 -> (k1, k2) -> Ordering
durable.OrderedTable.prefixOrdering prefix = cases
  (target, _) -> Universal.ordering prefix target

durable.OrderedTable.prefixOrdering.doc : Doc
durable.OrderedTable.prefixOrdering.doc =
  {{
  An ordering function that takes a prefix and a pair of keys and returns the
  ordering of the prefix relative to the first key in the pair. This ordering
  function is used to order keys in a {type OrderedTable} relative to a given
  prefix.
  }}

durable.OrderedTable.rangeClosed :
  OrderedTable k v -> k -> k -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.rangeClosed tree lo hi =
  rangeClosed.prefix tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.doc : Doc
durable.OrderedTable.rangeClosed.doc =
  {{
  Returns a {type Stream} of key-value pairs from a {type OrderedTable} where
  the key is greater than or equal to the given start key and less than or
  equal to the given end key, according to the ordering of the
  {type OrderedTable}.

  # Example

    This gets all key-value pairs from a {type OrderedTable} named
    "my-ordered-map" that are between the keys 10 and 20:

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" Universal.ordering
    OrderedTable.rangeClosed orderedMap 10 20
      |> (Stream.foreach cases
        (k, v) -> info "Key-value pair" [(Nat.toText k, v)])
    ```
  }}

durable.OrderedTable.rangeClosed.keys :
  OrderedTable k v -> k -> k -> '{Remote, Stream k} ()
durable.OrderedTable.rangeClosed.keys tree lo hi =
  rangeClosed.prefix.keys tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.keys.chunked :
  OrderedTable k v -> k -> k -> '{Remote, Stream [k]} ()
durable.OrderedTable.rangeClosed.keys.chunked tree lo hi =
  rangeClosed.prefix.keys.chunked tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.keys.chunked.doc : Doc
durable.OrderedTable.rangeClosed.keys.chunked.doc =
  {{ A version of {rangeClosed.keys} that returns the keys in chunks. }}

durable.OrderedTable.rangeClosed.keys.chunked.tx :
  OrderedTable k v -> k -> k -> '{Transaction, Stream [k]} ()
durable.OrderedTable.rangeClosed.keys.chunked.tx tree lo hi =
  rangeClosed.prefix.keys.chunked.tx tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.keys.chunked.tx.doc : Doc
durable.OrderedTable.rangeClosed.keys.chunked.tx.doc =
  {{
  A version of {rangeClosed.keys.chunked} that operates within a
  {type Transaction}, ensuring that the operation is atomic.
  }}

durable.OrderedTable.rangeClosed.keys.doc : Doc
durable.OrderedTable.rangeClosed.keys.doc =
  {{
  Returns a {type Stream} of keys from a {type OrderedTable} that are greater
  than or equal to the given start key and less than or equal to the given end
  key, according to the ordering of the {type OrderedTable}.

  # Example

    This gets all keys from a {type OrderedTable} named "my-ordered-map" that
    are between the keys 10 and 20:

    @typecheck ```
    db = Database.named "my-db"
    orderedMap = OrderedTable.named db "my-ordered-map" Universal.ordering
    rangeClosed.keys orderedMap 10 20
      |> Stream.foreach (k -> info "Got a key" [("key", Nat.toText k)])
    ```
  }}

durable.OrderedTable.rangeClosed.keys.tx :
  OrderedTable k v -> k -> k -> '{Transaction, Stream k} ()
durable.OrderedTable.rangeClosed.keys.tx tree lo hi =
  rangeClosed.prefix.keys.tx tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.keys.tx.doc : Doc
durable.OrderedTable.rangeClosed.keys.tx.doc =
  {{
  A version of {rangeClosed.keys} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.rangeClosed.prefix :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.rangeClosed.prefix tree ordering lo hi =
  rangeClosed.prefix.keys.chunked tree ordering lo hi |> bulkFetchValues tree

durable.OrderedTable.rangeClosed.prefix.doc : Doc
durable.OrderedTable.rangeClosed.prefix.doc =
  {{
  `` rangeClosed.prefix ord lo hi `` returns a {type Stream} of key-value pairs
  from a {type OrderedTable} where the first key has the prefix `lo` and the
  last key has the prefix `hi`, as ordered by the given ordering function
  `ord`.
  }}

durable.OrderedTable.rangeClosed.prefix.keys :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Remote, Stream k} ()
durable.OrderedTable.rangeClosed.prefix.keys tree ordering lo hi =
  rangeClosed.prefix.keys.tx tree ordering lo hi
    |> transact.stream (OrderedTable.database tree)

durable.OrderedTable.rangeClosed.prefix.keys.chunked :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Remote, Stream [k]} ()
durable.OrderedTable.rangeClosed.prefix.keys.chunked tree ordering lo hi =
  rangeClosed.prefix.keys.chunked.tx tree ordering lo hi
    |> transact.stream (OrderedTable.database tree)

durable.OrderedTable.rangeClosed.prefix.keys.chunked.doc : Doc
durable.OrderedTable.rangeClosed.prefix.keys.chunked.doc =
  {{ A version of {rangeClosed.prefix.keys} that returns the keys in chunks. }}

durable.OrderedTable.rangeClosed.prefix.keys.chunked.tx :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Transaction, Stream [k]} ()
durable.OrderedTable.rangeClosed.prefix.keys.chunked.tx tree ordering lo hi =
  use List isEmpty
  closeRange : '{g, Stream [k]} () -> '{g, Stream [k]} ()
  closeRange in = do match Stream.uncons in with
    Left _             -> ()
    Right (keys, rest) ->
      (inRange, outRange) =
        keys |> List.span
          (k -> let
            ord = ordering hi k
            ord === Equal || ord === Greater)
      when (Boolean.not (isEmpty inRange)) do emit inRange
      when (isEmpty outRange) do closeRange rest ()
  from.prefix.keys.chunked.tx tree ordering lo |> closeRange

durable.OrderedTable.rangeClosed.prefix.keys.chunked.tx.doc : Doc
durable.OrderedTable.rangeClosed.prefix.keys.chunked.tx.doc =
  {{
  A version of {rangeClosed.prefix.keys.chunked} that operates within a
  {type Transaction}, ensuring that the operation is atomic.
  }}

durable.OrderedTable.rangeClosed.prefix.keys.doc : Doc
durable.OrderedTable.rangeClosed.prefix.keys.doc =
  {{
  `` rangeClosed.prefix.keys ord lo hi `` returns a {type Stream} of keys from
  a {type OrderedTable} where the first key has the prefix `lo` and the last
  key has the prefix `hi`, as ordered by the given ordering function `ord`.
  }}

durable.OrderedTable.rangeClosed.prefix.keys.tx :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Transaction, Stream k} ()
durable.OrderedTable.rangeClosed.prefix.keys.tx tree ordering lo hi =
  rangeClosed.prefix.keys.chunked.tx tree ordering lo hi
    |> concatMap Function.id

durable.OrderedTable.rangeClosed.prefix.keys.tx.doc : Doc
durable.OrderedTable.rangeClosed.prefix.keys.tx.doc =
  {{
  A version of {rangeClosed.prefix.keys} that operates within a
  {type Transaction}, ensuring that the operation is atomic.
  }}

durable.OrderedTable.rangeClosed.prefix.tx :
  OrderedTable k v
  -> (prefix -> k -> Ordering)
  -> prefix
  -> prefix
  -> '{Transaction, Batch, Stream (k, v)} ()
durable.OrderedTable.rangeClosed.prefix.tx tree ordering lo hi =
  rangeClosed.prefix.keys.chunked.tx tree ordering lo hi
    |> bulkFetchValues.tx tree

durable.OrderedTable.rangeClosed.prefix.tx.doc : Doc
durable.OrderedTable.rangeClosed.prefix.tx.doc =
  {{
  A version of {rangeClosed.prefix} that operates within a {type Transaction},
  ensuring that the operation is atomic.
  }}

durable.OrderedTable.rangeClosed.tx :
  OrderedTable k v -> k -> k -> '{Transaction, Batch, Stream (k, v)} ()
durable.OrderedTable.rangeClosed.tx tree lo hi =
  rangeClosed.prefix.tx tree (internal.ordering tree) lo hi

durable.OrderedTable.rangeClosed.tx.doc : Doc
durable.OrderedTable.rangeClosed.tx.doc =
  {{
  A version of {OrderedTable.rangeClosed} that operates within a
  {type Transaction}, ensuring that the operation is atomic.
  }}

durable.OrderedTable.read : OrderedTable k v -> k ->{Remote} v
durable.OrderedTable.read tree k =
  toRemote do
    transact (OrderedTable.database tree) do OrderedTable.read.tx tree k

durable.OrderedTable.read.doc : Doc
durable.OrderedTable.read.doc =
  {{
  Reads the value associated with a key in a {type OrderedTable}. Takes a
  {type OrderedTable} and a key, and returns the value associated with the key
  in the {type OrderedTable}, if it exists. If the key does not exist, this
  operation throws an error in {type Remote}.
  }}

durable.OrderedTable.read.tx :
  OrderedTable k v -> k ->{Transaction, Exception} v
durable.OrderedTable.read.tx tree k =
  OrderedTable.tryRead.tx tree k
    |> (getOrElse' do abortWith "key not found in sorted map" ("key", k))

durable.OrderedTable.read.tx.doc : Doc
durable.OrderedTable.read.tx.doc =
  {{
  Reads the value associated with a key in a {type OrderedTable} within a
  {type Transaction}. Takes a {type OrderedTable} and a key, and returns the
  value associated with the key in the {type OrderedTable} within the current
  transaction. If the key does not exist, this operation throws an error in
  {type Remote}.
  }}

durable.OrderedTable.search :
  OrderedTable k v -> (k -> Ordering) -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.search tree ordering =
  search.keys.chunked tree ordering |> bulkFetchValues tree

durable.OrderedTable.search.keys :
  OrderedTable k v -> (k -> Ordering) -> '{Remote, Stream k} ()
durable.OrderedTable.search.keys tree ordering =
  search.keys.tx tree ordering |> transact.stream (OrderedTable.database tree)

durable.OrderedTable.search.keys.chunked :
  OrderedTable k v -> (k -> Ordering) -> '{Remote, Stream [k]} ()
durable.OrderedTable.search.keys.chunked tree ordering =
  search.keys.chunked.tx tree ordering
    |> transact.stream (OrderedTable.database tree)

durable.OrderedTable.search.keys.chunked.tx :
  OrderedTable k v -> (k -> Ordering) -> '{Transaction, Stream [k]} ()
durable.OrderedTable.search.keys.chunked.tx tree ordering =
  do
    match Transaction.tryRead.tx (internal.index tree) Root with
      None -> ()
      Some root ->
        use Nat +
        findSlot i = cases
          []        -> i
          k +: keys -> if ordering k === Less then i else findSlot (i + 1) keys
        goToLeaf node =
          if isLeaf node then node
          else node |> childAt tree (findSlot 0 (Node.keys node)) |> goToLeaf
        iterateFrom node =
          List.at 0 (pointers node)
            |> Optional.flatMap
                 (id -> Transaction.tryRead.tx (internal.index tree) id)
            |> (cases
                 None      -> ()
                 Some node ->
                   emit (Node.keys node)
                   iterateFrom node)
        emitFirstPage node =
          keys =
            node |> Node.keys |> List.dropWhile (k -> ordering k === Greater)
          when (Boolean.not (List.isEmpty keys)) do emit keys
        leaf = goToLeaf root
        emitFirstPage leaf
        iterateFrom leaf

durable.OrderedTable.search.keys.tx :
  OrderedTable k v -> (k -> Ordering) -> '{Transaction, Stream k} ()
durable.OrderedTable.search.keys.tx tree ordering =
  search.keys.chunked.tx tree ordering |> concatMap Function.id

durable.OrderedTable.search.tx :
  OrderedTable k v
  -> (k -> Ordering)
  -> '{Transaction, Batch, Stream (k, v)} ()
durable.OrderedTable.search.tx tree ordering =
  search.keys.chunked.tx tree ordering |> bulkFetchValues.tx tree

durable.OrderedTable.toOrderator :
  OrderedTable k v -> '{Remote, Orderator (k, '{Remote} v)} ()
durable.OrderedTable.toOrderator tree =
  toOrderator.keys tree
    |> Orderator.map (k -> (k, do OrderedTable.read tree k))

durable.OrderedTable.toOrderator.keys :
  OrderedTable k v -> '{Remote, Orderator k} ()
durable.OrderedTable.toOrderator.keys tree = do
  go : '{Remote, Stream k} () ->{Remote, Orderator k} ()
  go stream = match Stream.uncons stream with
    Left _          -> ()
    Right (k, next) ->
      match Orderator.yield k with
        Next        -> go next
        SkipWhile p ->
          ord k = if p k then Greater else Less
          go (search.keys tree ord)
  go (toStream.keys tree)

durable.OrderedTable.toOrderator.keys.doc : Doc
durable.OrderedTable.toOrderator.keys.doc =
  {{ Creates a {type Orderator} over the keys of a {type OrderedTable} }}

durable.OrderedTable.toOrderator.keys.tx :
  OrderedTable k v -> '{Transaction, Orderator k} ()
durable.OrderedTable.toOrderator.keys.tx tree = do
  go : '{Transaction, Stream k} () ->{Transaction, Orderator k} ()
  go stream = match Stream.uncons stream with
    Left _          -> ()
    Right (k, next) ->
      match Orderator.yield k with
        Next        -> go next
        SkipWhile p ->
          ord k = if p k then Greater else Less
          go (search.keys.tx tree ord)
  go (toStream.keys.tx tree)

durable.OrderedTable.toOrderator.keys.tx.doc : Doc
durable.OrderedTable.toOrderator.keys.tx.doc =
  {{
  Creates a {type Orderator} over the keys of a {type OrderedTable}. Unlike
  {toOrderator.keys}, this version is fully transactional, but note that it's
  easy to hit the transactional limit of 100 elements when doing big joins or
  even just several skip aheads with the resulting {type Orderator}: use with
  caution
  }}

durable.OrderedTable.toOrderator.tx :
  OrderedTable k v
  -> '{Transaction, Orderator (k, '{Transaction, Exception} v)} ()
durable.OrderedTable.toOrderator.tx tree =
  toOrderator.keys.tx tree
    |> Orderator.map (k -> (k, do OrderedTable.read.tx tree k))

durable.OrderedTable.toStream : OrderedTable k v -> '{Remote, Stream (k, v)} ()
durable.OrderedTable.toStream tree = OrderedTable.search tree (_ -> Less)

durable.OrderedTable.toStream.keys : OrderedTable k v -> '{Remote, Stream k} ()
durable.OrderedTable.toStream.keys tree = search.keys tree (_ -> Less)

durable.OrderedTable.toStream.keys.chunked :
  OrderedTable k v -> '{Remote, Stream [k]} ()
durable.OrderedTable.toStream.keys.chunked tree =
  search.keys.chunked tree (_ -> Less)

durable.OrderedTable.toStream.keys.chunked.tx :
  OrderedTable k v -> '{Transaction, Stream [k]} ()
durable.OrderedTable.toStream.keys.chunked.tx tree =
  search.keys.chunked.tx tree (_ -> Less)

durable.OrderedTable.toStream.keys.tx :
  OrderedTable k v -> '{Transaction, Stream k} ()
durable.OrderedTable.toStream.keys.tx tree = search.keys.tx tree (_ -> Less)

durable.OrderedTable.toStream.tx :
  OrderedTable k v -> '{Transaction, Batch, Stream (k, v)} ()
durable.OrderedTable.toStream.tx tree = search.tx tree (_ -> Less)

durable.OrderedTable.tryRead : OrderedTable k v -> k ->{Remote} Optional v
durable.OrderedTable.tryRead tree k =
  toRemote do
    transact (OrderedTable.database tree) do OrderedTable.tryRead.tx tree k

durable.OrderedTable.tryRead.tx :
  OrderedTable k v -> k ->{Transaction} Optional v
durable.OrderedTable.tryRead.tx tree k =
  Transaction.tryRead.tx (internal.values tree) k

durable.OrderedTable.write : OrderedTable k v -> k -> v ->{Remote} ()
durable.OrderedTable.write tree k v =
  toRemote do
    transact (OrderedTable.database tree) do OrderedTable.write.tx tree k v

durable.OrderedTable.write.tx :
  OrderedTable k v -> k -> v ->{Transaction, Random} ()
durable.OrderedTable.write.tx tree k v =
  use List +: size
  use Nat + / >
  use Node keys
  use Transaction write.tx
  use internal.Node Node
  get! : Optional a -> a
  get! = getOrBug "OrderedMap: empty list of keys"
  writeNode node = write.tx (internal.index tree) (Node.id node) node
  writeRoot k children =
    writeNode (Node Root [k] children (List.isEmpty children))
  insert k pointer node =
    slot = node |> findKeySlot tree k
    node
      |> Node.keys.modify (insertAt slot k)
      |> (pointers.modify match pointer with
           None         -> Function.id
           Some pointer -> insertAt (slot + 1) (Node.id pointer))
  overflows node =
    if isLeaf node then (node |> keys |> size) > maxFanout tree
    else (node |> Node.pointers |> size) > maxFanout tree
  splitNode node =
    (leftKeys, (medianKey, rightKeys)) =
      node |> keys |> halve |> Tuple.mapRight (List.uncons >> get!)
    if isLeaf node then
      use Node id
      rightLeaf =
        Node newId() (medianKey +: rightKeys) (node |> Node.pointers) true
      leftLeaf = Node (id node) leftKeys [id rightLeaf] true
      (leftLeaf, medianKey, rightLeaf)
    else
      pointers = node |> Node.pointers
      splitPoint =
        size pointers / 2 + (if size pointers |> Nat.isOdd then 1 else 0)
      let
        (leftChildren, rightChildren) = pointers |> List.splitAt splitPoint
        leftBranch = Node (Node.id node) leftKeys leftChildren false
        rightBranch = Node newId() rightKeys rightChildren false
        (leftBranch, medianKey, rightBranch)
  wasPresent = OrderedTable.tryRead.tx tree k
  write.tx (internal.values tree) k v
  match wasPresent with
    Some _ -> ()
    None   ->
      match findLeaf tree k with
        []                -> writeRoot k []
        leaf +: ancestors ->
          propagateInsertion node ancestors =
            if Boolean.not (overflows node) then writeNode node
            else
              (leftNode, medianKey, rightNode) = splitNode node
              writeNode leftNode
              writeNode rightNode
              match ancestors with
                []                  ->
                  leftId = newId()
                  writeNode (leftNode |> Node.id.set leftId)
                  writeRoot medianKey [leftId, Node.id rightNode]
                parent +: ancestors ->
                  node = parent |> insert medianKey (Some rightNode)
                  propagateInsertion node ancestors
          node = leaf |> insert k None
          propagateInsertion node ancestors

durable.README : Doc
durable.README =
  {{
  Durable data structures based on {type Storage} for use in Unison Cloud.

  {{
  docTable
    [ [ {{
        {type OrderedTable}
        }}
      , {{
        Sorted Map with read, write, delete, and range queries
        }}
      ]
    , [{{ {type Cell} }}, {{ A single durable value }}]
    , [ {{
        {type LinearLog}
        }}
      , {{
        A mutable array with known size, supporting adds at the end
        }}
      ]
    , [ {{
        {type Immutable}
        }}
      , {{
        Content-addressed datastore, garbage-collected via reference counting
        }}
      ]
    , [{{ {type Knn} }}, {{ Approximate nearest neighbor search index }}]
    ] }}
  }}

durable.up.Bytes.toBase16.text : Bytes -> Text
durable.up.Bytes.toBase16.text bs =
  match catch do Bytes.toBase16 bs |> fromUtf8 with
    Left e  -> bug "bug in Bytes.toBase16Hex, it somehow produced invalid utf8"
    Right a -> a

durable.up.Bytes.toBase16.text.doc : Doc
durable.up.Bytes.toBase16.text.doc =
  {{
  Like {Bytes.toBase16}, but converts the result to {type Text}.

  ```
  up.Bytes.toBase16.text 0xs2a3b90823402023948
  ```
  }}

test> durable.up.Bytes.toBase16.text.test = test.verify do
  use Nat *
  Each.repeat 20
  bs = Random.bytes (Random.natIn 0 100 * 32)
  test.ensureEqual (up.Bytes.toBase16.text bs) (toDebugText bs |> Text.drop 3)

durable.up.Bytes.toBase32Hex.text : Bytes -> Text
durable.up.Bytes.toBase32Hex.text bs =
  match catch do toBase32Hex bs |> fromUtf8 with
    Left e  -> bug "bug in Bytes.toBase32Hex, it somehow produced invalid utf8"
    Right a -> a

durable.up.Bytes.toBase32Hex.text.doc : Doc
durable.up.Bytes.toBase32Hex.text.doc =
  {{
  Like {toBase32Hex}, but converts the result to {type Text}.

  ```
  up.Bytes.toBase32Hex.text 0xs2a3b9082340202394802
  ```
  }}

durable.up.Float.argMax : (a -> Float) -> [a] ->{Exception} a
durable.up.Float.argMax f = argMin (f >> Float.negate)

durable.up.Float.argMin : (a ->{g} Float) -> [a] ->{g, Exception} a
durable.up.Float.argMin by as = match tryArgMin by as with
  None   -> raiseGeneric "Float.argMin []" ()
  Some a -> a

durable.up.Float.argMin.indexed : (a -> Float) -> [a] ->{Exception} (a, Nat)
durable.up.Float.argMin.indexed by as =
  use Float <
  use Nat +
  go1 i a f = cases
    []       -> (a, i)
    a1 +: as ->
      fa1 = by a1
      i' = i + 1
      if fa1 < f then go1 i' a1 fa1 as else go1 i' a f as
  go0 = cases
    []      -> raiseGeneric "Float.argMin []" ()
    a +: as -> go1 0 a (by a) as
  go0 as

durable.up.Float.argMin.indexed.doc : Doc
durable.up.Float.argMin.indexed.doc =
  {{ Like {argMin} but returns the index of the minimum element. }}

durable.up.Float.maxIndex : (a -> Float) -> [a] ->{Exception} Nat
durable.up.Float.maxIndex f as =
  at2 (argMin (at1 >> f >> Float.negate) (List.indexed as))

durable.up.Float.minBy : (a ->{g} Float) -> [a] ->{g} Float
durable.up.Float.minBy f as =
  go m = cases
    []     -> m
    h +: t -> go (Float.min m (f h)) t
  go Infinity as

durable.up.Float.selectMax : (a -> Float) -> [a] ->{Exception} (a, [a])
durable.up.Float.selectMax f as =
  use List ++
  use Nat +
  i = maxIndex f as
  (List.unsafeAt i as, List.take i as ++ List.drop (i + 1) as)

durable.up.Float.tryArgMin : (a ->{g} Float) -> [a] ->{g} Optional a
durable.up.Float.tryArgMin by as =
  use Float <
  go1 a f = cases
    []       -> Some a
    a1 +: as ->
      fa1 = by a1
      if fa1 < f then go1 a1 fa1 as else go1 a f as
  go0 = cases
    []      -> None
    a +: as -> go1 a (by a) as
  go0 as

durable.up.Map.toStream : data.Map k b -> '{Stream (k, b)} ()
durable.up.Map.toStream m = do Map.foldLeftWithKey (_ k a -> emit (k, a)) () m

durable.up.Map.toStream.doc : Doc
durable.up.Map.toStream.doc =
  use Map toStream
  {{
  `` toStream m `` is just like `` Map.toList m `` but returns a {type Stream},
  useful when lazily enumerating the entries of a {type data.Map}.

  An example:

  ```
  Map.fromList [("a", 1), ("b", 2), ("c", 3)]
    |> toStream
    |> Stream.take 2
    |> Stream.toList
  ```
  }}

test> durable.up.Map.toStream.tests = test.verify do
  use Nat range
  m = Map.fromList <| (Each.toList do (each (range 0 20), each (range 0 20)))
  r1 = Map.toStream m |> Stream.toList
  r2 = Map.toList m
  test.ensureEqual r1 r2

durable.up.Stream.distinctAdjacentBy :
  (i ->{g1} o) -> '{g} t -> '{g, g1, Stream i} t
durable.up.Stream.distinctAdjacentBy by s = do
  go prev = cases
    { a }             -> a
    { emit hd -> tl } ->
      key = Some (by hd)
      if key === prev then handle tl() with go key
      else
        emit hd
        handle tl() with go key
  handle s() with go None

durable.up.Stream.distinctBy : (i ->{g1} o) -> '{g} t -> '{g1, g, Stream i} t
durable.up.Stream.distinctBy by s = do
  use Nat ==
  use Set size
  go seen = cases
    { a }             -> a
    { emit hd -> tl } ->
      seen2 = Set.insert (by hd) seen
      if size seen2 == size seen then handle tl() with go seen
      else
        emit hd
        handle tl() with go seen2
  handle s() with go Set.empty

durable.up.Stream.distinctBy.doc : Doc
durable.up.Stream.distinctBy.doc =
  use Nat /
  {{
  `` Stream.distinctBy by s `` is just like `` List.distinctBy by s `` but
  returns a {type Stream}.

  An example:

  ```
  Stream.range 0 10000000
    |> Stream.distinctBy (x -> x / 10)
    |> Stream.take 5
    |> Stream.toList
  ```
  }}

test> durable.up.Stream.distinctBy.tests = test.verify do
  use Nat /
  n = each [1, 2, 3, 4, 5, 6, 7]
  nums = Nat.range 0 30
  r1 = Stream.distinctBy (x -> x / n) (Stream.fromList nums) |> Stream.toList
  r2 = List.distinctBy (x -> x / n) nums
  test.ensureEqual r1 r2

durable.up.Stream.toSet! : '{g, Stream a} r ->{g} Set a
durable.up.Stream.toSet! stream =
  go acc = cases
    { r }              -> acc
    { emit a -> cont } -> handle cont() with go (Set.insert a acc)
  handle stream() with go Set.empty

durable.util.nestedTable : fn -> parent -> Table k v
durable.util.nestedTable fn parent =
  name = blake2b_256 (fn, parent) |> up.Bytes.toBase32Hex.text
  Table.Table name

durable.util.normalizedGzipDistance : x -> y -> Float
durable.util.normalizedGzipDistance x y =
  use Value serialize value
  normalizedGzipDistanceBytes (serialize (value x)) (serialize (value y))

durable.util.normalizedGzipDistance.doc : Doc
durable.util.normalizedGzipDistance.doc =
  {{
  Like {normalizedGzipDistanceBytes} but works for any two values, which need
  not even be of the same type.

  ```
  normalizedGzipDistance ("hello there", 0) "hello there"
  ```
  }}

durable.util.normalizedGzipDistance.examples.ex1 : Text
durable.util.normalizedGzipDistance.examples.ex1 =
  "Japan’s Seiko Epson Corp. has developed a 12-gram flying microrobot."

durable.util.normalizedGzipDistance.examples.ex2 : Text
durable.util.normalizedGzipDistance.examples.ex2 =
  "The latest tiny flying robot has been unveiled in Japan."

durable.util.normalizedGzipDistance.examples.ex3 : Text
durable.util.normalizedGzipDistance.examples.ex3 =
  "Michael Phelps won the gold medal in the 400 individual medley."

durable.util.normalizedGzipDistanceBytes : Bytes -> Bytes -> Float
durable.util.normalizedGzipDistanceBytes x y =
  use Bytes ++
  use Float - /
  C bs = Bytes.size (gzip.compress bs) |> Float.fromNat
  cx = C x
  cy = C y
  (C (x ++ y) - Float.min cx cy) / Float.max cx cy

durable.util.normalizedGzipDistanceBytes.doc : Doc
durable.util.normalizedGzipDistanceBytes.doc =
  use Text toUtf8
  {{
  [Normalized Compression Distance](https://en.wikipedia.org/wiki/Normalized_compression_distance)
  using {gzip.compress} as the compression algorithm.

  This returns a value between 0.0 and 1.0, where 0.0 means the two values are
  equal, and 1.0 means they are maximally dissimilar.

  **Examples:**

  ```
  msg1 = "hello there, my name is alice"
  msg2 = "hello there, my name is bob"
  normalizedGzipDistanceBytes (toUtf8 msg1) (toUtf8 msg2)
  ```

  Here's an example of two dissimilar values, we're using the hash function
  {blake2b_256} to obtain two different random {type Bytes}:

  ```
  normalizedGzipDistanceBytes (blake2b_256 1) (blake2b_256 2)
  ```

  **Also see:** {normalizedGzipDistanceText}, {normalizedGzipDistance}
  }}

durable.util.normalizedGzipDistanceText : Text -> Text -> Float
durable.util.normalizedGzipDistanceText x y =
  use Text toUtf8
  normalizedGzipDistanceBytes (toUtf8 x) (toUtf8 y)

durable.util.normalizedGzipDistanceText.doc : Doc
durable.util.normalizedGzipDistanceText.doc =
  use Bytes toHex
  {{
  [Normalized Compression Distance](https://en.wikipedia.org/wiki/Normalized_compression_distance)
  using {gzip.compress} as the compression algorithm.

  This returns a value between 0.0 and 1.0, where 0.0 means the two values are
  equal, and 1.0 means they are maximally dissimilar.

  **Examples:**

  ```
  msg1 = "hello there, my name is alice"
  normalizedGzipDistanceText msg1 msg1
  ```

  ```
  msg1 = "hello there, my name is alice"
  msg2 = "hello there, my name is bob"
  normalizedGzipDistanceText msg1 msg2
  ```

  Here's an example with two dissimilar values, using the hash function
  {blake2b_256} to obtain two different random {type Bytes}:

  ```
  msg1 = blake2b_256 1 |> toHex
  msg2 = blake2b_256 2 |> toHex
  normalizedGzipDistanceText msg1 msg2
  ```

  **Also see:** {normalizedGzipDistance}, {normalizedGzipDistanceBytes}
  }}

durable.util.randomName : '{Random} Text
durable.util.randomName = do Random.bytes 256 |> up.Bytes.toBase32Hex.text

durable.util.Text.paragraphs : Text -> [Text]
durable.util.Text.paragraphs t =
  use List :+
  use Pattern many run
  use Text ++
  nl = chars "\n\r"
  blankline = Pattern.join [nl, many (chars " \t\f"), nl]
  untilNewline = Pattern.capture (Pattern.some (notChars "\n\r"))
  sp = many space
  go acc cur t =
    match run untilNewline t with
      None -> acc
      Some ([line], rem) ->
        match run blankline rem with
          None -> go acc (cur ++ line ++ Text.take 1 rem) (Text.drop 1 rem)
          Some (_, rem) -> go (acc :+ (cur ++ line)) "" (Pattern.drop sp rem)
      Some _ -> bug "impossible: untilNewline only captures one thing"
  go [] "" (Pattern.drop sp t)

durable.util.Text.words : Text -> [Text]
durable.util.Text.words t =
  use Pattern join many
  c = Class.not whitespace
  word = join [Pattern.capture (Pattern.some (patterns.char c)), many space]
  captures (join [many space, many word]) t

durable.util.Text.words.alphanumeric : Text -> [Text]
durable.util.Text.words.alphanumeric t =
  use Class +
  use Pattern join many
  use patterns char
  c = Class.alphanumeric + in "'ʼ"
  nc = many (char (Class.not c))
  word = join [Pattern.capture (Pattern.some (char c)), nc]
  captures (join [nc, many word]) t

Environment.Config.doc : Doc
Environment.Config.doc =
  {{
  A simple ability for accessing dynamic configuration of a service or batch
  job. Config values are set using {setValue} and unset using {deleteValue}.

  @typecheck ```
  env = Environment.named "my-env"
  setValue env "api-key" (getEnv "SECRET_API_KEY")
  Cloud.submit env do
    key = Config.lookup "api-key" |> getOrBug "expected api-key in config"
    todo "do service logic using api-key"
  ```

  Config values are stored encrypted on Unison Cloud and only exist unencrypted
  in memory while the service or batch job is running.
  }}

Environment.Config.expect : Text ->{Environment.Config, Exception} Text
Environment.Config.expect key =
  match Config.lookup key with
    Some value -> value
    None ->
      Exception.raise
        (Failure
          (typeLink MissingKey)
          ("No value found for key '" Text.++ key Text.++ "'")
          (Any (key : Text)))

Environment.Config.expect.doc : Doc
Environment.Config.expect.doc =
  {{
  `Config.expect key` looks up the value associated with the provided `key` in
  the configuration. If no value is found, an exception is raised.
  }}

Environment.Config.failure.MissingKey.doc : Doc
Environment.Config.failure.MissingKey.doc =
  {{ The requested configuration key was not found in the environment. }}

Environment.Config.lookup.doc : Doc
Environment.Config.lookup.doc =
  {{
  Looks up a configuration value. Takes a {type Text} key and returns
  {type Optional} {type Text} containing the value of the key in the
  environment, if it exists.

  # Example

    This looks up the value of the value of "api-key" in the {type Environment}
    "my-env":

    @typecheck ```
    env = Environment.named "my-env"
    setValue env "api-key" (getEnv "SECRET_API_KEY")
    Cloud.submit env do
      key = Config.lookup "api-key" |> getOrBug "expected api-key in config"
      todo "do service logic using api-key"
    ```
  }}

Environment.default : '{Exception, Cloud} Environment
Environment.default = do Environment.named "default"

Environment.default.doc : Doc
Environment.default.doc =
  {{
  The default {type Environment}, with the name `"default"`, useful for testing
  or short examples.

  Provides access to {Database.default}.

  The computation is idempotent so you can force it whenever you want.
  }}

Environment.delete : Environment ->{Exception, Cloud} ()
Environment.delete env = Either.toException (Environment.delete.impl env)

Environment.delete.doc : Doc
Environment.delete.doc =
  {{
  `Cloud.Environment.delete env` deletes the provided environment. It will
  remove all values associated with the environment, and will prevent any new
  services from being deployed to the environment.
  }}

Environment.deleteValue : Environment -> Text ->{Exception, Cloud} ()
Environment.deleteValue env key = Either.toException (deleteValue.impl env key)

Environment.deleteValue.doc : Doc
Environment.deleteValue.doc =
  {{
  `Cloud.Enviroment.deleteValue environment key` deletes the `value` assigned
  to the `key` within the provided `environment`.
  }}

Environment.doc : Doc
Environment.doc =
  {{
  A Unison Cloud computation or service can be deployed with an
  {type Environment} to provide runtime access to configuration (including
  secrets) via the {type Environment.Config} ability.

  For example, an {type Environment} might represent the staging environment
  for your chatbot service and may include a database URI/name/password and
  some API keys.

  Use {Environment.named} to create an {type Environment} and {setValue} add
  configuration/secrets to it.
  }}

Environment.id : Environment -> Environment.Id
Environment.id = cases Environment id _ -> id

Environment.id.doc : Doc
Environment.id.doc = {{ The unique identifier assigned to an environment. }}

Environment.Id.fromJson : '{Decoder} Environment.Id
Environment.Id.fromJson = do Environment.Id.Id Decoder.text()

Environment.Id.toText : Environment.Id -> Text
Environment.Id.toText = cases Environment.Id.Id t -> t

Environment.list : '{Exception, Cloud} [Environment]
Environment.list = do Either.toException list.impl

Environment.name : Environment -> Text
Environment.name = cases Environment _ id -> id

Environment.name.doc : Doc
Environment.name.doc =
  {{
  A user-friendly display name for an environment, such as "chatbot-staging".
  }}

Environment.named : Text ->{Exception, Cloud} Environment
Environment.named name = Either.toException (Environment.create.impl name)

Environment.named.doc : Doc
Environment.named.doc =
  {{
  `Cloud.Enviroment.create name` creates a new environment with the given
  display name. The {Environment.id} of the returned {type Environment} will be
  the canonical identifier for the environment, and the environment name is a
  user-friendly label as described by {Environment.name}.
  }}

Environment.named.validateName : Text ->{Exception} ()
Environment.named.validateName name =
  use Nat < >
  validChars =
    Pattern.join
      [ asciiLetter
      , Pattern.many
          (Pattern.oneOf
            (asciiLetter +| [patterns.digit, patterns.charIn [?-]]))
      ]
  length = Text.size name
  when
    (length < 1 || length > 63
    || Boolean.not (isFullMatch validChars name))
    do
    msg =
      "Environment names should start with a letter, have length >1 and <64, and contain only {a-z}, {A-Z}, {0-9} and hyphen (-)"
    Exception.raise (failure msg name)

Environment.setValue : Environment -> Text -> Text ->{Exception, Cloud} ()
Environment.setValue env key value =
  Either.toException (setValue.impl env key value)

Environment.setValue.doc : Doc
Environment.setValue.doc =
  {{
  `Cloud.Enviroment.setValue environment key value` sets the `value` assigned
  to the `key` within the provided `environment`. The value will be available
  to all services deployed to the given environment.
  }}

Environment.toText : Environment -> Text
Environment.toText = cases
  Environment id name ->
    Environment.Id.toText id Text.++ " (" Text.++ name Text.++ ")"

errors.UnknownDaemon.unknownDaemonHash : DaemonHash -> Failure
errors.UnknownDaemon.unknownDaemonHash hash =
  use Text ++
  Failure
    (typeLink UnknownDaemon)
    ("unkown daemon hash: " ++ DaemonHash.toText hash)
    (Any (hash : DaemonHash))

errors.UnknownService.unknownServiceHash : ServiceHash a b -> Failure
errors.UnknownService.unknownServiceHash serviceHash =
  use Text ++
  Failure
    (typeLink UnknownService)
    ("No service found for ServiceHash '"
      ++ ServiceHash.toText serviceHash
      ++ "'")
    (Any serviceHash)

examples.calculate7 : '{Remote} Nat
examples.calculate7 = do
  use Nat +
  three = forkAt region! do 1 + 2
  four = forkAt region! do 3 + 1
  await three + await four

examples.deployLoggingService :
  '{Exception, Cloud} ServiceHash HttpRequest HttpResponse
examples.deployLoggingService = do
  env = Environment.named "test"
  deployHttp env loggingService

examples.fetchMyServiceLogs : '{IO, Exception} [Json]
examples.fetchMyServiceLogs =
  do logs.service myServiceHash QueryOptions.default

examples.httpServices : '{IO, Exception} Text
examples.httpServices =
  main.local do
    use Path /
    service = cases
      req| Routes.get (root / "greet") req  ->
        HttpResponse.ok (Body (Text.toUtf8 "Hello World"))
      _ -> HttpResponse.notFound
    env = Environment.named "env"
    serviceHash = deployHttp.remote env service |> ServiceHash.toText
    job =
      pool.wrap do
        req =
          HttpRequest.get
            (parseOptional "http://localhost:8080"
              |> getOrBug "malformed uri"
              |> path.set (root / "h" / serviceHash / "greet"))
        request req |> HttpResponse.body |> Body.toBytes |> fromUtf8
    submit.remote env job

examples.interactive : '{IO, Exception} ()
examples.interactive = main.local.serve do
  greeter : '{Remote} ()
  greeter = do
    toRemote do info "hello" []
    Remote.sleep (millis 400)
    greeter()
  Cloud.submit myEnvironment greeter

examples.loggingService : HttpRequest ->{Remote, Log} HttpResponse
examples.loggingService = cases
  HttpRequest method _ uri headers body ->
    use Method toText
    event = do Object [("method", Json.Text (toText method))]
    go = do
      info "request" [("method", toText method)]
      HttpResponse.ok (Body (Text.toUtf8 "logged"))
    toRemote go

examples.main : '{IO, Exception} ()
examples.main = do
  Cloud.run do
    use Text ++
    result = submit.remote (Environment.named "default") calculate7
    printLine ("result: " ++ Nat.toText result)

examples.myEnvironment : Environment
examples.myEnvironment = Environment (Environment.Id.Id "someID#12345") "name"

examples.myServiceHash : ServiceHash HttpRequest HttpResponse
examples.myServiceHash =
  ServiceHash "2JMnIko5m5YhbZnMwJmZEScXv__SkgfmVOvn4dH45F0"

examples.namedHttpServices : '{IO, Exception} [Text]
examples.namedHttpServices =
  main.local do
    use HttpResponse ok
    use Path /
    use Text toUtf8
    service = cases
      req
        | Routes.get root req              ->
          ok (Body (toUtf8 "Cannot happen"))
        | Routes.get (root / "") req       -> ok (Body (toUtf8 "My service"))
        | Routes.get (root / "greet") req  ->
          ok (Body (toUtf8 "My service greet"))
      _ -> HttpResponse.notFound
    env = Environment.named "env"
    serviceName = ServiceName.named "myService"
    serviceHash = deployHttp.remote env service
    basePath = ServiceName.assign serviceName serviceHash
    path1 = root / "s" / "myService"
    path2 = root / "s" / "myService" / ""
    path3 = root / "s" / "myService" / "greet"
    testCall path = submit.remote env do
      toRemote do
        req = HttpRequest.get (basePath |> path.set path)
        request req |> HttpResponse.body |> Body.toBytes |> fromUtf8
    [path1, path2, path3] |> List.map testCall

examples.runBoth : '{Remote} (Nat, Nat)
examples.runBoth = do
  use Nat +
  add1 n1 = do n1 + 1
  add2 n2 = do n2 + 2
  Remote.both (add1 1) (add2 2)

examples.runBothLogged : '{Remote} (Nat, Nat)
examples.runBothLogged = do
  use Nat + toText
  add1 n1 = toRemote do
    result = n1 + 1
    info "add1" [(toText n1, toText result)]
    result
  add2 n2 = toRemote do
    result = n2 + 2
    info "add2" [(toText n2, toText result)]
    result
  Remote.both (do add1 1) do add2 2

examples.serviceCallAndConfig :
  '{IO, Exception} (Optional Text, Optional Text, Optional Text)
examples.serviceCallAndConfig = main.local do
  use Environment named
  service : Text ->{Remote} Optional Text
  service k = toRemote do Config.lookup k
  env = named "myEnv"
  setValue env "name" "Ada"
  setValue env "surname" "Lovelace"
  serviceHash = deploy.remote env service
  job = do
    use Services call
    a = call serviceHash "name"
    b = call serviceHash "surname"
    c = call serviceHash "nope"
    (a, b, c)
  env2 = named "default"
  setValue env2 "name" "Nemo"
  submit.remote env2 do toRemote job

examples.simpleBatch.main : '{IO, Exception} Nat
examples.simpleBatch.main = Cloud.main do
  use Nat +
  environment = Environment.default()
  Cloud.submit environment do
    result = Remote.parMap (n -> n + 1) (Nat.range 0 1000)
    result |> Nat.sum

examples.simpleHttp.main :
  '{IO, Exception} ServiceHash HttpRequest HttpResponse
examples.simpleHttp.main = Cloud.main do
  helloService : HttpRequest -> HttpResponse
  helloService request = HttpResponse.ok (Body (Text.toUtf8 "Hello world"))
  deployHttp Environment.default() helloService

examples.simpleHttpNamed.main : '{IO, Exception} URI
examples.simpleHttpNamed.main = Cloud.main do
  helloService : HttpRequest -> HttpResponse
  helloService request = HttpResponse.ok (Body (Text.toUtf8 "Hello world"))
  serviceHash = deployHttp Environment.default() helloService
  serviceName = ServiceName.named "hello-world-service"
  ServiceName.assign serviceName serviceHash

examples.simpleLogHttp.main :
  '{IO, Exception} ServiceHash HttpRequest HttpResponse
examples.simpleLogHttp.main = do
  Cloud.run do
    environment = Environment.default()
    simpleHttpService : HttpRequest ->{Log} HttpResponse
    simpleHttpService = cases
      request ->
        info "Received request" []
        HttpResponse.ok (Body (Text.toUtf8 "Hello world"))
    deployHttp environment simpleHttpService

examples.simpleStateful :
  Database
  -> Table URI Nat
  -> HttpRequest
  ->{Exception, Storage, Log} HttpResponse
examples.simpleStateful database table request =
  use Nat +
  use Transaction write.tx
  uri = HttpRequest.uri request
  info "Received request" [("uri", URI.toText uri)]
  transact database do match Transaction.tryRead.tx table uri with
    None               -> write.tx table uri 1
    Some activityCount -> write.tx table uri (activityCount + 1)
  HttpResponse.ok (Body (Text.toUtf8 "Hello world"))

examples.simpleStatefulHttp.main :
  '{IO, Exception} ServiceHash HttpRequest HttpResponse
examples.simpleStatefulHttp.main = do
  Cloud.run do
    environment = Environment.default()
    database = Database.named "myDatabase"
    Database.assign database environment
    table : Table URI Nat
    table = Table.Table "requestCounter"
    deployHttp environment (simpleStateful database table)

examples.uploadFile :
  Environment
  -> Database
  -> Table Text Bytes
  -> FilePath
  -> Text
  ->{IO, Exception, Cloud} ()
examples.uploadFile env db tbl file key =
  use Text ++
  contents : Bytes
  contents = readFile file
  Cloud.submit env do Storage.write db tbl key contents
  printLine
    ("Successfully uploaded ("
      ++ Nat.toText (Bytes.size contents)
      ++ " bytes)")

examples.uploadFile.doc : Doc
examples.uploadFile.doc =
  {{
  `` uploadFile env db tbl file key `` is a batch job that uploads a `file` to
  `tbl` under the given {type Table} `key`.

  For this to work, the {type Database} must be accessible to the given
  {type Environment}, using {Database.assign}.
  }}

examples.webSocketCleanup.handler :
  websockets.WebSocket ->{Exception, Remote, WebSockets} ()
examples.webSocketCleanup.handler ws =
  addFinalizer (result -> toRemote do info "remote websocket was closed" [])
  msg = WebSockets.receive ws
  WebSockets.send ws msg
  WebSockets.close ws

examples.webSocketEcho.deploy : '{IO, Exception} URI
examples.webSocketEcho.deploy = do
  Cloud.run do
    handler ws =
      msg = WebSockets.receive ws
      WebSockets.send ws msg
      WebSockets.close ws
    service _ = Right handler
    serviceName = ServiceName.named "echo-service"
    hash = deployHttpWebSocket Environment.default() service
    ServiceName.assign serviceName hash

internal.cloud.runThunk :
  AccessToken
  -> (QueryOptions ->{IO, Exception} [Json])
  -> Environment
  -> Connection
  -> '{Http, Remote, Scratch} a
  ->{IO, Exception} a
internal.cloud.runThunk accessToken fetchLogs environment connection thunk =
  use CloudResponse decode
  use Exception raise
  use Link Term
  use Text ++
  logThreadId = IO.ref None
  logThread : JobId -> '{IO} ()
  logThread jobId =
    do
      options = QueryOptions.default |> search.set (Some (JobId.toText jobId))
      let
        (Left f) =
          catchAll do
            (do pageLogs options fetchLogs)
              |> Stream.map Json.toText
              |> Stream.foreach printLine
        threadKilled = typeLink ThreadKilledFailure
        when (failureType f !== threadKilled) do
          Debug.trace "error fetching logs" f
  stopLogThread : '{IO, Exception} ()
  stopLogThread = do match mutable.Ref.read logThreadId with
    None          -> ()
    Some threadId -> concurrent.kill threadId
  lookupTerms : [Term] ->{IO, Exception} [(Term, Bytes)]
  lookupTerms =
    List.map
      (term ->
        (match Code.lookup term with
          Some code -> (term, Code.serialize code)
          None ->
            raise
              (Failure
                (typeLink UnknownTerm)
                "The server requested the definition for a term that I can't find. This is a bug in the Unison runtime or Unison Cloud."
                (Any term))))
  sendAndReceiveResponse : CloudRequest ->{IO, Exception, Decode} CloudResponse
  sendAndReceiveResponse req =
    handle CloudRequest.send connection req
    with cases
      { () } -> decode()
      { raise sendFailure -> _ } ->
        e =
          match handleFailure (_ _ -> None) (decode >> Some) with
            None -> sendFailure
            Some (FailureReply serverMsg) ->
              msg =
                "An error occurred while sending a request to Unison Cloud. The server responded with:\n"
                  ++ serverMsg
              Generic.failure msg ()
            Some otherResponse ->
              msg =
                "An error occurred while sending a request to Unison Cloud. I expected the server to return an error message, but it returned something else."
              Failure
                (typeLink ProtocolError)
                msg
                (Any (otherResponse : CloudResponse))
        raise e
  go : CloudResponse ->{IO, Exception, Decode} a
  go = cases
    JobStarted jobid ->
      match getEnv.impl "UNISON_CLOUD_SUBMIT_DISABLE_LOG_STREAMING" with
        Right "1"    -> ()
        Right "true" -> ()
        Right "TRUE" -> ()
        _            ->
          threadId = concurrent.fork (logThread jobid)
          mutable.Ref.write logThreadId (Some threadId)
      go decode()
    TermRequest requestedTerms ->
      go (sendAndReceiveResponse (TermReply (lookupTerms requestedTerms)))
    TaskResult (Left failure) -> raise failure
    TaskResult (Right resultBytes) -> decodeResult resultBytes
    FailureReply t -> raise (Generic.failure t (Any ()))
  initialRequest =
    ForkRequest
      accessToken
      (Environment.id environment |> Environment.Id.toText)
      (Thunk.create thunk)
  handle
    fromConnection connection do go (sendAndReceiveResponse initialRequest)
  with cases
    { throw decodeError -> _ } ->
      stopLogThread()
      raise
        (Failure
          (typeLink SerializationError)
          "Could not decode response from Unison Cloud"
          (Any decodeError))
    { raise f -> _ } ->
      stopLogThread()
      raise f
    { a } ->
      stopLogThread()
      a

internal.CloudRequest.send : Connection -> CloudRequest ->{IO, Exception} ()
internal.CloudRequest.send connection msg =
  use Bytes ++ size
  use Text toUtf8
  sendPayload : Nat -> Bytes ->{IO, Exception} ()
  sendPayload type' payload =
    typeBytes = encodeNat32be type'
    sizeBytes = encodeNat32be (size payload)
    Connection.send connection (typeBytes ++ sizeBytes ++ payload)
  match msg with
    ForkRequest (AccessToken token) environmentId (Thunk thunkBytes) ->
      tokenBytes = toUtf8 token
      envBytes = toUtf8 environmentId
      payload =
        encodeNat16be (size tokenBytes)
          ++ tokenBytes
          ++ encodeNat16be (size envBytes)
          ++ envBytes
          ++ thunkBytes
      sendPayload 104 payload
    TermReply termReply ->
      payload = Value.serialize (Value.value termReply)
      sendPayload 102 payload

internal.CloudResponse.decode : '{IO, Exception, Decode} CloudResponse
internal.CloudResponse.decode =
  do
    use Decode nat32be
    use Text ++
    tipe = nat32be()
    size = nat32be()
    payload = nextBytes size
    match tipe with
      201 -> TermRequest.decode payload
      212 -> JobStarted.decode payload
      202 -> TaskResult.decode payload
      203 -> FailureReply.decode payload
      _ ->
        Exception.raise
          (Failure
            (typeLink SerializationError)
            ("Unexpected CloudResponse type: " ++ Nat.toText tipe)
            (Any tipe))

internal.CloudResponse.FailureReply.decode : Bytes ->{Exception} CloudResponse
internal.CloudResponse.FailureReply.decode bs =
  decode = do FailureReply (fromUtf8 bs)
  match toEither do runDecode decode bs with
    Left _ ->
      Exception.raise
        (Failure
          (typeLink SerializationError)
          "Error decoding FailureReply message"
          (Any ()))
    Right v -> v

internal.CloudResponse.JobStarted.decode : Bytes -> CloudResponse
internal.CloudResponse.JobStarted.decode bs =
  JobStarted (JobId (up.Bytes.toBase16.text bs))

internal.CloudResponse.TaskResult.decode :
  Bytes ->{IO, Exception} CloudResponse
internal.CloudResponse.TaskResult.decode bs = TaskResult (decodeResult bs)

internal.CloudResponse.TermRequest.decode :
  Bytes ->{IO, Exception} CloudResponse
internal.CloudResponse.TermRequest.decode bs = TermRequest (decodeResult bs)

internal.connect : Cloud.ClientConfig ->{IO, Exception} Connection
internal.connect config =
  use Text ++
  host = ClientConfig.host config
  port = tcpPort config
  tlsConfig = ClientConfig.tlsConfig config
  match client.Config.proxy (httpConfig config) with
    None ->
      match tlsConfig with
        None -> Connection.client host port
        Some tlsConfig ->
          sock = Socket.client host port
          tls =
            onException (do Socket.close sock) do
              Tls.handshake (newClient tlsConfig sock)
          send = TlsSocket.send tls
          receive = do TlsSocket.receive tls
          close =
            do
              finally (do Socket.close sock) do
                unison_base_3_22_0.ignore (catch do TlsSocket.terminate tls)
          Connection send receive close
    Some proxyCfg ->
      match connectViaProxy.impl proxyCfg tlsConfig host port Headers.empty with
        Right r -> r
        Left connectResponse ->
          msg =
            "Failed to connect to Unison Cloud through the configured proxy. The proxy returned an HTTP failure with error code "
              ++ Status.toText (HttpResponse.status connectResponse)
          e =
            Failure
              (typeLink UnexpectedResponseStatus)
              msg
              (Any (connectResponse : HttpResponse))
          Exception.raise e

internal.credentials.accessTokenFromCredentialsFile :
  HostName ->{IO, Exception} AccessToken
internal.credentials.accessTokenFromCredentialsFile host =
  use HostName toText
  use Text ++
  decoder =
    do
      use Decoder text
      use object at at!
      activeProfile = at! "active_profile" text
      hostName = toText host
      token =
        at!
          "credentials"
          (at
            activeProfile (at hostName (at "tokens" (at "access_token" text))))
      AccessToken token
  out = do
    use up.FilePath /
    path = dataDirectory() / "unisonlanguage" / "credentials.json"
    readFile path |> fromUtf8 |> Decoder.run decoder
  userFriendlyMsg =
    "Error reading credentials for '"
      ++ toText host
      ++ "'. You may need to run the ucm `auth.login` command."
  catch out
    |> Either.mapLeft
         (e ->
           Failure
             (typeLink CredentialsError)
             userFriendlyMsg
             (Any (Failure.message e)))
    |> Either.toException

internal.credentials.addAuthHeader : AccessToken -> HttpRequest -> HttpRequest
internal.credentials.addAuthHeader = cases
  AccessToken accessToken ->
    headers.modify
      (Headers.setHeader "Authorization" ("Bearer " Text.++ accessToken))

internal.decodeResult : Bytes ->{IO, Exception} a
internal.decodeResult bs =
  match Value.deserialize.impl bs with
    Left e ->
      Exception.raise (Failure (typeLink SerializationError) e (Any ()))
    Right v ->
      match load v with
        Left e ->
          Exception.raise
            (Failure
              (typeLink SerializationError)
              "A Unison Cloud response contained terms/types unknown to the Unison client. This shouldn't happen, since the client initiated the request."
              (Any e))
        Right result -> result

internal.docs.blobListExample : Doc
internal.docs.blobListExample =
  {{
  If a database has the following blob keys:

  * `books/unison_in_the_enterprise.epub`
  * `documents/notes.txt`
  * `documents/photos/cat.jpg`
  * `documents/photos/dog.gif`
  * `documents/taxes_2024.pdf`

  Then calling `list` with a prefix of `documents/` will return the following
  results:

  * A {BlobResult} with the {type Metadata} for `documents/notes.txt`
  * A `` PrefixResult "photos/" `` representing the __common prefix__ (often
    thought of as a __directory__) shared by multiple keys (
    `documents/photos/cat.jpg` and `documents/photos/dog.gif` )
  * A {BlobResult} with the {type Metadata} for `documents/taxes_2024.pdf`

  The result will not contain an entry for
  `books/unison_in_the_enterprise.epub`, because it does not match the prefix
  `documents/`.

  The result will not contain more than `maxResults` entries. If the query
  produced more than `maxResults` results, then the returned {type PageToken}
  can be passed into a subsequent call to paginate through more results.
  }}

internal.docs.blobNamespaces : Doc
internal.docs.blobNamespaces =
  {{
  {type Blobs} operations that act on raw {type Bytes} are in the `Blobs.bytes`
  namespace, and operations that act on typed Unison values are in the
  `Blobs.typed` namespace. These two namespaces are isolated from each other,
  which means:

  * You cannot write a value with {bytes.create} and then read it with
    {typed.read} (it will return {None}).
  * You cannot write a value with {typed.create} and then read it with
    {bytes.read} (it will return {None}).
  * A typed blob and a bytes blob can have the same {type Key} but are separate
    entities.
  * Results from {typed.prefixQuery} and {typed.list} will not include bytes
    blobs.
  * Results from {bytes.prefixQuery} and {bytes.list} will not include typed
    blobs.
  }}

internal.docs.blobPrefixQueryExample : Doc
internal.docs.blobPrefixQueryExample =
  {{
  If a database has the following blob keys:

  * `books/unison_in_the_enterprise.epub`
  * `documents/photos/cat.jpg`
  * `documents/taxes/2024.pdf`
  * `documents/todo.txt`

  Then calling `prefixQuery` with a prefix of `documents/t` will return the
  {type Metadata} for `documents/taxes/2024.pdf` and `documents/todo.txt`. It
  will not return results for the other blobs, because they do not start with
  the string `documents/t`.

  The result will not contain more than `maxResults` entries. If the query
  produced more than `maxResults` results, then the returned {type PageToken}
  can be passed into a subsequent call to paginate through more results.
  }}

internal.expectApiSuccess : Text -> HttpResponse ->{Exception} ()
internal.expectApiSuccess context response =
  use HttpResponse status
  use List ++
  if HttpResponse.isSuccess response then ()
  else
    serverErrMsg =
      response
        |> HttpResponse.body
        |> Body.toBytes
        |> fromUtf8.impl
        |> Either.fold (do "") Function.id
    errMsg =
      Text.join
        ""
        ([ "The Unison Cloud API returned a failure status while "
        , context
        , "."
        , "\nHTTP status code: "
        , status response |> Status.toText
        ]
          ++ (if Text.isEmpty serverErrMsg then []
             else ["\nError description:\n", serverErrMsg])
          ++ (if (response |> status |> code) === 401 then
               [ "\nYou may need to run the ucm auth.login command to create or renew your authentication session."
               ]
             else []))
    Exception.raise
      (Failure
        (typeLink UnexpectedResponseStatus)
        errMsg
        (Any ((status response) : HttpResponse.Status)))

internal.json.Decoder.customDomainDetails : '{Decoder} (HostName, Text)
internal.json.Decoder.customDomainDetails =
  use Decoder text
  use object at!
  decodeHost = text >> HostName
  do (at! "domainName" decodeHost, at! "serviceName" text)

internal.json.Decoder.uriStripTrailingSlash : '{Decoder} URI
internal.json.Decoder.uriStripTrailingSlash = do
  use Char ==
  use Text ++
  uriText = Text.dropRightWhile (c -> c == ?/) Decoder.text()
  match parseOptional uriText with
    Some uri -> uri
    None     -> Decoder.fail ("Failed to decode URI : " ++ uriText)

internal.location.tags.default : [Text]
internal.location.tags.default =
  ["config", "http", "log", location.tag, "services", "state"]

internal.services.serialization.encodeService :
  (a ->{Remote} b) ->{IO, Exception, Stream Bytes} ()
internal.services.serialization.encodeService handleRequest =
  protocolVersion = 0
  encode.byte protocolVersion
  compressedBlob =
    do
      use encode nat32be variableSizeBytes
      handleRequestValue = Value.value handleRequest
      variableSizeBytes nat32be encode.value handleRequestValue
      variableSizeBytes
        nat32be encode.universal (dependenciesRecursive handleRequestValue)
  encode.compressed.zlib compressedBlob ()

internal.services.serialization.encodeService.doc : Doc
internal.services.serialization.encodeService.doc =
  {{
  Serializes a "service" (a function that can perform {type Remote} abilities)
  and all of its dependencies. The result can be stored and can later be
  deserialized to handle a service request.
  }}

internal.tests.cloud.all :
  Remote.Duration
  ->{IO, Exception, Cloud} [(Text, '{IO, Exception, Cloud, Random} ())]
internal.tests.cloud.all syncWaitTime =
  use List ++
  testEnv = Environment.named "test-env"
  standard syncWaitTime testEnv ++ premium testEnv

internal.tests.cloud.basicState :
  Remote.Duration ->{Exception, Cloud, Random} ()
internal.tests.cloud.basicState syncWaitTime =
  use Text ++
  runId = Nat.toText Random.nat!
  go env db =
    use Cloud submit
    use test ensureEqual
    table : Table Text Nat
    table = Table.Table "my-table"
    let
      (beforeOne, afterOne) =
        submit env do
          unison_base_3_22_0.ignore do
            {{
            Wait to ensure that the environment has gained access to the
            database.
            }}
          Remote.sleep syncWaitTime
          tx = do
            beforeOne = Transaction.tryRead.tx table "foo"
            Transaction.write.tx table "foo" 1
            afterOne = Transaction.tryRead.tx table "foo"
            Transaction.write.tx table "foo" 2
            (beforeOne, afterOne)
          reraise! do toRemote do transact db tx
      (afterTwo, afterThree, afterDeletion) =
        submit env do
          tx = do
            afterTwo = Transaction.tryRead.tx table "foo"
            Transaction.write.tx table "foo" 3
            afterThree = Transaction.tryRead.tx table "foo"
            Transaction.delete.tx table "foo"
            afterDeletion = Transaction.tryRead.tx table "foo"
            (afterTwo, afterThree, afterDeletion)
          reraise! do toRemote do transact db tx
      ensureEqual beforeOne None
      ensureEqual afterOne (Some 1)
      ensureEqual afterTwo (Some 2)
      ensureEqual afterThree (Some 3)
      ensureEqual afterDeletion None
  bracket
    (do Environment.named ("test-env-" ++ runId))
    Environment.delete
    (env ->
      bracket
        (do Database.named ("test-basic-state-" ++ runId))
        Database.delete
        (db ->
          bracket (do Database.assign db env) (do Database.unassign db env) do
            go env db))

internal.tests.cloud.blobs.crud :
  Remote.Duration ->{Exception, Cloud, Random} ()
internal.tests.cloud.blobs.crud syncWaitTime =
  use Cloud submit
  use Duration < abs
  use List map sort
  use Nat == toText
  use Random natIn
  use Remote fork
  use test ensureEqual raiseFailure
  use time.Duration seconds
  runId = Random.nat!
  isRestrictedOpFailure = cases
    Left (Failure failureType _ _) ->
      failureType === typeLink RestrictedOperation
    _ -> false
  testTypedApi db =
    use List ++ size
    use typed create delete list prefixQuery read
    test db key expectedETag content ts =
      (actualContent, actualMeta1) =
        getOrElse'
          (do raiseFailure "No object found for key" key) (read db key)
      ensureEqual content actualContent
      actualETag = etag actualMeta1
      ensureEqual expectedETag actualETag
      let
        actualMeta2 =
          getOrElse'
            (do raiseFailure "No metadata found" key)
            (typed.readMetadata db key)
        ensureEqual actualMeta1 actualMeta2
        mod = lastModified actualMeta2
        ensuring do abs (between ts mod) < seconds +60
      ensuring do isLeft (typed.tryCreate db key content)
    key1 = Key "level-1/level-2/foo"
    key2 = Key "level-1/bar"
    ensureEqual None (read db key1)
    let
      (key1ContentV1, key1ContentV2, key2Content) =
        randomly do
          g = do fill' (natIn 0 10) (Random.optional do intIn minInt maxInt)
          (g(), g(), g())
      finally (do delete db key1) do
        key1V1ETag = create db key1 key1ContentV1
        test db key1 key1V1ETag key1ContentV1 now!
        key1V2ETag = typed.write db key1 key1ContentV2
        test db key1 key1V2ETag key1ContentV2 now!
        key2ETag = create db key2 key2Content
        finally (do delete db key2) do
          ensureEqual
            (PrefixQueryResults None [])
            (prefixQuery db (Some 11) "not-a-thing" None)
          ensureEqual
            (PrefixQueryResults None [])
            (prefixQuery db (Some 11) "level-1/level-2/not-a-thing" None)
          let
            res = prefixQuery db (Some 11) "level-1/level-2/foo" None
            ensureEqual None (PrefixQueryResult.nextPageToken res)
            ensureEqual
              [(key1, key1V2ETag)]
              (blobs res |> map (meta -> (Metadata.key meta, etag meta)))
          let
            use PrefixQueryResult nextPageToken
            res1 = prefixQuery db (Some 1) "level-1/" None
            ensuring do isSome (nextPageToken res1)
            ensuring do size (blobs res1) == 1
            res2 = prefixQuery db (Some 1) "level-1/" (nextPageToken res1)
            ensuring do size (blobs res2) == 1
            results = blobs res1 ++ blobs res2
            ensureEqual
              [(key2, key2ETag), (key1, key1V2ETag)]
              (sort results |> map (meta -> (Metadata.key meta, etag meta)))
          res1 = list db (Some 1) "level-1/" None
          ensuring do isSome (PrefixListResults.nextPageToken res1)
          ensuring do size (PrefixListResults.results res1) == 1
          res2 =
            list db (Some 1) "level-1/" (PrefixListResults.nextPageToken res1)
          ensuring do size (PrefixListResults.results res2) == 1
          results =
            PrefixListResults.results res1 ++ PrefixListResults.results res2
          ensureEqual
            [Right (key2, key2ETag), Left "level-1/level-2/"]
            (sort results |> (map cases
              BlobResult meta -> Right (Metadata.key meta, etag meta)
              PrefixResult p  -> Left p))
      ensureEqual None (read db key1)
      ensureEqual None (read db key2)
  testBytesApi db =
    use List ++
    use Nat - >
    use Remote randomBytes
    use bytes create delete list prefixQuery read
    test db key expectedETag content ts =
      (actualContent, actualMeta1) =
        getOrElse'
          (do raiseFailure "No object found for key" key) (read db key)
      ensureEqual content actualContent
      actualETag = etag actualMeta1
      ensureEqual expectedETag actualETag
      let
        actualMeta2 =
          getOrElse'
            (do raiseFailure "No metadata found" key)
            (bytes.readMetadata db key)
        ensureEqual actualMeta1 actualMeta2
        mod = lastModified actualMeta2
        ensuring do abs (between ts mod) < seconds +60
      ensuring do isLeft (bytes.tryCreate db key content)
      randomly do
        contentSize = Bytes.size content
        if contentSize > 0 then
          start = natIn 0 contentSize
          end = natIn start contentSize
          expected =
            Bytes.drop start content
              |> Bytes.take (end - start |> Nat.increment)
          ensureEqual
            (Some (expected, expectedETag)) (readRange db key start end)
        else
          unison_base_3_22_0.ignore "You can't read a range from an empty blob"
    key1 = Key "level-1/level-2/foo"
    key2 = Key "level-1/bar"
    ensureEqual None (read db key1)
    key1ContentV1 = randomBytes (natIn 0 129)
    key1V1ETag = create db key1 key1ContentV1
    finally (do delete db key1) do
      test db key1 key1V1ETag key1ContentV1 now!
      key1ContentV2 = randomBytes (natIn 0 192)
      key1ContentV2Stream : '{Remote, Stream Bytes} ()
      key1ContentV2Stream =
        do randomly do splits.bytes (natIn 1 20) key1ContentV2 ()
      key1V2ETag = writeStreaming db key1 key1ContentV2Stream
      test db key1 key1V2ETag key1ContentV2 now!
      key2Content = randomBytes (natIn 0 43)
      key2ETag = create db key2 key2Content
      finally (do delete db key2) do
        ensureEqual
          (PrefixQueryResults None [])
          (prefixQuery db (Some 11) "not-a-thing" None)
        ensureEqual
          (PrefixQueryResults None [])
          (prefixQuery db (Some 11) "level-1/level-2/not-a-thing" None)
        let
          res = prefixQuery db (Some 11) "level-1/level-2/foo" None
          ensureEqual None (PrefixQueryResult.nextPageToken res)
          ensureEqual
            [(key1, key1V2ETag, Bytes.size key1ContentV2)]
            (blobs res
              |> map (meta -> (Metadata.key meta, etag meta, byteCount meta)))
        let
          use PrefixQueryResult nextPageToken
          res1 = prefixQuery db (Some 1) "level-1/" None
          ensuring do isSome (nextPageToken res1)
          ensuring do List.size (blobs res1) == 1
          res2 = prefixQuery db (Some 1) "level-1/" (nextPageToken res1)
          ensuring do List.size (blobs res2) == 1
          results = blobs res1 ++ blobs res2
          ensureEqual
            [ (key2, key2ETag, Bytes.size key2Content)
            , (key1, key1V2ETag, Bytes.size key1ContentV2)
            ]
            (sort results
              |> map (meta -> (Metadata.key meta, etag meta, byteCount meta)))
        res1 = list db (Some 1) "level-1/" None
        ensuring do isSome (PrefixListResults.nextPageToken res1)
        ensuring do List.size (PrefixListResults.results res1) == 1
        res2 =
          list db (Some 1) "level-1/" (PrefixListResults.nextPageToken res1)
        ensuring do List.size (PrefixListResults.results res2) == 1
        results =
          PrefixListResults.results res1 ++ PrefixListResults.results res2
        ensureEqual
          [ Right (key2, key2ETag, Bytes.size key2Content)
          , Left "level-1/level-2/"
          ]
          (sort results
            |> (map cases
              BlobResult meta ->
                Right (Metadata.key meta, etag meta, byteCount meta)
              PrefixResult p -> Left p))
    ensureEqual None (read db key1)
    ensureEqual None (read db key2)
  envName = "test-blobs-crud-" Text.++ toText runId
  dbName = "test-blobs-crud-" Text.++ toText runId
  bracket
    (do Database.named dbName) Database.delete (db ->
      bracket
        (do Environment.named envName) Environment.delete (env -> let
          let
            (bytesRes, typedRes) =
              submit env do
                k = Key "cannot-read-before-db-assignment"
                (bytes.tryRead db k, typed.tryRead db k)
            ensuring do isRestrictedOpFailure bytesRes
            ensuring do isRestrictedOpFailure typedRes
          Database.assign db env
          finally (do Database.unassign db env) do
            submit env do
              Remote.sleep syncWaitTime
              typed = fork pool() do testTypedApi db
              bytes = fork pool() do randomly do testBytesApi db
              await typed
              await bytes))

internal.tests.cloud.daemons.crud : Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.daemons.crud env =
  use Cloud submit
  use Daemon assign
  use Remote.Promise read write_
  use Text ++
  use create remote
  runId = Nat.toText Random.nat!
  rmPromises = cases
    (p1, p2, p3, p4, p5, p6) ->
      submit env do
        List.foreach
          (p -> finalizer do Promise.delete p) [p1, p2, p3, p4, p5, p6]
  bracket
    (do
    submit env do
      (detached!, detached!, detached!, detached!, detached!, detached!))
    rmPromises
    cases
    (v0Started, v0Ended, v1Started, v1Ended, v2Started, v2Ended) ->
      d0 = do
        finalizer do write_ v0Ended ()
        write_ v0Started ()
        sleepForever() |> absurd
      d1 = do
        finalizer do write_ v1Ended ()
        write_ v1Started ()
      d2 = do
        finalizer do write_ v2Ended ()
        write_ v2Started ()
        sleepForever() |> absurd
      bracket
        (do remote env d0)
        DaemonHash.delete
        (hash0 ->
          bracket
            (do remote env d1)
            DaemonHash.delete
            (hash1 ->
              bracket
                (do remote env d2)
                DaemonHash.delete
                (hash2 ->
                  bracket
                    (do Daemon.named ("daemon-crud-" ++ runId))
                    Daemon.delete
                    (daemon -> let
                      assign daemon hash0
                      submit env do
                        debug "waiting for v0 to start" []
                        read v0Started
                        debug "done waiting for v0 to start" []
                      assign daemon hash1
                      submit env do
                        debug "waiting for v0 to end" []
                        read v0Ended
                        debug "done waiting for v0 to end" []
                        debug "waiting for v1 to start" []
                        read v1Started
                        debug "done waiting for v1 to start" []
                      assign daemon hash2
                      submit env do
                        debug "waiting for v1 to end" []
                        read v1Ended
                        debug "done waiting for v1 to end" []
                        debug "waiting for v2 to start" []
                        read v2Started
                        debug "done waiting for v2 to start" []
                      Daemon.unassign daemon
                      submit env do
                        debug "waiting for v2 to end" []
                        read v2Ended
                        debug "done waiting for v2 to end" []))))

internal.tests.cloud.database.crud :
  Remote.Duration ->{Exception, Cloud, Random} ()
internal.tests.cloud.database.crud syncWaitTime =
  use Nat toText
  use Remote sleep
  use Storage tryRead
  use Text ++
  runId = Random.nat!
  envName = "test-env-crud-" ++ toText runId
  dbName = "test-db-crud-" ++ toText runId
  table : Table Text Nat
  table = Table.Table "test"
  bracket
    (do Database.named dbName) Database.delete (db ->
      bracket
        (do Environment.named envName) Environment.delete (env -> let
          Database.assign db env
          res = Cloud.submit env do
            sleep syncWaitTime
            tryRead db table "foo"
          test.ensureEqual res None
          Database.unassign db env
          res2 = submit.impl env do
            toRemote do
              sleep syncWaitTime
              tryRead db table "foo"
          ensuring do match res2 with
            Left (Failure t _ _) | t === typeLink RestrictedOperation -> true
            _                    -> false))

internal.tests.cloud.deploy.crud :
  Remote.Duration -> Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.deploy.crud syncWaitTime env =
  use Nat +
  use Services call
  magicNumber = Random.nat!
  addMagicNumber n = magicNumber + n
  serviceHash =
    Cloud.deploy
      env (n -> let
        info "this log line tests other abilities" []
        addMagicNumber n)
  res = Cloud.submit env do toRemote do call serviceHash 3
  test.ensureEqual res (addMagicNumber 3)
  undeploy serviceHash
  resAfterUndeploy = submit.impl env do
    Remote.sleep syncWaitTime
    toRemote do call serviceHash 3
  match resAfterUndeploy with
    Left (Failure errType _ _)| errType === typeLink UnknownService  -> ()
    _ ->
      test.raiseFailure
        "expected UnknownService failure"
        [ ("serviceHash", Any (serviceHash : ServiceHash Nat Nat))
        , ("resAfterUndeploy", Any (resAfterUndeploy : Either Failure Nat))
        ]

internal.tests.cloud.deployAndCall :
  Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.deployAndCall env =
  use Remote fork
  runId = Nat.toText Random.nat!
  parMapReduce : b -> (a -> b) -> (b -> b -> b) -> [a] ->{Remote} b
  parMapReduce zero f op = cases
    []     -> zero
    [a]    -> f a
    [l, r] -> op (f l) (f r)
    as     ->
      (l, r) = halve as
      lr = fork here! do parMapReduce zero f op l
      rr = fork here! do parMapReduce zero f op r
      op (await lr) (await rr)
  sumByKey : [(Text, Nat)] ->{Remote} data.Map Text Nat
  sumByKey input =
    use Nat +
    use Text ++
    unison_base_3_22_0.ignore
      ("Giving this service a unique hash for this run: " ++ runId)
    input
      |> parMapReduce
        data.Map.empty
        (cases (key, value) -> Map.singleton key value)
        (Map.unionWith (+))
  testService serviceHash =
    use Map ==
    res =
      Cloud.submit env do
        toRemote do
          Services.call
            serviceHash [("one", 1), ("two", 1), ("three", 3), ("two", 1)]
    ensuring do res == Map.fromList [("one", 1), ("two", 2), ("three", 3)]
  bracket (do Cloud.deploy env sumByKey) undeploy testService

internal.tests.cloud.deployWS.crud :
  Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.deployWS.crud env =
  use Bytes isEmpty
  use Exception raise
  use HttpRequest post
  use Nat +
  use Remote fail
  use Text ++
  use URI /
  use WebSockets close receive send
  use abilities repeat
  use flipped deprecated
  use pool wrap
  use test ensureEqual
  echoMessageCount = 40
  sessionMessages = [0, 1, 2, 42, maxNat]
  magicNumber = Random.nat!
  decodeNatMsg : Message ->{Remote} Nat
  decodeNatMsg = cases
    BinaryMessage bs ->
      match decodeNat64be bs with
        Some (n, remainder) ->
          if isEmpty remainder then n
          else failure "got more than a 64 bit nat back" bs |> fail
        None -> failure "expected binary message but got none" () |> fail
    o -> failure "expected binary message" o |> fail
  websocketEchoService = cases
    HttpRequest GET _ (URI _ _ (Path ["magic-number"]) _ _) _ _ ->
      encodeNat64be magicNumber |> Body |> HttpResponse.ok |> Left
    HttpRequest POST _ (URI _ _ (Path ["session"]) _ _) _ _ ->
      Right
        (webSocket ->
          reraise! do
            addFinalizer (wrap do close webSocket)
            send webSocket (TextMessage "What is your name?")
            match receive webSocket with
              TextMessage name ->
                send webSocket (TextMessage ("Hello, " ++ name))
                repeat (List.size sessionMessages) do
                  n = decodeNatMsg (receive webSocket)
                  response = n + magicNumber |> encodeNat64be |> BinaryMessage
                  send webSocket response
              o                -> failure "unexpected message" o |> fail)
    HttpRequest POST _ (URI _ _ (Path ["echo-session"]) _ _) _ _ ->
      Right
        (ws -> let
          addFinalizer (wrap do close ws)
          toRemote do repeat echoMessageCount do send ws (receive ws))
    o ->
      body = toDebugText o |> Text.toUtf8 |> Body
      Left
        (HttpResponse
          (Status 404 "Not Found") Version.http11 Headers.empty body)
  testService serviceUri =
    Cloud.submit env do
      match Http.get (serviceUri / "magic-number")
        |> Function.tap expectSuccess
        |> HttpResponse.body
        |> Body.toBytes
        |> decodeNat64be with
        Some (n, remainder) ->
          use Body empty
          if isEmpty remainder then ()
          else raise (failure "expected empty remainder" remainder)
          ensureEqual n magicNumber
          bracket
            (do
              websockets.HttpWebSocket.webSocket
                (post (serviceUri / "session") empty))
            close
            (ws ->
              (match receive ws with
                TextMessage "What is your name?" ->
                  send ws (TextMessage "Grace")
                  match receive ws with
                    TextMessage "Hello, Grace" ->
                      Each.run do
                        n = each sessionMessages
                        send ws (BinaryMessage (encodeNat64be n))
                        res = decodeNatMsg (receive ws)
                        ensureEqual res (magicNumber + n)
                    o -> failure "expected greeting" o |> raise
                o -> failure "expected text response" o |> raise))
          bracket
            (do
              websockets.HttpWebSocket.webSocket
                (post (serviceUri / "echo-session") empty))
            close
            (ws ->
              let
                msgs =
                  initialize
                    echoMessageCount
                    (i -> TextMessage ("This is message " ++ Nat.toText i))
                deprecated msgs (send ws)
                deprecated msgs (msg -> ensureEqual msg (receive ws)))
        o -> failure "unexpected response" o |> raise
  bracket
    (do Cloud.deploy env websocketEchoService)
    undeploy
    (serviceHash ->
      bracket
        (do exposeHttpWebSocket serviceHash)
        (_ -> unexposeHttpWebSocket.impl serviceHash |> Either.toException)
        testService)

internal.tests.cloud.durableConstructorsHashCheck : '{IO, Exception} [Result]
internal.tests.cloud.durableConstructorsHashCheck =
  do
    _ = Clock.monotonic()
    record =
      """
        Some durable types follow a pattern where the hash of their own
        constructor function is used to uniquely distinguish their Table
        name from others. This was a mistake, as now if the hash of those
        function changes, you can no longer refer to them in storage.
        This test checks that those hashes don't change.
        It calls monotonic() because we don't want this test to be cached,
        it needs to run even if Unison internals change but this codebase
        doesn't.

        As of: ⊙ #gddpimgsuq

        Now evaluating any watch expressions (lines starting with `>`)... Ctrl+C cancels.

        1 | > blake2b_256 durable.OrderedTable.named |> toBase32Hex.text
              ⧩
              "9dtrm5sdb0sh61bvu2fooo1n2f4hke0igednbd3eu1e8h6vg0a2g===="

        2 | > blake2b_256 durable.Cell.nested |> toBase32Hex.text
              ⧩
              "41fdqe7ambnb8nmkoohk7gfq3lhl7uh0dl1p5n8a71l1pchd5060===="

        3 | > blake2b_256 durable.Immutable.nested |> toBase32Hex.text
              ⧩
              "t7b8phrr95tjmabal5cim37iqmpm6vdue6kb5ohhsaoo5rrt0qog===="

        4 | > blake2b_256 durable.Knn.nested |> toBase32Hex.text
              ⧩
              "dnaol3oo9r025918idnn814phit5h69eo2o79hislgjdteukpq9g===="

        5 | > blake2b_256 durable.LinearLog.nested |> toBase32Hex.text
              ⧩
              "3c3du8mmg9alvnuq58ri2vfb7dhrh6gb5lfeu8nmmq1onvd9ekmg===="

        6 | > blake2b_256 durable.OrderedTable.nested |> toBase32Hex.text
              ⧩
              "8j604btf4sb62040uiog1ub0u0kpe69voihbph63m671qdtp9li0===="
      """
    checkHash name fn hash =
      use Text ++
      hash' = blake2b_256 fn |> up.Bytes.toBase32Hex.text
      if hash === hash' then Ok ("Hash of " ++ name ++ " unchanged")
      else Result.Fail ("Hash of " ++ name ++ " changed")
    [ checkHash
        "OrderedTable.named"
        OrderedTable.named
        "9dtrm5sdb0sh61bvu2fooo1n2f4hke0igednbd3eu1e8h6vg0a2g===="
    , checkHash
        "Cell.nested"
        Cell.nested
        "41fdqe7ambnb8nmkoohk7gfq3lhl7uh0dl1p5n8a71l1pchd5060===="
    , checkHash
        "Immutable.nested"
        Immutable.nested
        "t7b8phrr95tjmabal5cim37iqmpm6vdue6kb5ohhsaoo5rrt0qog===="
    , checkHash
        "Knn.nested"
        Knn.nested
        "dnaol3oo9r025918idnn814phit5h69eo2o79hislgjdteukpq9g===="
    , checkHash
        "LinearLog.nested"
        LinearLog.nested
        "3c3du8mmg9alvnuq58ri2vfb7dhrh6gb5lfeu8nmmq1onvd9ekmg===="
    , checkHash
        "OrderedTable.nested"
        OrderedTable.nested
        "8j604btf4sb62040uiog1ub0u0kpe69voihbph63m671qdtp9li0===="
    ]

internal.tests.cloud.environment.crud :
  Remote.Duration ->{Exception, Cloud, Random} ()
internal.tests.cloud.environment.crud syncWaitTime =
  use Cloud submit
  use Config lookup
  use Environment list
  use Remote sleep
  use Text ++ ==
  use test ensureEqual
  envName = "test-env-crud-" ++ Nat.toText Random.nat!
  bracket
    (do Environment.named envName) Environment.delete (env -> let
      ensuring do List.contains env list()
      key = "my-key"
      value = "my-value"
      bracket
        (do setValue env key value) (_ -> deleteValue env key) (_ -> let
          returnedValue = submit env do
            sleep syncWaitTime
            toRemote do lookup key
          ensureEqual (Some value) returnedValue)
      returnedValue = submit env do
        sleep syncWaitTime
        toRemote do lookup key
      ensureEqual None returnedValue)
  ensuring do List.none (Environment.name >> (==) envName) list()

internal.tests.cloud.exposeAndCall :
  Remote.Duration -> Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.exposeAndCall syncWaitTime env =
  use Cloud deploy
  use HttpResponse addHeader
  use Remote.Duration zero
  use Text ++ toUtf8
  use Version http11
  runId = Nat.toText Random.nat!
  handleReq serviceVersion = cases
    HttpRequest
      POST _ (URI _ _ (Path ["test", pathParam]) query _) headers body ->
      match getValues "Content-Type" headers with
        [contentType] ->
          response =
            forkAt region! do
              HttpResponse.ok body
                |> addHeader "Path-Param" pathParam
                |> addHeader "Query-Param" (RawQuery.encode query)
                |> addHeader "Content-Type" contentType
                |> addHeader "Run-Id" runId
                |> addHeader "Service-Version" serviceVersion
                |> addHeader "Base-Path" (basePath headers |> Path.toText)
          await response
        other ->
          errMsg =
            "expected exactly one Content-Type header, got "
              ++ Text.join "," other
          HttpResponse
            (Status 400 "Bad Request")
            http11
            Headers.empty
            (errMsg |> toUtf8 |> Body)
    _ -> HttpResponse.notFound
  testServiceAtUri expectedVersion expectedBasePath baseUri =
    use test ensureEqual
    response =
      use URI /
      headers = Headers.singleton "Content-Type" "text/plain"
      uri =
        (URI scheme authority path _ fragment) =
          baseUri / "test" / "my-path-param"
        query = RawQuery "foo=bar&baz=quux"
        URI scheme authority path query fragment
      req =
        HttpRequest POST http11 uri headers ("hello, there" |> toUtf8 |> Body)
      Cloud.submit env do request req
    expectSuccess response
    responseHeaders = HttpResponse.headers response
    ensureEqual ["my-path-param"] (getValues "Path-Param" responseHeaders)
    ensureEqual ["?foo=bar&baz=quux"] (getValues "Query-Param" responseHeaders)
    ensureEqual
      "hello, there"
      (response |> HttpResponse.body |> Body.toBytes |> fromUtf8)
    ensureEqual ["text/plain"] (getValues "Content-Type" responseHeaders)
    ensureEqual [expectedBasePath] (getValues "Base-Path" responseHeaders)
    ensureEqual [expectedVersion] (getValues "Service-Version" responseHeaders)
  testService
    serviceName
    expectedServiceVersion
    delayAfterAssignment
    serviceHash
    hashBasedUri =
    testServiceAtUri
      expectedServiceVersion
      ("/h/" ++ ServiceHash.toText serviceHash)
      hashBasedUri
    uri = ServiceName.assign serviceName serviceHash
    if Universal.gt delayAfterAssignment zero then
      submit.remote env do Remote.sleep delayAfterAssignment
    else ()
    testServiceAtUri
      expectedServiceVersion ("/s/" ++ ServiceName.name serviceName) uri
  bracket
    (do ServiceName.named ("test-service-" ++ runId))
    ServiceName.delete
    (serviceName ->
      bracket
        (do deploy env (handleReq "1.0"))
        undeploy
        (serviceHashV1 ->
          bracket
            (do exposeHttp serviceHashV1)
            (do unexposeHttp serviceHashV1)
            (v1HashUri ->
              let
                testService serviceName "1.0" zero serviceHashV1 v1HashUri
                bracket
                  (do deploy env (handleReq "2.0"))
                  undeploy
                  (serviceHashV2 ->
                    bracket
                      (do exposeHttp serviceHashV2)
                      (do unexposeHttp serviceHashV2)
                      (v2HashUri ->
                        testService
                          serviceName
                          "2.0"
                          syncWaitTime
                          serviceHashV2
                          v2HashUri)))))

internal.tests.cloud.premium :
  Environment -> [(Text, '{IO, Exception, Cloud, Random} ())]
internal.tests.cloud.premium testEnv =
  [("daemon CRUD", do daemons.crud testEnv)]

internal.tests.cloud.runAll : '{IO, Exception} [Result]
internal.tests.cloud.runAll = do
  syncWaitTime = Remote.Duration.seconds 5
  runTestSuite (Cloud.run do cloud.all syncWaitTime)

internal.tests.cloud.runAllLocally : '{IO, Exception} [Result]
internal.tests.cloud.runAllLocally =
  do
    syncWaitTime = micros 0
    Cloud.run.local do
      Random.run do
        List.map
          (cases (name, test) -> runLabeled name test) (cloud.all syncWaitTime)

internal.tests.cloud.runStandard : Remote.Duration ->{IO, Exception} [Result]
internal.tests.cloud.runStandard syncWaitTime =
  testEnv = Cloud.run do Environment.named "test-env"
  runTestSuite (standard syncWaitTime testEnv)

internal.tests.cloud.runTestSuite :
  [(Text, '{IO, Exception, Cloud, Random} ())] ->{IO, Exception} [Result]
internal.tests.cloud.runTestSuite tests = Random.run do
  rng = io()
  run : (Text, '{IO, Exception, Cloud, Random} ()) ->{IO} Result
  run = cases (name, test) -> rng do runLabeled name do Cloud.run test
  List.parMap run tests

internal.tests.cloud.scratchOps : Environment ->{Exception, Cloud} ()
internal.tests.cloud.scratchOps env =
  use test ensureEqual
  program =
    do
      hashKey : Hashed Nat
      hashKey = Hashed (Hash (Remote.randomBytes 16))
      a = lookupHashed hashKey
      _ = saveHashed hashKey 0
      b = lookupHashed hashKey
      _ =
        saveHashed
          hashKey
          (Optional.fold
            (do
              test.raiseFailure
                "Expected an existing value in Scratch lookup" hashKey)
            Nat.increment
            b)
      c = lookupHashed hashKey
      (a, b, c)
  let
    (a, b, c) = Cloud.submit env program
    ensureEqual None a
    ensureEqual (Some 0) b
    ensureEqual (Some 1) c

internal.tests.cloud.serviceName.assignViaId :
  Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.serviceName.assignViaId env =
  use Text ++
  runId = Random.nat!
  logic _ = runId
  name = "test-service-" ++ Nat.toText runId
  bracket (do Cloud.deploy env logic) undeploy cases
    serviceHash ->
      bracket (do ServiceName.named name) ServiceName.delete cases
        serviceName ->
          serviceNoName = ServiceName (ServiceName.id serviceName) ""
          _ = ServiceName.assign serviceNoName serviceHash
          Cloud.submit env do
            res = callName serviceNoName ()
            test.ensureEqual res runId

internal.tests.cloud.serviceName.crud :
  Remote.Duration -> Environment ->{Exception, Cloud, Random} ()
internal.tests.cloud.serviceName.crud syncWaitTime env =
  use Remote sleep
  use Text ++
  use submit impl
  serviceHash = Cloud.deploy env Nat.increment
  testService service =
    res = Cloud.submit env do
      sleep syncWaitTime
      toRemote do callName service 3
    test.ensureEqual res 4
  service = ServiceName.named ("test-service-" ++ Nat.toText Random.nat!)
  bracket
    (do ServiceName.assign service serviceHash)
    (do ServiceName.unassign service)
    do testService service
  resAfterUnassign = impl env do
    sleep syncWaitTime
    toRemote do callName service 3
  ensuring do match resAfterUnassign with
    Left (Failure errType _ _) -> errType === typeLink UnknownService
    _                          -> false
  ServiceName.delete service
  resAfterDelete = impl env do
    sleep syncWaitTime
    toRemote do callName service 3
  ensuring do match resAfterDelete with
    Left (Failure errType _ _) -> errType === typeLink UnknownService
    _                          -> false

internal.tests.cloud.standard :
  Remote.Duration
  -> Environment
  ->{IO, Exception} [(Text, '{IO, Exception, Cloud, Random} ())]
internal.tests.cloud.standard syncWaitTime testEnv =
  use List ++
  submitRemoteTest :
    assertions.Test a -> (Text, '{IO, Exception, Cloud, Random} ())
  submitRemoteTest = cases
    assertions.Test.Test label program check ->
      (label, do check (catch do Cloud.submit testEnv program))
  remoteTests =
    List.map (Exists.apply submitRemoteTest) (assertions.pure ++ impure())
  remoteTests
    ++ [ ("basic submit", do cloud.submit testEnv)
    , ("blobs CRUD", do blobs.crud syncWaitTime)
    , ("environment CRUD", do environment.crud syncWaitTime)
    , ("deployment CRUD", do deploy.crud syncWaitTime testEnv)
    , ("Scratch operations", do scratchOps testEnv)
    , ("service name CRUD", do serviceName.crud syncWaitTime testEnv)
    , ("service name assign via Id", do assignViaId testEnv)
    , ("deploy service", do deployAndCall testEnv)
    , ("deploy websocket service", do deployWS.crud testEnv)
    , ("expose service", do exposeAndCall syncWaitTime testEnv)
    , ("database crud", do database.crud syncWaitTime)
    , ("basic State", do basicState syncWaitTime)
    , ("state via service calls", do stateViaService syncWaitTime)
    ]

internal.tests.cloud.stateViaService :
  Remote.Duration ->{Exception, Cloud, Random} ()
internal.tests.cloud.stateViaService dbEnvSyncWaitTime =
  runId = Nat.toText Random.nat!
  mkName n = Text.join "-" [n, runId]
  withEnv name f =
    bracket (do Environment.named (mkName name)) Environment.delete f
  withDb name f = bracket (do Database.named (mkName name)) Database.delete f
  withAssign db env f =
    bracket (do Database.assign db env) (_ -> Database.unassign db env) f
  withDeploy env logic f = bracket (do Cloud.deploy env logic) undeploy f
  _ =
    """
    We are testing that the interpreter runs a service call in the
    correct environment, even when the service logic contains forks
    """
  logic db _ = await (forkAt pool() do transact db do ())
  r =
    withEnv
      "test-env1" (env1 ->
        withDb
          "test-db" (db ->
            withAssign db env1 do
              withEnv
                "test-env2" (env2 ->
                  withDeploy
                    env1 (logic db) (hash ->
                      Cloud.submit env2 do
                        Remote.sleep dbEnvSyncWaitTime
                        tryCall hash ()))))
  test.ensureEqual r Right()

internal.tests.cloud.submit : Environment ->{Exception, Cloud} ()
internal.tests.cloud.submit env =
  res = Cloud.submit env calculate7
  test.ensureEqual res 7

internal.Thunk.create : '{Http, Remote, Scratch} a -> Thunk
internal.Thunk.create thunk =
  bs = Value.serialize (Value.value thunk)
  Thunk bs

internal.up.base.reflection.Link.Term.isBuiltin : Link.Term -> Boolean
internal.up.base.reflection.Link.Term.isBuiltin =
  Term.toText >> startsWith "##"

internal.up.base.reflection.Link.Term.isBuiltin.doc : Doc
internal.up.base.reflection.Link.Term.isBuiltin.doc =
  {{
  Returns `true` if the provided term is a builtin term provided by the Unison
  runtime, and `false` if it is a user/library-defined term.

  See [this GitHub issue](https://github.com/unisonweb/unison/issues/3855) for
  discussion.
  }}

internal.up.base.Set.isEmpty : Set k -> Boolean
internal.up.base.Set.isEmpty = cases internal.Set m -> Map.isEmpty m

internal.up.base.Value.dependenciesRecursive :
  reflection.Value ->{IO, Exception} data.Map Link.Term Code
internal.up.base.Value.dependenciesRecursive value =
  use Link Term
  use List ++
  use data Map
  go : Map Term Code -> [Term] -> Map Term Code
  go seen = cases
    [] -> seen
    term +: rest ->
      if Map.contains term seen
        || isBuiltin term then go seen rest
      else
        match Code.lookup term with
          Some code ->
            go (Map.insert term code seen) (Code.dependencies code ++ rest)
          None ->
            Exception.raise
              (Failure
                (typeLink UnknownTerm) "missing code for term" (Any term))
  go data.Map.empty (Value.dependencies value)

internal.up.base.Value.dependenciesRecursive.doc : Doc
internal.up.base.Value.dependenciesRecursive.doc =
  {{
  Similar to {Value.dependencies}, but recursively finds transitive
  dependencies and their {type Code}.

  {{
  docCallout
    (Some {{ ℹ️ }})
    {{
    The map of dependencies returned will not include builtin terms, since they
    are assumed to be available in the runtime environment.
    }} }}
  }}

(internal.up.FilePath./) : FilePath -> Text -> FilePath
parentPath internal.up.FilePath./ child =
  use Text ++
  (FilePath parentText) = parentPath
  FilePath (parentText ++ "/" ++ child)

internal.up.http.Headers.setHeader : Text -> Text -> Headers -> Headers
internal.up.http.Headers.setHeader name value = cases
  Headers headerMap -> Headers (Map.insert name [value] headerMap)

internal.up.http.Headers.setHeader.doc : Doc
internal.up.http.Headers.setHeader.doc =
  {{ Set a header value, replacing any previous value for that header. }}

internal.up.http.HttpRequest.addParam.doc : Doc
internal.up.http.HttpRequest.addParam.doc =
  {{ Add a query parameter to an {type HttpRequest}. }}

(internal.up.http.URI./) : URI -> Text -> URI
uri internal.up.http.URI./ pathSegment =
  use Path /
  path.modify (p -> p / pathSegment) uri

internal.up.http.URI.slash.doc : Doc
internal.up.http.URI.slash.doc = {{ Append a path segment to a {type URI}. }}

internal.XDG.dataDirectory : '{IO, Exception} FilePath
internal.XDG.dataDirectory =
  do
    use Function id
    use Text ++
    use up.FilePath /
    dataDirs2 =
      [ ("XDG_DATA_HOME", id)
      , ("APPDATA", id)
      , ("HOME", home -> home / ".local" / "share")
      ]
    getDataDir = cases
      (envVar, f) ->
        getEnv.impl envVar |> Either.right |> Optional.map (FilePath >> f)
    findMap getDataDir dataDirs2
      |> (getOrElse' do
        Exception.raise
          (Failure
            (typeLink MissingEnvVar)
            ("Could not locate data directory. Expected one of the following environment variables to be set: "
              ++ Text.join ", " (List.map at1 dataDirs2))
            (Any ())))

JobId.doc : Doc
JobId.doc =
  {{
  A {type JobId} is a unique identifier for a job that is running on the cloud.
  }}

JobId.toText : JobId -> Text
JobId.toText = cases JobId txt -> txt

JobId.toText.doc : Doc
JobId.toText.doc =
  {{ Converts a {type JobId} to its {type Text} representation. }}

Log.atLevel.lazyJson : Level -> 'Text -> 'Json ->{Log} ()
Log.atLevel.lazyJson level message event =
  event' = do
    use Json text
    use List ++
    msg = message()
    match event() with
      Object obj -> Object (obj ++ [toJson level, ("message", text msg)])
      e          -> Object [toJson level, ("message", text msg), ("data", e)]
  match level with
    Debug -> debugLazyJson event'
    _     -> Log.lazyJson event'

Log.customLevel : Text -> Text -> [(Text, Text)] ->{Log} ()
Log.customLevel lvl msg attrs = lazy lvl (do msg) do attrs

Log.customLevel.json : Text -> Text -> Json ->{Log} ()
Log.customLevel.json lvl msg json = customLevel.lazyJson lvl (do msg) do json

Log.customLevel.lazy : Text -> 'Text -> '[(Text, Text)] ->{Log} ()
Log.customLevel.lazy lvl msg attrs =
  customLevel.lazyJson lvl msg do
    Json.object (List.map (Tuple.mapRight Json.text) attrs())

Log.customLevel.lazyJson : Text -> 'Text -> 'Json ->{Log} ()
Log.customLevel.lazyJson lvl = atLevel.lazyJson (Custom lvl)

Log.debug : Text -> [(Text, Text)] ->{Log} ()
Log.debug message data =
  debug.json message (Json.object (List.map (Tuple.mapRight Json.text) data))

Log.debug.doc : Doc
Log.debug.doc =
  {{
  Log a `Debug` {type Level} message with simple data in a key/value format.

  Ex:

  ``` unison
  Log.debug
    "Benchmark salesSummary"
    [("took", "204ms"), ("numberOfSales", "43512")]
  ```

  Or without any data:

  ``` unison
  Log.debug "Successfully Parsed User" []
  ```

  Using the `Debug` {type Level} will show up with a small debug icon in the
  Log UI be colored accordingly.
  }}

Log.debug.json : Text -> Json ->{Log} ()
Log.debug.json message json = debug.lazyJson (do message) do json

Log.debug.json.doc : Doc
Log.debug.json.doc =
  {{
  Log a `Debug` {type Level} message with {type Json} data.

  Ex:

  ``` unison
  Log.debug.json
    "Benchmark salesSummary"
    Json.object [("took", Json.text "204ms"), ("numberOfSales", Json.nat 43512)]
  ```

  Or without any data:

  ``` unison
  Log.debug.json "Deprecation Warning" Json.null
  ```

  Using the `Debug` {type Level} will show up with a small debug icon in the
  Log UI be colored accordingly.
  }}

Log.debug.lazyJson : 'Text -> 'Json ->{Log} ()
Log.debug.lazyJson message json = atLevel.lazyJson Debug message json

Log.debug.lazyJson.doc : Doc
Log.debug.lazyJson.doc =
  {{
  Similar to {debug.json}, but the data is wrapped in thunks. This is useful in
  avoiding a performance hit when encoding {type Json}.
  }}

Log.doc : Doc
Log.doc =
  use logs service
  {{
  Logging ability. Log messages can be arbitrary {type Json}, using the low
  level functions {{ docLink (docEmbedTermLink do Log.json) }} and
  {Log.lazyJson} but there are convenience functions for common cases. Unison
  Cloud's log viewer is set up to nicely render these. Here's a short example:

  {{ docExampleBlock 0 do
    info "beginning transmogrification..." []
    warn "operation failed, ignoring" [("name", "bob"), ("fruit", "🍍")] }}

  {{
  docCallout
    None
    {{
    Note that there's usually no need to manually add timestamps. Log entries
    get timestamped for you by Unison Cloud's handler of this ability, and
    these timestamps are shown in the log viewer.
    }} }}

  This example uses the convenience functions:

  {{
  docSignature
    [ docEmbedSignatureLink do info
    , docEmbedSignatureLink do warn
    , docEmbedSignatureLink do Log.error
    , docEmbedSignatureLink do debug
    ] }}

  These all take a message and a list of attributes which will be rendered as a
  table in the log viewer.

  You can also log arbitrary JSON at a given level using:

  {{
  docSignature
    [ docEmbedSignatureLink do info.json
    , docEmbedSignatureLink do warn.json
    , docEmbedSignatureLink do error.json
    , docEmbedSignatureLink do debug.json
    ] }}

  If your log messages are expensive to compute, you can use the lazy variants,
  such as {{ docSignatureInline (docEmbedSignatureLink do info.lazyJson) }} and
  so on (see {{ docLink (docEmbedTermLink do warn.lazyJson) }},
  {{ docLink (docEmbedTermLink do error.lazyJson) }},
  {{ docLink (docEmbedTermLink do debug.lazyJson) }}).

  # Need something custom?

    There's custom levels using an arbitrary {type Text} tag:

    {{
    docSignature
      [ docEmbedSignatureLink do customLevel
      , docEmbedSignatureLink do lazy
      , docEmbedSignatureLink do customLevel.json
      , docEmbedSignatureLink do customLevel.lazyJson
      ] }}

    And you can always drop down to using one of:

    {{
    docSignature
      [ docEmbedSignatureLink do Log.lazyJson
      , docEmbedSignatureLink do debug.lazyJson
      ] }}

  # Reading Logs

    You can read the logs through the Cloud UI, or programmatically using the
    {{ docLink (docEmbedTermLink do service) }} function. Here's an example {{
    docExampleBlock 0 do
      getJsonLogs =
        do
          serviceHash = ServiceName.named "my-service" |> resolve
          jsonLogs =
            service serviceHash (QueryOptions.default |> limit.set (Some 5))
          jsonLogs
      Cloud.run getJsonLogs }}
  }}

Log.error : Text -> [(Text, Text)] ->{Log} ()
Log.error message data =
  error.json message (Json.object (List.map (Tuple.mapRight Json.text) data))

Log.error.doc : Doc
Log.error.doc =
  {{
  Log an `Error` {type Level} message with simple data in a key/value format.

  Ex:

  ``` unison
  Log.error
    "Application Error"
    [("errorType", "ParseError")]
  ```

  Or without any data:

  ``` unison
  Log.error "Application Error" []
  ```

  Using the `Error` {type Level} will show up with a small error icon in the
  Log UI be colored accordingly.
  }}

Log.error.json : Text -> Json ->{Log} ()
Log.error.json message json = error.lazyJson (do message) do json

Log.error.json.doc : Doc
Log.error.json.doc =
  {{
  Log an `Error` {type Level} message with {type Json} data.

  Ex:

  ``` unison
  Log.error.json
    "Application Error"
    (Json.object [("errorType", Json.text "ParseError" )])
  ```

  Or without any data:

  ``` unison
  Log.error.json "Application Error" Json.null
  ```

  Using the `Error` {type Level} will show up with a small error icon in the
  Log UI be colored accordingly.
  }}

Log.error.lazyJson : 'Text -> 'Json ->{Log} ()
Log.error.lazyJson message json = atLevel.lazyJson Error message json

Log.error.lazyJson.doc : Doc
Log.error.lazyJson.doc =
  {{
  Similar to {error.json}, but the data is wrapped in thunks. This is useful in
  avoiding a performance hit when encoding {type Json}.
  }}

Log.ignore : '{g, Log} r ->{g} r
Log.ignore =
  go = cases
    { r }                    -> r
    { debugLazyJson _ -> k } -> handle k() with go
    { Log.lazyJson _ -> k }  -> handle k() with go
  thunk -> (handle thunk() with go)

Log.ignore.doc : Doc
Log.ignore.doc = {{ Ignore all logs generated by a delayed computation. }}

Log.info : Text -> [(Text, Text)] ->{Log} ()
Log.info message data =
  info.json message (Json.object (List.map (Tuple.mapRight Json.text) data))

Log.info.doc : Doc
Log.info.doc =
  {{
  Log an `Info` {type Level} message with simple data in a key/value format.

  Ex:

  ``` unison
  Log.info
    "GET /products"
    [("query", "croissant")]
  ```

  Or without any data:

  ``` unison
  Log.info "GET /products" []
  ```

  Using the `Info` {type Level} will show up with a small info icon in the Log
  UI.
  }}

Log.info.json : Text -> Json ->{Log} ()
Log.info.json message json = info.lazyJson (do message) do json

Log.info.json.doc : Doc
Log.info.json.doc =
  {{
  Log an `Info` {type Level} message with {type Json} data.

  Ex:

  ``` unison
  Log.info.json
    "GET /products"
    (Json.object [("query", Json.text "croissant")])
  ```

  Or without any data:

  ``` unison
  Log.info.json "GET /products" (Json.null)
  ```

  Using the `Info` {type Level} will show up with a small info icon in the Log
  UI.
  }}

Log.info.lazyJson : 'Text -> 'Json ->{Log} ()
Log.info.lazyJson message json = atLevel.lazyJson Info message json

Log.info.lazyJson.doc : Doc
Log.info.lazyJson.doc =
  {{
  Similar to {info.json}, but the data is wrapped in thunks. This is useful in
  avoiding a performance hit when encoding {type Json}.
  }}

Log.json : Json ->{Log} ()
Log.json json = Log.lazyJson do json

Log.Level.toJson : Level -> (Text, Json)
Log.Level.toJson = cases
  Info     -> ("level", Json.Text "info")
  Warn     -> ("level", Json.Text "warn")
  Error    -> ("level", Json.Text "error")
  Debug    -> ("level", Json.Text "debug")
  Custom t -> ("level", Json.Text t)

Log.printToHandle : Handle -> '{g, Log} r ->{g, IO, Exception} r
Log.printToHandle out =
  use Json toText
  use Text ++
  go = cases
    { r }                      -> r
    { debugLazyJson msg -> k } ->
      putText out (toText msg() ++ "\n")
      handle k() with go
    { Log.lazyJson msg -> k }  ->
      putText out (toText msg() ++ "\n")
      handle k() with go
  thunk -> (handle thunk() with go)

Log.printToHandle.doc : Doc
Log.printToHandle.doc = {{ Print all logs to the provided {type Handle}. }}

Log.printToStdOut : '{g, Log} r ->{g, IO, Exception} r
Log.printToStdOut = printToHandle stdOut

Log.printToStdOut.doc : Doc
Log.printToStdOut.doc =
  {{ Print all logs within a delayed computation to {stdOut}. }}

Log.stripDebug : '{g, Log} r ->{g, Log} r
Log.stripDebug =
  use Log lazyJson
  go = cases
    { r }                    -> r
    { debugLazyJson _ -> k } -> handle k() with go
    { lazyJson msg -> k }    ->
      lazyJson msg
      handle k() with go
  thunk -> (handle thunk() with go)

Log.stripDebug.doc : Doc
Log.stripDebug.doc =
  {{
  Strip [debug]({debugLazyJson}) logs from a delayed computation. Other logs
  (generated by {Log.lazyJson}) are preserved.
  }}

Log.warn : Text -> [(Text, Text)] ->{Log} ()
Log.warn message data =
  warn.json message (Json.object (List.map (Tuple.mapRight Json.text) data))

Log.warn.doc : Doc
Log.warn.doc =
  {{
  Log a `Warn` {type Level} message with simple data in a key/value format.

  Ex:

  ``` unison
  Log.warn
    "Deprecation Warning"
    [("upgradeLibrary", "@unison/cloud")]
  ```

  Or without any data:

  ``` unison
  Log.warn "Deprecation Warning" []
  ```

  Using the `Warn` {type Level} will show up with a small warning icon in the
  Log UI be colored accordingly.
  }}

Log.warn.json : Text -> Json ->{Log} ()
Log.warn.json message json = warn.lazyJson (do message) do json

Log.warn.json.doc : Doc
Log.warn.json.doc =
  {{
  Log a `Warn` {type Level} message with {type Json} data.

  Ex:

  ``` unison
  Log.warn.json
    "Deprecation Warning"
    (Json.object [("upgradeLibrary", Json.text "@unison/cloud")])
  ```

  Or without any data:

  ``` unison
  Log.warn.json "Deprecation Warning" Json.null
  ```

  Using the `Warn` {type Level} will show up with a small warning icon in the
  Log UI be colored accordingly.
  }}

Log.warn.lazyJson : 'Text -> 'Json ->{Log} ()
Log.warn.lazyJson message json = atLevel.lazyJson Warn message json

Log.warn.lazyJson.doc : Doc
Log.warn.lazyJson.doc =
  {{
  Similar to {warn.json}, but the data is wrapped in thunks. This is useful in
  avoiding a performance hit when encoding {type Json}.
  }}

pool :
  '{Remote} Location
    {Blobs,
    Environment.Config,
    Http,
    websockets.HttpWebSocket,
    Log,
    Scratch,
    Services,
    Storage,
    WebSockets}
pool =
  do
    match region! with
      Location id _ -> cast (Location id tags.default)
      x ->
        Remote.fail
          (failure
            "Current region does not have the expected constructor. This is a Unison Cloud bug."
            x)

pool.doc : Doc
pool.doc =
  {{
  The pool of default nodes for the current region.

  These nodes support all of the default abilities (which are listed in the
  return type), but they do not support other abilities such as GPU.

  This returns a {type Location} representing a pool of nodes. See {pool.near}
  for a single nearby default node.
  }}

pool.far :
  '{Remote} Location
    {Blobs,
    Environment.Config,
    Http,
    websockets.HttpWebSocket,
    Log,
    Scratch,
    Services,
    Storage,
    WebSockets}
pool.far = do Remote.far pool() here!

pool.far.doc : Doc
pool.far.doc =
  {{
  A node within the current region (but not the current node) that supports the
  default abilities.

  This returns a single node. See {pool} for a regional pool of default nodes.

  See {pool.near} for a nearby default node.
  }}

pool.near :
  '{Remote} Location
    {Blobs,
    Environment.Config,
    Http,
    websockets.HttpWebSocket,
    Log,
    Scratch,
    Services,
    Storage,
    WebSockets}
pool.near = do Remote.near pool() here!

pool.near.doc : Doc
pool.near.doc =
  {{
  A nearby node (often the current node) that supports the default abilities.

  This returns a single node. See {pool} for a regional pool of default nodes.

  See {pool.far} for a default node that __isn't__ the current node.
  }}

pool.toRemote :
  '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Random,
  Log,
  Scratch} a
  ->{Remote} a
pool.toRemote = pool.wrap >> force

pool.toRemote.cached :
  '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Log,
  Scratch} a
  ->{Remote} a
pool.toRemote.cached a = toRemote do
  h = Hashed (Hash (blake2b_256 (MemoTagged a)))
  match lookupHashed h with
    None   ->
      r = a()
      saveHashed h r
      r
    Some a -> a

pool.toRemote.cached.doc : Doc
pool.toRemote.cached.doc =
  use Cell modifyGet named
  use Nat +
  use Storage.doc example
  use toRemote cached
  {{
  Just like {{ docLink (docEmbedTermLink do toRemote) }}, but looks up the
  computation in the {type Scratch} cache, and if it's a miss, populates the
  {type Scratch} cache after evaluation.

  Here's a silly example:

  ```
  example do
    c = named exampleDb "cell" 0
    _ = cached do modifyGet c (x -> x + 10)
    cached do modifyGet c (x -> x + 10)
  ```

  Notice that the result is still `10`, as the second call is cached. If we do
  an uncached evaluation, the result is `20`, as expected:

  ```
  example do
    c = named exampleDb "cell" 0
    _ = cached do modifyGet c (x -> x + 10)
    toRemote do modifyGet c (x -> x + 10)
  ```
  }}

pool.toRemote.cachedFor :
  time.Duration
  -> '{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Log,
  Scratch} a
  ->{Remote} a
pool.toRemote.cachedFor ttl a = toRemote do
  use Instant + >
  guid = "200a10bffdab1316d603c7573c9c0610820cc30e"
  h = Hashed (Hash (blake2b_256 (MemoTagged a, guid)))
  match lookupHashed h with
    None                 ->
      r = a()
      saveHashed h (r, now! + ttl)
      r
    Some (r, expiration) ->
      t = now!
      if expiration > t then r
      else
        r = a()
        saveHashed h (r, t + ttl)
        r

pool.toRemote.cachedFor.doc : Doc
pool.toRemote.cachedFor.doc =
  use Cell modifyGet
  use Nat * +
  use Remote sleepMicroseconds
  use time.Duration seconds
  {{
  Just like {toRemote.cached}, but allows the cache to expire after the given
  {type time.Duration}. For example:

  ```
  Storage.doc.example do
    c = Cell.named exampleDb "cell" 0
    _ = cachedFor (seconds +10) do modifyGet c (x -> x + 10)
    _ = cachedFor (seconds +10) do modifyGet c (x -> x + 10)
    sleepMicroseconds (11 * 1000000)
    cachedFor (seconds +10) do modifyGet c (x -> x + 14)
  ```

  The first {modifyGet} is cached, but after the {sleepMicroseconds}, the cache
  expires and the {modifyGet} is run once again.
  }}

pool.toRemote.cachedFor.example :
  '{IO, Exception} (Text, Text, Text, Boolean, Boolean)
pool.toRemote.cachedFor.example = Cloud.main do
  use Cloud submit
  use Environment default
  use Instant toText
  use Nat *
  use time.Duration seconds
  _ =
    """
    An example to illustrate expiring caches. `t1` and `t2`
    should be equal since they are computed within 5 seconds
    of each other, but after the sleep, the timestamp should update.
    """
  loc = submit default() do Remote.near pool() here!
  submit default() do
    t1 = await <| (forkAt loc do cachedFor (seconds +5) do now!)
    t2 = await <| (forkAt loc do cachedFor (seconds +5) do now!)
    Remote.sleepMicroseconds (6 * 1000000)
    t3 = await <| (forkAt loc do cachedFor (seconds +5) do now!)
    (toText t1, toText t2, toText t3, t1 === t2, t2 === t3)

test> pool.toRemote.cachedFor.tests = test.verify do
  use Cell modifyGet
  use Nat * +
  use time.Duration seconds
  e = Storage.doc.example do
    c = Cell.named exampleDb "cell" 0
    _ = cachedFor (seconds +10) do modifyGet c (x -> x + 10)
    _ = cachedFor (seconds +10) do modifyGet c (x -> x + 10)
    Remote.sleepMicroseconds (11 * 1000000)
    cachedFor (seconds +10) do modifyGet c (x -> x + 14)
  test.ensureEqual e (Right 24)

pool.toRemote.doc : Doc
pool.toRemote.doc =
  {{
  Run a delayed computation that requires default location abilities, using
  only the {type Remote} ability. This is accomplished by forking (usually
  within the same machine) to a node that supports the default abilities.

  This can be useful for turning the function into a form that can be passed
  into {forkAt} and similar functions.

  See {pool.wrap} to wrap an arbitrary function as opposed to a delayed
  computation.
  }}

pool.wrap :
  (a
  ->{Environment.Config,
  Exception,
  Http,
  Blobs,
  Services,
  Storage,
  Remote,
  websockets.HttpWebSocket,
  WebSockets,
  Random,
  Log,
  Scratch} b)
  -> a
  ->{Remote} b
pool.wrap f a =
  l = pool.near()
  if isHere l then coerceAbilities (do randomly do reraise! do f a) ()
  else await (forkAt l do randomly do f a)

pool.wrap.doc : Doc
pool.wrap.doc =
  {{
  Transform a function that uses the default location abilities into a function
  that uses only the {type Remote} ability.

  This can be useful for turning the function into a form that can be passed
  into {deploy.remote}.

  See {toRemote} for a version specialized to delayed computations.
  }}

README : Doc
README =
  use Cloud submit
  use Remote both doc
  use webSocketEcho deploy
  {{
  {{
  Image
    {{
    Unison Cloud Splash
    }}
    {{
    https://share.unison-lang.org/static/unison-cloud-splash.svg
    }}
    None }}

  # 🌤 The official client for [Unison Cloud](https://www.unison.cloud/)

    This library provides a set of types and functions for building and
    deploying services and running distributed batch computations on our cloud
    computing platform, [Unison Cloud](https://app.unison.cloud/). With the
    Cloud client, a Unison program can deploy Unison code with minimal
    ceremony.

    Here's a sample of things you can do with this library:

    * Deploy HTTP services
    * Issue native Unison-to-Unison requests between services
    * Create persistent storage for your services
    * Instrument your services with logging
    * Launch a distributed computation in the cloud

    The remainder of this document will walk through a few examples of how to
    use the Cloud client, with links to learn more at the end.

  # 🚀 Deploy an HTTP service

    ## Prerequisites

       To interact with the Unison Cloud via this client, you'll need:

       * The
         [Unison Codebase Manager](https://www.unison-lang.org/learn/quickstart/)
       * A [Unison account](https://share.unison-lang.org). There's just one
         Unison account covering both Cloud and Share. If you don't already
         have one, you can click the signup button on
         [Unison Share](https://share.unison-lang.org).
       * [Cloud access](https://unison.cloud/signup/?plan=Free) for your
         account. After you have access, you may need to rerun `auth.login` in
         UCM.

    ## "Hello world" in 5 lines of code

       The code below is a complete service deployment. We'll break down the
       components in more detail in a bit.

           @source{simpleHttp.main}

       The {Cloud.main} function encloses a codeblock for describing
       interactions with the Cloud via the {type Cloud} ability. You'll use
       ordinary Unison programs like this to deploy and manage your services
       and batch jobs.

       {{
       docAside
         {{
         {Cloud.run} runs your code in the cloud, whereas {Cloud.run.local} or
         {run.local.serve} are for local development.
         }} }}

       The {{ docLink (docEmbedTermLink do deployHttp) }} function takes both
       an {type Environment} (produced here using {Environment.default}), which
       controls what config and storage resources the service can access, and
       the service logic, {{
       docExample 1 do
         helloService -> (helloService : (HttpRequest -> HttpResponse)) }}
       which accepts a request and produces a response.

       The service logic function can also use abilities such as {type Log}
       (for logging), {type Storage} (for durable storage access),
       {type Environment.Config} (for loading config values and secrets),
       {type Remote} (for spawning computations in parallel on multiple
       locations), and so on. See the full docs for
       {{ docLink (docEmbedTermLink do deployHttp) }}.

       A service can have many routes and use any of the abilities found in the
       {{ docLink (docEmbedTermLink do deployHttp) }} signature, but in this
       case it's a pure function which always returns a 200 response with the
       body `"Hello world"`.

       {{
       docAside
         {{
         In most cases, writing http services with the higher-level
         [Routes library](/@unison/routes) is a good idea.
         }} }}

    ## Running the service

       Kick off the deployment with the
       [`run`](https://www.unison-lang.org/learn/usage-topics/running-programs/)
       command in the UCM.

       ``` ucm
       @unison/cloud/main> run simpleHttp.main

         Service exposed successfully at:
           https://<username>.services.unison.cloud/h/aFSi8xVtxvDxMb37RxFxfZyj0c3LE2t_VeQz-wqxWtY

         ServiceHash.ServiceHash "aFSi8xVtxvDxMb37RxFxfZyj0c3LE2t_VeQz-wqxWtY"
       ```

       Once the http service is deployed to the Cloud, the UCM will print out
       the URI produced as a result of the deployment and return a
       {type ServiceHash} value. You can view logs for any deployed services at
       the [Unison Cloud UI](https://app.unison.cloud), or you can stream logs
       for a service to your local console using {service.tail.console}.

       The {type ServiceHash} is dependent upon the hash of the function
       provided to {{ docLink (docEmbedTermLink do deployHttp) }}; so
       deployments of the same function will be idempotent but subsequent
       changes to the service (for example adding new endpoints or changing
       JSON decoders) will result in a different hash.

       {{
       docCallout
         (Some {{ 📌 }})
         {{
         To make a request to a service from its hash, you can use the
         following URI format, replacing `<username>` with your Unison account
         username:

         ``` raw
         https://<username>.services.unison.cloud/h/<serviceHash>
         ```
         }} }}

       At this point, we can test the http service with curl in a regular
       terminal:

       ``` bash
       > curl https://myUser.services.unison.cloud/h/aFSi8xVtxvDxMb37RxFxfZyj0c3LE2t_VeQz-wqxWtY

       Hello world
       ```

       ### Naming services and deploying new versions

           When you deploy a service to the Cloud with
           {{ docLink (docEmbedTermLink do deployHttp) }}, the
           {type ServiceHash} returned uniquely identifies a snapshot of the
           deployed code. Service deployments, just like regular Unison
           functions are
           [**content addressed**](https://www.unison-lang.org/learn/the-big-idea/)
           and immutable.

           However, it's not uncommon to need a more stable and human-friendly
           name for consumers of your API, one whose backing implementation can
           change over time. Adding a {ServiceName} for your service is an
           optional next step.

               @source{simpleHttpNamed.main}

           This code takes our earlier {simpleHttp.main} example and creates a
           {type ServiceName} for it. You can use {ServiceName.assign} again
           later to replace the implementation of the service with a new one,
           and {ServiceName.unassign} to remove any backing implementation from
           that {type ServiceName}.

           {{
           docAside
             {{
             💡 By analogy, a {type ServiceHash} is like a git commit, and a
             {type ServiceName} is like a branch name that points to a
             different commit as you release new versions of your service.
             (Another analogy might be that a {type ServiceName} is a like a
             typed domain name, and a {type ServiceHash} is like an IP address
             that this domain name points to.)
             }} }}

           {{ docCallout
             (Some {{ 📌 }}) {{
             A named service will have a URI which follows the pattern:

             ``` raw
             https://<username>.services.unison.cloud/s/<serviceName>
             ```
             }} }}

    ## Using durable storage

       Let's take a look at a service which uses logging and durable storage:

       {{ docSource [docSourceElement (docEmbedTermLink do simpleStateful) []]
       }}

       The {type Log} ability used above can be added anywhere in your Unison
       program to log messages in {type Text} or {type Json} formats. These
       messages will automatically be associated with the service which
       generated them. They'll also receive a timestamp and log level which can
       be used to filter messages in [the Cloud UI](https://app.unison.cloud/).

       {{
       docAside
         {{
         [💡 You can read more about using the `Log` ability and its utility
         functions here]({type Log}).
         }} }}

       The {type Storage} ability is the persistent storage layer for the
       Cloud. {{ docLink (docEmbedTermLink do simpleStateful) }} has been
       updated with {type Database} and {type Table} arguments so that the
       service can read and write to it.

       {{
       docAside
         {{
         [💡 Learn more about using the Storage ability here]({type Storage})
         }} }}

       The entry point to the cloud service deployment needs to be updated to
       describe the storage resources:

           @source{simpleStatefulHttp.main}

       Note that the {type Database} needs to be created and assigned to an
       {type Environment} before it can be used by the service. Use
       {Database.assign} for this, and {Database.unassign} to remove an
       environment's access to a {type Database}.

  # 🧮 Submit a distributed job

    The Cloud client also provides a way to run distributed, one-off jobs.
    These kinds computations can be run in parallel to perform large scale data
    processing tasks, and to load data into Unison Cloud for use by your
    services.

    Here's a simple example of a distributed computation which increments and
    then sums together a range of numbers using the {type Remote} ability:

        @source{simpleBatch.main}

    The result is computed on the Cloud and returned to the local machine.

    {{
    docCallout
      (Some {{ 📌 }})
      {{
      You can save the result of running cloud computations back in your
      codebase with the `add.run` command.
      }} }}

  # 🔄 Create a WebSocket service

    HTTP handlers in Unison Cloud can be simple functions from
    {type HttpRequest} to {type HttpResponse}, like the HTTP example above, but
    It is also possible to deploy services that also handle
    {type websockets.WebSocket} connections by using {deployHttpWebSocket}.
    instead of {deployHttp}.

    Here's an example deploying a simple WebSocket echo service:

        @source{deploy}

    If your websocket handler needs to perform cleanup when the connection is
    closed, you can call {addFinalizer} which will get called when your handler
    exits. Here's an updated example of the above handler that logs when the
    websocket is closed:

        @source{webSocketCleanup.handler}

    ## The Remote Interface

       The {type Remote} ability bears a special relationship to the Unison
       Cloud. We've seen that the {type Remote} ability can be handled by
       top-level functions like {{ docLink (docEmbedTermLink do submit) }} or
       {{ docLink (docEmbedTermLink do deploy) }}:

       {{ docSignature [docEmbedSignatureLink do submit] }}

       But it's also possible to use the full set of abilities supported by the
       Cloud when using generic {type Remote} functions. Let's say we want to
       use the {type Log} ability in the following example:

           @source{runBoth}

       {{
       docAside
         {{ [💡 You can read more about using the Remote ability here]({doc}) }}
       }}

       The signature of {both} doesn't seem to permit the use of the {type Log}
       ability.

           @signature{both}

       However, we can use the {{ docLink (docEmbedTermLink do toRemote) }}
       function from the Cloud client to embed a {type Log} call in programs
       written against the {type Remote} API.

       {{ docSignature [docEmbedSignatureLink do toRemote] }}

       Updating the example with a few calls to {{
       docSignatureInline (docEmbedSignatureLink do info) }} looks like this:

           @source{runBothLogged}

    ## Privacy and security disclaimer

       The free tier of Unison Cloud is intended to be a fun and low-friction
       way to experiment with Unison and distributed programming. While there
       are safeguards in place, programs submitted to the free tier of Unison
       Cloud run on shared infrastructure. If you have high security needs for
       your application and want to discuss whether Unison is a good fit, you
       can discuss your use case with us in the chat widget on
       [Unison Cloud](https://www.unison.cloud).

  # 🗺 Learn more

    Related concepts and libraries:

    * [Cloud code examples and template project](https://share.unison-lang.org/@unison/cloud-start)
    * [Unison services preview blog post](https://www.unison-lang.org/whats-new/unison-services-preview/)
    * [The Services ability]({type Services})
    * [The Storage ability]({type Storage})
    * [The Log ability]({type Log})
    * [More about Cloud Environments]({Environment})
    * [The Config ability]({type Environment.Config})
    * [The Remote ability]({doc})
    * [The Http client library](https://share.unison-lang.org/@unison/httpclient)
    * [The Routes http service library](https://share.unison-lang.org/@unison/routes)
  }}

ReleaseNotes : Doc
ReleaseNotes =
  {{
  **Additions**

  * Adds the {type Batch} storage ability, for performing bulk read requests
    against a database.
    * {type Batch} can be handled by {batchRead} and {transact}
  * Adds {cachedFor}, a version of {toRemote} which accepts a TTL duration for
    cached evaluation.

  **Changes**

  * {transact} accepts {type Random} and {type Batch} in transactions
  * {transact.stream} also accepts {type Random} and {type Batch} in
    transactions.
  * Performance improvements to {type OrderedTable} read queries

  **Deprecations**

  * `Storage.transact.random` has been renamed to {random.deprecated}. Use
    {transact} instead.
  }}

Remote.assuming : (Location {} -> Location g) -> '{g, Remote} a ->{Remote} a
Remote.assuming assume a =
  t = Remote.fork (assume here!) a
  await t

Remote.assuming.doc : Doc
Remote.assuming.doc =
  {{
  `` assuming assume r `` forces `r` at the current location, assuming that the
  current location can handle all the abilities used by `r`.

  * {{ docExample 1 do r -> assuming Scratch.assume r }}
  * {{ docExample 1 do r -> assuming (Scratch.assume >> Scratch.assume) r }}
  }}

Remote.at : Location g -> '{g, Exception, Remote} t ->{Remote} t
Remote.at loc a = await (forkAt loc a)

Remote.await : Task a ->{Remote} a
Remote.await t =
  p = promise t
  out = Remote.Promise.read p
  Thread.cancel (thread t)
  Promise.delete p
  Remote.reraise out

Remote.await.doc : Doc
Remote.await.doc =
  use Nat +
  use Remote fork
  use docs run
  {{
  `` await task `` waits until the given {type Task} is complete and returns
  its result.

  A {type Task} can only be unwrapped with {await} __one time__. If the
  eventual value may need to be awaited multiple times, see {awaitRetain}
  instead.

  ```
  forkedTask : '{Remote} Nat
  forkedTask = do
    slowTask = fork here! do
      Remote.sleepMicroseconds 10000
      1 + 1
    await slowTask
  run forkedTask
  ```

  While it is possible to control the flow of a {type Remote} program with
  primitives like {fork} and {await}, a number of other {type Remote} functions
  provide additional semantics for handling cancellations or failures.

  For example, the faster of the two awaited tasks below fails, but the slower
  task completes successfully before the failure is raised. Using {Remote.both}
  instead would surface the failure immediately and cancel the long-running
  task.

  ```
  slowAwait : '{Remote} (Nat, Nat)
  slowAwait = do
    slowTask = fork here! do
      Remote.sleep (Remote.Duration.seconds 10)
      1 + 1
    failFast = fork here! do Remote.fail (failure "failed!" ())
    (await slowTask, await failFast)
  run slowAwait
  ```

  **See also**

  * {Remote.cancel} for cancelling a task
  * {awaitRetain} for a task that may need to be awaited multiple times
  }}

Remote.awaitRetain : Task a ->{Remote} a
Remote.awaitRetain t =
  p = promise t
  out = Remote.Promise.read p
  Remote.reraise out

Remote.awaitRetain.doc : Doc
Remote.awaitRetain.doc =
  {{
  `` awaitRetain task `` awaits the result of the given {type Task}, but unlike
  {await} the Task can be awaited multiple times.

  ```
  forkedTask : '{Remote} Nat
  forkedTask = do
    use Nat +
    task1 = Remote.fork here! do 1 + 1
    awaitRetain task1 + awaitRetain task1
  docs.run forkedTask
  ```

  In the example above the task is awaited twice, but it will only be computed
  once.
  }}

Remote.both : '{Remote} a -> '{Remote} b ->{Remote} (a, b)
Remote.both l r = Remote.scope do
  lt = forkAt here! l
  rt = forkAt here! r
  match race (do awaitRetain lt) do awaitRetain rt with
    Left a  -> (a, awaitRetain rt)
    Right b -> (awaitRetain lt, b)

Remote.both.doc : Doc
Remote.both.doc =
  use Nat +
  use Remote both fail sleep
  use Remote.Duration seconds
  use docs run
  {{
  `` both left right `` runs two {type Remote} computations in parallel and
  returns a tuple of their results.

  ```
  left : '{Remote} Nat
  left = do
    leftTask = forkAt region! do 1 + 1
    await leftTask
  right : '{Remote} Nat
  right = do
    rightTask = forkAt region! do 2 + 2
    await rightTask
  run do both left right
  ```

  If either of the arguments fails, the other task will immediately be
  cancelled and the overall result of {both} will be a failure. {both} is
  distinct from forking two tasks and awaiting them in a tuple because of this
  cancellation behavior.

  The following example using {both} will stop the `` Either.left `` task from
  continuing to run after the `` Either.right `` task fails :

  ```
  bothExample : '{Remote} (Nat, Nat)
  bothExample = do
    left : '{Remote} Nat
    left = do
      leftTask = forkAt region! do
        sleep (seconds 10)
        1 + 1
      await leftTask
    right : '{Remote} Nat
    right = do
      rightTask = forkAt region! do fail (failure "oh no!" ())
      await rightTask
    both left right
  run bothExample
  ```

  Though the signature below may be the same, this next example would run for
  the full duration of the left task before returning a failure:

  ```
  twoForksExample : '{Remote} (Nat, Nat)
  twoForksExample = do
    leftTask = forkAt region! do
      sleep (seconds 10)
      1 + 1
    rightTask = forkAt region! do fail (failure "oh no!" ())
    (await leftTask, await rightTask)
  run twoForksExample
  ```

  **See also**

  * {race} for running two computations in parallel and returning the result of
    the first to complete
  * {Remote.parMap} for applying a function to each element of a list in
    parallel
  }}

Remote.cancel : Task a ->{Remote} ()
Remote.cancel = cases
  Task thread promise ->
    Thread.cancel thread
    Promise.delete promise

Remote.cancel.doc : Doc
Remote.cancel.doc =
  use Remote cancel sleep
  use Remote.Duration seconds
  use docs run
  {{
  `` cancel task `` cancels the given {type Task} and all of its still-running
  subtasks.

  When cancelling tasks, it's important to consider whether the cancellation
  would leave resources in an inconsistent state. Finer grained control over
  cancellation and resource safety can be managed with functions like
  {Remote.scope} and {finalizer}.

  In the example below, the top-level task is cancelled, triggering its subtask
  to stop what might have been computationally expensive work.

  ```
  cancelExample : '{Remote} ()
  cancelExample = do
    slowTask : Task Text
    slowTask = forkAt here! do
      sleep (seconds 1)
      evenSlowerTask = forkAt region! do
        sleep (seconds 10)
        "🦥 🦥 🦥"
      await evenSlowerTask
    cancel slowTask
  run cancelExample
  ```

  Awaiting an already cancelled task will result in a {type Failure} being
  raised.

  ```
  example : '{Remote} Nat
  example = do
    use Nat +
    slowTask = Remote.fork here! do
      sleep (seconds 1)
      1 + 1
    cancel slowTask
    await slowTask
  run example
  ```
  }}

Remote.catchOnly :
  Type -> '{g, Exception, Remote} t ->{g, Exception, Remote} Either Failure t
Remote.catchOnly typ r = match Remote.try r with
  Left f@(Failure t _ _) | Boolean.not (t === typ) -> Exception.raise f
  r                      -> r

Remote.detach : Location g -> '{g, Exception, Remote} t ->{Remote} Thread
Remote.detach loc a = Remote.detachAt (Remote.near loc here!) a

Remote.detachAt : Location g -> '{g, Exception, Remote} t ->{Remote} Thread
Remote.detachAt loc prog = Remote.reraise (tryDetachAt loc prog)

Remote.detachAt_ : Location g -> '{g, Exception, Remote} t ->{Remote} ()
Remote.detachAt_ loc a = unison_base_3_22_0.ignore (Remote.detachAt loc a)

Remote.detach_ : Location g -> '{g, Exception, Remote} t ->{Remote} ()
Remote.detach_ loc a = detachAt_ (Remote.near loc here!) a

Remote.doc : Doc
Remote.doc =
  use Nat +
  use Remote cancel catchOnly try
  use docs run
  {{
  {type Remote} is an ability for distributed programming, based on a model of
  forking background tasks (possibly at other locations) and awaiting their
  results.

      @signatures{forkAt, await}

  {forkAt} returns a {type Task} which represents the future result of the
  running {type Remote} computation. Thus, multiple tasks can be started
  running in the background, in parallel, and their results combined.

  Here's a simple example of forking a few tasks locally:

  ```
  run do
    t1 = forkAt here! do 1 + 1
    t2 = forkAt here! do 2 + 2
    await t1 + await t2
  ```

  The {forkAt} function can be used to fork computations at any
  {type Location}, (including {here!}) that provides the abilities required by
  the computation. {type Location} values are typed based on the abilities they
  support. We might have GPU-typed locations, locations with access to a that
  can issue HTTP requests, and so on.

  For instance, in this example, the forked computation needs {type Stream}, so
  it requires that the {type Location} passed in support that as well.

      @source{docs.examples.ex1}

  # Error handling

    {type Remote} tasks can fail. {tryAwait}, {try}, {catchOnly}, can be used
    to catch and respond to these failures.

        @signatures{tryAwait, try, catchOnly}

  # Task trees and structured concurrency

    {forkAt} creates a __subtask__ said to __linked__ to the current task. When
    the current ask completes, any still-running subtasks are terminated. This
    is typically the right thing and avoids dangling tasks that continue
    running even though their result is never accessed.

    For instance, in this example, one of the subtasks is an infinite loop
    whose result isn't needed to produce the result.

        @source{taskTrees}

    The infinite looping task will be cancelled automatically when the overall
    task completes.

    ```
    taskTrees
    ```

    ## Lower-level primitives

       TODO low-level Promise, Thread, `scope`, `addFinalizer`

    ## Garbage collection of tasks

       When a task completes at a location, its result is kept in memory until
       an {await} or {cancel}. If you use higher-level primitives like {forkAt}
       this happens automatically and you don't need to explicitly {cancel}.

       If you're using the lower level primitives, you take responsibility for
       making sure you {await} or {cancel} any tasks you've created.

  # Handlers

    * `Remote.run.threaded` is a local parallel handler, using local threads.
    * {run} is a (rather slow) sequential local handler based on cooperative
      threading. It's useful for testing and examples in documentation.
    * The [cloud client](https://share.unison-lang.org/@unison/cloud) contains
      higher-level versions of both the threaded and pure handler with a more
      streamlined api, you should prefer those in most cases.
    * [Unison Cloud](https://unison.cloud) has a handler that does distributed
      execution on a managed pool of nodes in the cloud.

    Coming soon:

    * "Chaos monkey" handlers that simulate network partitions and failures
    * Local handlers that measure simulated network traffic
      [Unison Cloud](http://unison.cloud)
  }}

Remote.docs.examples.ex1 : Location {g, Stream Text} ->{Remote} Nat
Remote.docs.examples.ex1 region =
  use Nat +
  t1 = forkAt region do
    emit "Important logging message!"
    1 + 1
  t2 = forkAt region do 41 + 1
  await t1 + await t2

Remote.docs.examples.taskTrees : Either Failure Nat
Remote.docs.examples.taskTrees =
  use Nat +
  infiniteLoop _ =
    Remote.sleep (Remote.Duration.hours 1)
    infiniteLoop()
  docs.run do
    t2 = forkAt here! infiniteLoop
    t1 = forkAt here! do 1 + 1
    await t1

Remote.docs.run : '{Exception, Remote, Scratch} a -> Either Failure a
Remote.docs.run r = at2 (simulate 42 do eval testPool r)

Remote.docs.run.doc : Doc
Remote.docs.run.doc =
  use Nat +
  use Remote fork
  {{
  A sequential local interpreter of {type Remote}. Also handles the
  {type Scratch} abilities using an in-memory handler.

  ```
  docs.run do
    t1 = fork here! do 1 + 1
    t2 = fork here! do 100 + 100
    await t1 + await t2
  ```

  This implementation is quite slow and is more suitable for small examples in
  tests or documentation.
  }}

test> Remote.docs.run.tests.test1 =
  unison_base_3_22_0.ignore "it should fork and await a computation"
  testFunction = do
    use Nat +
    task = Remote.fork here! do 1 + 1
    await task
  res = docs.run testFunction
  check (res === Right 2)

test> Remote.docs.run.tests.test2 =
  unison_base_3_22_0.ignore "it should handle nested forking and awaiting"
  testFunction = do
    use Remote fork
    use Text ++
    task1 = fork here! do "t1"
    task2 = fork here! do
      t1 = await task1
      task3 = fork here! do "t3"
      t3 = await task3
      t1 ++ "t2" ++ t3
    await task2
  res = docs.run testFunction
  check (res === Right "t1t2t3")

test> Remote.docs.run.tests.test3 =
  unison_base_3_22_0.ignore "it should modify a ref in a forked task"
  testFunction = do
    use Nat +
    use Ref update
    ref = detached 42
    task1 = Remote.fork here! do
      update ref (a -> a + 1)
      update ref (a -> a + 2)
      Remote.Ref.read ref
    r = await task1
    Ref.delete ref
    r
  res = docs.run testFunction
  check (res === Right 45)

(Remote.Duration.*) : Nat -> Remote.Duration -> Remote.Duration
t Remote.Duration.* d =
  use Nat *
  Duration.Duration (t * toMicros d)

(Remote.Duration.+) : Remote.Duration -> Remote.Duration -> Remote.Duration
d1 Remote.Duration.+ d2 =
  use Nat +
  Duration.Duration (toMicros d1 + toMicros d2)

Remote.Duration.hours : Nat -> Remote.Duration
Remote.Duration.hours t =
  use Remote.Duration *
  60 * Remote.Duration.minutes t

Remote.Duration.micros : Nat -> Remote.Duration
Remote.Duration.micros t = Duration.Duration t

Remote.Duration.millis : Nat -> Remote.Duration
Remote.Duration.millis t =
  use Remote.Duration *
  1000 * micros t

Remote.Duration.minutes : Nat -> Remote.Duration
Remote.Duration.minutes t =
  use Remote.Duration *
  60 * Remote.Duration.seconds t

Remote.Duration.seconds : Nat -> Remote.Duration
Remote.Duration.seconds t =
  use Remote.Duration *
  1000 * millis t

Remote.Duration.toBaseDuration : Remote.Duration -> time.Duration
Remote.Duration.toBaseDuration d =
  use Nat /
  micros = toMicros d
  secs = Nat.toInt (micros / 1000000)
  nanos = Nat.mod micros 1000000
  internal.Duration secs nanos

Remote.Duration.toMicros : Remote.Duration -> Nat
Remote.Duration.toMicros = cases Duration.Duration t -> t

Remote.Duration.zero : Remote.Duration
Remote.Duration.zero = Duration.Duration 0

Remote.eval : Location g -> '{g, Exception, Remote} t ->{Remote} t
Remote.eval loc a =
  t = Remote.fork loc a
  await t

Remote.eval.doc : Doc
Remote.eval.doc =
  {{
  `` eval loc a `` evaluates `a` at the current location, using evidence that
  the current location supports the abilities provided by `loc`.
  }}

Remote.failure.cancelled : Failure
Remote.failure.cancelled =
  Failure (typeLink Cancelled) "thread was cancelled" (Any ())

Remote.failure.deserializationError : Text -> Failure
Remote.failure.deserializationError msg =
  Failure (typeLink DeserializationError) msg (Any ())

Remote.failure.failedToLoadTerms : [Link.Term] -> Failure
Remote.failure.failedToLoadTerms terms =
  Failure
    (typeLink FailedToLoadTerms) "failed to fetch required terms" (Any terms)

Remote.failure.NoAvailableLocations.doc : Doc
Remote.failure.NoAvailableLocations.doc =
  {{
  An error tag indicating that no available [locations]({type Location}) were
  found.

  For example, this can appear in a {type Failure} returned by {tryNear} or
  {tryFar} if no nodes are available in the candidate pool.
  }}

Remote.failure.promiseDeleted : Promise.Id -> Failure
Remote.failure.promiseDeleted id =
  Failure (typeLink PromiseDeleted) "Promise was deleted" (Any id)

Remote.failure.RestrictedOperation.doc : Doc
Remote.failure.RestrictedOperation.doc =
  {{
  An error tag indicating that a computation includes a restricted operation.

  For example, consider the following code:

  ```
  do forkAt region! do coerceAbilities do getEnv "POSTGRES_PASSWORD"
  ```

  The {type Location} returned by {region!} doesn't include the {type IO}
  ability, but the user code has used {coerceAbilities} to try to sneak in some
  malicious code. A {type Remote} handler could detect the restricted operation
  and return a {type Failure} with a {type RestrictedOperation} error tag.
  }}

Remote.failure.timeout : reason -> Failure
Remote.failure.timeout reason =
  Failure (typeLink Timeout) "timeout" (Any reason)

Remote.failure.unknownHash : Text -> h -> Failure
Remote.failure.unknownHash msg h = Failure (typeLink UnknownHash) msg (Any h)

Remote.failure.UnknownHash.doc : Doc
Remote.failure.UnknownHash.doc =
  {{
  Failure type for errors caused by a hash not being found.

  **See:** {unknownHash} convenience function.
  }}

Remote.failure.unknownHash.doc : Doc
Remote.failure.unknownHash.doc =
  {{
  Create a {type Failure} of type {type UnknownHash}.

  For example, a client of a content-addressed storage layer might fail with ``
  unknownHash "key not found" h `` if it's missing a hash `h`.
  }}

Remote.failure.UnknownLocation.doc : Doc
Remote.failure.UnknownLocation.doc =
  {{
  The specfied location cannot be found. For example, this failure can arise if
  a program attempts to fork to a node that has shut down.
  }}

Remote.failure.unknownPromise : Promise.Id -> Failure
Remote.failure.unknownPromise id =
  Failure (typeLink UnknownPromise) "lookup of unknown promise" (Any id)

Remote.failure.unknownRef : Ref.Id -> Failure
Remote.failure.unknownRef id =
  Failure (typeLink UnknownRef) "unknown atomic reference ID" (Any id)

Remote.failure.unknownTask : UID -> Failure
Remote.failure.unknownTask tid =
  Failure (typeLink UnknownTask) "lookup of unknown task" (Any tid)

Remote.failure.ValueTooLarge.doc : Doc
Remote.failure.ValueTooLarge.doc =
  {{
  A program attempted to serialize a value that was larger than the configured
  limit.

  A number of actions can result in this error, including:

  * Calling {saveHashed} with a large value
  * Calling {Remote.Promise.write} with a large value (if a remote node is
    awaiting the result)
  * Calling {forkAt} with a function that captures a large amount of data
  }}

Remote.far : Location g -> Location g2 ->{Remote} Location g
Remote.far region origin = Remote.reraise (tryFar region origin)

Remote.finalizer : '{Remote} () ->{Remote} ()
Remote.finalizer fin = addFinalizer (_ -> fin())

Remote.finalizerError : '{Remote} () ->{Remote} ()
Remote.finalizerError fin = addFinalizer cases
  Completed -> ()
  _         -> fin()

Remote.fork : Location g -> '{g, Exception, Remote} t ->{Remote} Task t
Remote.fork loc a = forkAt (Remote.near loc here!) a

Remote.fork.doc : Doc
Remote.fork.doc =
  {{
  `` Remote.fork withLocationAbilities computation `` spawns a __local__
  {type Task} for a computation.

  The {type Location} argument is not used to direct where the computation will
  be run. Instead, the location argument contains type parameters which
  represent the set of abilities that the computation is permitted to use
  locally.

  **See also**

  * {forkAt} for spawning a task at a specific location
  * {type Location} for more information about locations
  }}

Remote.forkAt : Location g -> '{g, Exception, Remote} t ->{Remote} Task t
Remote.forkAt loc prog =
  use Remote.Promise write_
  result = Promise.empty()
  thread = Remote.scope do
    finalizerError do Promise.delete result
    Remote.detachAt loc do
      addFinalizer cases
        Completed        -> ()
        Cancelled        -> write_ result (Left cancelled)
        Outcome.Failed e -> write_ result (Left e)
      Remote.Promise.write result (Right prog())
  finalizer do Thread.cancel thread
  Task thread result

Remote.forkAt.doc : Doc
Remote.forkAt.doc =
  use Remote fork
  {{
  `` forkAt location computation `` creates a background {type Task} for a
  computation at the given {type Location}. The {type Task} that it returns
  represents the future value of the computation, which can be unwrapped with
  functions like {await}. Both {forkAt} and {fork} handle the details of
  cleaning up the {type Task} that they spawn.

  The function below is using `` region! `` to randomly select a location in
  the region of the cloud to run the task.

  ```
  forkAtExample : '{Remote} Nat
  forkAtExample = do
    use Nat +
    slowTask = forkAt region! do
      Remote.sleep (Remote.Duration.seconds 1)
      1 + 1
    await slowTask
  docs.run forkAtExample
  ```

  In the type signature, @inlineSignature{forkAt}, the {type Location} argument
  and the computation argument share a common type parameter. This type
  parameter represents the set of abilities that the computation is permitted
  to use at the given location. By design, you cannot run a function which uses
  an ability, for example, {{ docCode {{ '{IO} Nat }} }}, at a location like
  `Location {Http, Log}` that doesn't match it.

  **See also**

  * {fork} for spawning a task at the current location
  * {await} for waiting for a task to complete
  * {type Location} for more information about locations
  }}

Remote.forkBracketUsefulNotes.doc : Doc
Remote.forkBracketUsefulNotes.doc =
  {{
  `forkBracketAt loc acquire release u` acquires a resource, starts a task with
  access to the resource, then in all cases calls `release` when finished.

  The implementation is similar in spirit to:

  @typecheck ```
  bracket acquire release u =
    r = acquire()
    e = Remote.try u
    release r
    Remote.reraise e
  ```

  ... except that the real implementation has to be more complicated to make
  sure that interrupts of the returned task don't lead to the `release` action
  not being executed.

  # Important caveats

    Note that since a {type Location} could be hit by an asteroid or unplugged
    at any time, the `release` function should only be used to cleanup
    resources that wouldn't survive a reboot anyway. So it's fine to use this
    function to close a socket or a file handle or a database connection.

    If it's being used to release access to some __remote__ resource then that
    could be problematic if the location running the {#fubiqj35oa} crashes
    while holding the resource, but before calling `release`.
  }}

Remote.gatherSuccessful :
  Location g -> (a ->{g, Remote} b) -> [a] ->{Remote} [b]
Remote.gatherSuccessful loc f as =
  use List map
  tbs = map (a -> Remote.fork loc do f a) as
  rights (map (t -> Remote.try do await t) tbs)

Remote.isComplete : Task a ->{Remote} Boolean
Remote.isComplete t = isSome (Promise.readNow (promise t))

Remote.isHere : Location g ->{Remote} Boolean
Remote.isHere l =
  use Location locationId
  locationId l === locationId here!

Remote.isHere.doc : Doc
Remote.isHere.doc =
  use Location locationId
  {{
  `` isHere loc `` returns `` true `` if `` locationId loc `` is the same as
  the ``locationId here!``.
  }}

Remote.isIncomplete : Task a ->{Remote} Boolean
Remote.isIncomplete t = isNone (Promise.readNow (promise t))

Remote.link : Task a1 -> Task a ->{Remote} ()
Remote.link lifetime t = detach_ here! do
  _ = Remote.try do awaitRetain lifetime
  Remote.cancel t

Remote.link.doc : Doc
Remote.link.doc =
  {{
  `` Remote.link lifetime t `` cancels `t` when `lifetime` completes.

  It calls {awaitRetain} on `lifetime` to await its completion.
  }}

Remote.localize :
  Location g ->{Remote} Optional ('{g, Exception, Remote} a ->{Remote} a)
Remote.localize loc = if isHere loc then Some (eval loc) else None

Remote.localize.doc : Doc
Remote.localize.doc =
  {{
  `` localize loc `` returns {Some} if ``isHere loc``, providing an evaluation
  function for local evaluation.

  It's handy when receiving a location dynamically, and you want to short
  circuit or specially handle the case where the provided location happens to
  be where you are right now.
  }}

Remote.Location.doc : Doc
Remote.Location.doc =
  {{
  {type Location} represents logical locations in a distributed computation.
  Its type parameter is a phantom type that ensures calls to {forkAt} use only
  the abilities assumed provided by the location where the task is forked.

  During actual distributed execution, a handler of {type Remote} will logical
  {type Location} values to actual network addresses. Multiple locations may be
  mapped to the same physical network address.

  There are a few ways to obtain locations. They can be created via operations
  of {type Remote}:

      @signatures{here!, Remote.near, Remote.far, workLocation}

  They can also be created statically via the {Location} constructor. This is
  generally used to declare static locations that refer to compute pools or
  regions which aren't created dynamically.
  }}

Remote.Location.Id.toText : Location.Id -> Text
Remote.Location.Id.toText = cases LocationId (UID bs) -> Bytes.toHex bs

Remote.Location.locationId : Location g -> Location.Id
Remote.Location.locationId = cases
  Location i _ -> i
  Near i _ _ _ -> i
  Far i _ _ _  -> i

Remote.Location.tags : Location g -> [Text]
Remote.Location.tags = cases
  Location _ ts -> ts
  Near _ _ _ ts -> ts
  Far _ _ _ ts  -> ts

Remote.Location.tags.add : Text -> Location g -> Location g
Remote.Location.tags.add tag loc =
  match loc with
    Location id1 ts| Boolean.not (List.contains tag ts)  ->
      Location id1 (insertSortedDistinct tag ts)
    Near id1 id2 id3 ts| Boolean.not (List.contains tag ts)  ->
      Near id1 id2 id3 (insertSortedDistinct tag ts)
    Far id1 id2 id3 ts| Boolean.not (List.contains tag ts)  ->
      Far id1 id2 id3 (insertSortedDistinct tag ts)
    _ -> loc

Remote.Location.unsafe.cast : Location g -> Location g2
Remote.Location.unsafe.cast loc = unsafeExtract (Any loc)

Remote.Location.unsafe.cast.doc : Doc
Remote.Location.unsafe.cast.doc =
  {{ Cast the ability requirements of a {type Location}. }}

Remote.near : Location g -> Location g2 ->{Remote} Location g
Remote.near region loc = Remote.reraise (tryNear region loc)

Remote.own : Task a ->{Remote} ()
Remote.own t = finalizer do Remote.cancel t

Remote.parMap : (a ->{Remote} t) -> [a] ->{Remote} [t]
Remote.parMap f as =
  Remote.scope do
    parMap.impl (Nat.min 16 (Nat.max 2 (List.size as Nat./ 4))) true f as

Remote.parMap.doc : Doc
Remote.parMap.doc =
  use Nat > increment
  use Remote parMap
  use docs run
  {{
  `` parMap f list `` takes a list of elements and a function to apply to each
  element. It performs the function in parallel over the elements and returns
  the transformed list respecting the order of the input list.

  ```
  parMapExample : '{Remote} [Nat]
  parMapExample = do
    parMap
      (n ->
        (if Nat.isEven n then
          Remote.sleepMicroseconds 20
          increment n
        else increment n)) [1, 2, 3, 4, 5]
  run parMapExample
  ```

  If __any__ of the tasks fail, the overall result of {parMap} will be a
  failure.

  ```
  parMapFailure : '{Remote} [Nat]
  parMapFailure =
    do
      parMap
        (n ->
          (if n > 0 then increment n else Remote.fail (failure "oh no!" ())))
        [3, 2, 1, 0]
  run parMapFailure
  ```

  **See also**

  * {Remote.both} for running two computations in parallel and returning a
    tuple of their results
  * {raceMap} for applying a function to each element of a list in parallel and
    returning the result of the first to complete
  }}

Remote.parMap.impl : Nat -> Boolean -> (a ->{Remote} b) -> [a] ->{Remote} [b]
Remote.parMap.impl parAwaitChunkSize parAwait f = cases
  [] -> []
  [a] -> [f a]
  as ->
    use List map
    use Remote.Promise write_
    result : Remote.Promise (Either Failure [b])
    result = Promise.empty()
    tasks =
      as |> (map cases
        a ->
          forkAt here! do
            addFinalizer cases
              Outcome.Failed f -> write_ result (Left f)
              _                -> ()
            f a)
    _ =
      forkAt here! do
        r =
          if parAwait then
            chunks = List.chunk parAwaitChunkSize tasks
            List.join
              (Remote.parMap.impl parAwaitChunkSize false (map await) chunks)
          else map await tasks
        write_ result (Right r)
    Remote.Promise.read result |> Remote.reraise

Remote.ping : Remote.Duration -> Location g ->{Remote} Boolean
Remote.ping t loc =
  isRight (Remote.try do Remote.timeout t do Remote.at loc do ())

Remote.Promise.delete : Remote.Promise a ->{Remote} ()
Remote.Promise.delete p = Remote.reraise (Promise.tryDelete p)

Remote.Promise.empty : '{Remote} Remote.Promise a
Remote.Promise.empty = do
  p = detached!
  finalizer do Promise.delete p
  p

Remote.Promise.id : Remote.Promise a -> Promise.Id
Remote.Promise.id = cases Promise id _ -> id

Remote.Promise.read : Remote.Promise a ->{Remote} a
Remote.Promise.read p = Remote.reraise (Remote.Promise.tryRead p)

Remote.Promise.readNow : Remote.Promise a ->{Remote} Optional a
Remote.Promise.readNow p = Remote.reraise (tryReadNow p)

Remote.Promise.write : Remote.Promise a -> a ->{Remote} Boolean
Remote.Promise.write p v = Remote.reraise (Promise.tryWrite p v)

Remote.Promise.write_ : Remote.Promise a -> a ->{Remote} ()
Remote.Promise.write_ p v =
  unison_base_3_22_0.ignore (Remote.Promise.write p v)

Remote.race : '{Remote} a -> '{Remote} b ->{Remote} Either a b
Remote.race l r =
  Remote.scope do
    use Remote reraise
    use Remote.Promise write_
    first = detached!
    finalizer do Promise.delete first
    ll = forkAt here! do
      out = tryScope l
      write_ first (Left out)
    rr = forkAt here! do
      out = tryScope r
      write_ first (Right out)
    Remote.Promise.read first
      |> Either.mapLeft reraise
      |> Either.mapRight reraise

Remote.race.doc : Doc
Remote.race.doc =
  use Remote sleep
  use Remote.Duration seconds
  use docs run
  {{
  `` race left right `` races two {type Remote} computations against each
  other. The winning result is captured in an {type Either} (if the ``
  Either.left `` computation completes before the ``Either.right``, {race} will
  return a value of {Left}.) The losing side of the race will be cancelled to
  avoid orphaned long-running tasks.

  ```
  turtle : '{Remote} Text
  turtle = do
    sleep (seconds 1)
    "🐢"
  hare : '{Remote} Text
  hare = do "🐇"
  run do race hare turtle
  ```

  The behavior of {race} when one of the arguments fails depends on whether the
  successful task has been able to complete before the failure is raised.

  The following call to {race} will return a failure overall because failure
  happens before the slower computation is able to finish.

  ```
  turtle : '{Remote} Text
  turtle = do
    sleep (seconds 1)
    "🐢"
  hare : '{Remote} Text
  hare = do Remote.fail (failure "💤 oh no!" ())
  run do race turtle hare
  ```

  **See also**

  * {Remote.both} for running two computations in parallel and returning a
    tuple of their results
  * {raceMap} for returning the result of the first remote operation to
    complete when applied to a list
  }}

Remote.raceMap : (a ->{Remote} b) -> List.Nonempty a ->{Remote} b
Remote.raceMap f xs =
  xs
    |> List.Nonempty.foldMap (l r -> do Either.unwrap (race l r)) (a -> do f a)
    |> force

Remote.raceMap.doc : Doc
Remote.raceMap.doc =
  use Nat >
  use Nonempty Nonempty
  use Remote sleep
  use docs run
  {{
  `` raceMap f list `` takes a list of elements and a function to apply to each
  element. It performs the function in parallel over the elements and returns
  the result of the first task to complete.

  The remaining running tasks will be cancelled.

  ```
  raceMapExample : '{Remote} Nat
  raceMapExample = do
    raceMap
      (n -> let
        sleep (millis n)
        n) (Nonempty 5 [4, 3, 2, 1])
  run raceMapExample
  ```

  {raceMap} will return a failure overall if the failure happens before any of
  the other tasks complete.

  ```
  raceMapFailure : '{Remote} Nat
  raceMapFailure = do
    raceMap
      (n ->
        (if n > 1 then
          sleep (millis n)
          n
        else Remote.fail (failure "oh no!" ()))) (Nonempty 5 [4, 3, 2, 1])
  run raceMapFailure
  ```
  }}

Remote.randomly : '{g, Random} t ->{g, Remote} t
Remote.randomly p =
  use List +:
  use Remote randomBytes
  go = cases
    { a } -> a
    { Random.nat! -> k } ->
      match decodeNat64be (randomBytes 8) with
        Some (seed, _) -> handle k seed with go
        None -> bug "Remote.randomBytes 8 returned fewer than 8 bytes"
    { Random.bytes n -> k } ->
      bs = randomBytes n
      handle k bs with go
    { split! -> k } ->
      rngs rem = match decodeNat64be rem with
        Some (seed, rem) -> fromSplitmix seed +: rngs rem
        None             -> []
      match rngs (randomBytes 32) with
        [a, b, c, d] ->
          rng = mix (mix a b) (mix c d)
          handle k (RNG.run rng) with go
        _            -> bug "Remote.randomBytes 32 didn't return 32 bytes"
  handle p() with go

Remote.randomly.doc : Doc
Remote.randomly.doc =
  {{
  `` randomly a `` evaluates `a`, providing it with a source of randomness.

  ```
  docs.run do randomly do List.replicate 4 do Random.natIn 0 100
  ```

  See {rng}.
  }}

Remote.Ref.cas : Remote.Ref a -> Remote.Ref.Ticket a -> a ->{Remote} Boolean
Remote.Ref.cas r ticket a = Remote.reraise (tryCas r ticket a)

Remote.Ref.delete : Remote.Ref a ->{Remote} ()
Remote.Ref.delete r = Remote.reraise (Ref.tryDelete r)

Remote.Ref.getThenUpdate : Remote.Ref a -> (a -> a) ->{Remote} a
Remote.Ref.getThenUpdate ref f = Remote.Ref.modify ref (a -> (f a, a))

Remote.Ref.id : Remote.Ref a -> Ref.Id
Remote.Ref.id = cases Ref id _ -> id

Remote.Ref.locationId : Remote.Ref a -> Location.Id
Remote.Ref.locationId = cases Ref _ l -> l

Remote.Ref.modify : Remote.Ref a -> (a -> (a, b)) ->{Remote} b
Remote.Ref.modify r f =
  use Nat * +
  use Remote.Ref cas readForCas
  if Location.locationId here! === Ref.locationId r then
    go = do
      (t, a0) = readForCas r
      (a, out) = f a0
      if cas r t a then out else go()
    go()
  else
    base = 3000
    go k micros =
      (t, a0) = readForCas r
      (a, out) = f a0
      if cas r t a then out
      else
        micros = randomly do base * Random.natIn 0 (Nat.pow 2 k)
        Remote.sleepMicroseconds micros
        go (k + 1) micros
    await (forkAt (Location (Ref.locationId r) []) do go 1 base)

Remote.Ref.new : a ->{Remote} Remote.Ref a
Remote.Ref.new a =
  r = detached a
  finalizer do Ref.delete r
  r

Remote.Ref.read : Remote.Ref a ->{Remote} a
Remote.Ref.read r = at2 (Remote.Ref.readForCas r)

Remote.Ref.readForCas : Remote.Ref a ->{Remote} (Remote.Ref.Ticket a, a)
Remote.Ref.readForCas r = Remote.reraise (tryReadForCas r)

Remote.Ref.update : Remote.Ref a -> (a -> a) ->{Remote} ()
Remote.Ref.update ref f = Remote.Ref.modify ref (a -> (f a, ()))

Remote.Ref.updateThenGet : Remote.Ref a -> (a -> a) ->{Remote} a
Remote.Ref.updateThenGet ref f =
  Remote.Ref.modify
    ref (a -> let
      b = f a
      (b, b))

Remote.Ref.write : Remote.Ref a -> a ->{Remote} ()
Remote.Ref.write r a = Remote.reraise (Ref.tryWrite r a)

Remote.reraise : Either Failure b ->{Remote} b
Remote.reraise = cases
  Left e  -> Remote.fail e
  Right a -> a

Remote.reraise! : '{g, Exception, Remote} a ->{g, Remote} a
Remote.reraise! e = Remote.reraise (catch e)

Remote.rng : '{Remote} RNG
Remote.rng = do match decodeNat64be (Remote.randomBytes 8) with
  Some (seed, _) -> fromSplitmix seed
  None           -> bug "Remote.randomBytes 8 returned fewer than 8 bytes"

Remote.rng.doc : Doc
Remote.rng.doc =
  {{
  Obtain a {type RNG}, seeded using {Remote.randomBytes}

  ```
  docs.run do
    r = rng()
    RNG.run r do List.replicate 5 do Random.natIn 0 10
  ```
  }}

Remote.run.Interrupt.checkInterrupt : run.Interrupt ->{IO} Boolean
Remote.run.Interrupt.checkInterrupt = cases
  run.Interrupt.Interrupt interrupt _ ->
    concurrent.Promise.tryRead interrupt |> isSome

Remote.run.Interrupt.interruptAndAwaitFinalization : run.Interrupt ->{IO} ()
Remote.run.Interrupt.interruptAndAwaitFinalization = cases
  run.Interrupt.Interrupt triggerInterruption awaitFinalization ->
    concurrent.Promise.write_ triggerInterruption ()
    concurrent.Promise.read awaitFinalization

Remote.run.Interrupt.interruptibly :
  run.Interrupt -> '{IO, Exception} t ->{IO, Exception, Abort} t
Remote.run.Interrupt.interruptibly = cases
  run.Interrupt.Interrupt interrupt _, action ->
    use concurrent fork
    use concurrent.Promise read write_
    use kill impl
    outcome = Promise.new()
    left = fork do read interrupt |> Left |> write_ outcome
    right = fork do catchAll action |> Right |> write_ outcome
    match read outcome with
      Left _  ->
        _ = impl right
        Abort.abort
      Right a ->
        _ = impl left
        Either.toException a

Remote.run.Interrupt.signalFinalization : run.Interrupt ->{IO} ()
Remote.run.Interrupt.signalFinalization = cases
  run.Interrupt.Interrupt _ signal -> concurrent.Promise.write_ signal ()

Remote.run.pure :
  (∀ g x. '{g} x ->{Scope s} x)
  -> '{Remote} a
  ->{Random, Scope s} Either Failure a
Remote.run.pure handleRest prog =
  use Either mapRight
  use List +: foreach
  use Location locationId
  use Map toList
  use Nat == toInt
  use Remote fail
  use data Map
  use data.Map empty
  use modify deprecated
  use mutable.Ref read write
  use unison_base_3_22_0 ignore
  insert : mutable.Ref {Scope s} (Map k v) -> k -> v -> ()
  insert ref k v = deprecated ref (Map.insert k v)
  delete : mutable.Ref {Scope s} (Map k v) -> k -> ()
  delete ref k = deprecated ref (data.Map.delete k)
  update : mutable.Ref {Scope s} (Map k v) -> k -> (v ->{g} v) -> ()
  update ref k f = deprecated ref (Map.adjust f k)
  get : mutable.Ref {Scope s} (Map k v) -> k -> Optional v
  get ref k = read ref |> Map.get k
  rng = rng.scope()
  newId : '{Scope s} UID
  newId = do rng UID.random
  here : Location {}
  here = Location (LocationId newId()) []
  clock : mutable.Ref {Scope s} Nat
  clock = Scope.ref 0
  runQueue :
    mutable.Ref {Scope s} (Map (Nat, UID) (Thread.Id, '{Remote, Scope s} Any))
  runQueue = Scope.ref empty
  schedule : Thread.Id -> '{Remote, Scope s} Any -> ()
  schedule thread thunk = insert runQueue (read clock, newId()) (thread, thunk)
  sleep : Nat -> Thread.Id -> '{Remote} Any -> '{Scope s} ()
  sleep delay thread resume =
    use Nat +
    rnd = newId()
    scheduledAt = read clock + delay
    insert runQueue (scheduledAt, rnd) (thread, resume)
    cancelAction = do delete runQueue (scheduledAt, rnd)
    cancelAction
  refs : mutable.Ref {Scope s} (Map Ref.Id (Nat, Any))
  refs = Scope.ref empty
  newRef : ∀ a. a -> Remote.Ref a
  newRef a =
    id = Ref.Id.Id newId()
    insert refs id (0, Any a)
    Ref id (locationId here)
  promises :
    mutable.Ref
      {Scope s}
      (Map
        Promise.Id
        (Either
          (Map Thread.Id (Either Failure Any -> '{Remote, Scope s} Any))
          (Either Failure Any)))
  promises = Scope.ref empty
  newPromise : ∀ a. '{Scope s} Remote.Promise a
  newPromise = do
    id = Promise.Id.Id newId()
    insert promises id (Left empty)
    Promise id (locationId here)
  writePromise : ∀ a. Remote.Promise a -> a -> Either Failure Boolean
  writePromise promise value =
    id = Promise.id promise
    match get promises id with
      None -> Left (unknownPromise id)
      Some (Right _) -> Right false
      Some (Left waiters) ->
        v = Right (Any value)
        waiters
          |> toList
          |> (foreach cases (t, resume) -> schedule t (resume v))
        insert promises id (Right v)
        Right true
  readPromise :
    ∀ a.
      Remote.Promise a
      -> Thread.Id
      -> (Either Failure a ->{Remote, Scope s} Any)
      -> '{Scope s} ()
  readPromise promise thread resume =
    id = Promise.id promise
    match get promises id with
      None -> schedule thread do resume (Left (unknownPromise id))
      Some (Left waiters) ->
        update promises id do
          waiters
            |> Map.insert thread (v -> do resume (mapRight unsafeExtract v))
            |> Left
      Some (Right a) ->
        out = mapRight unsafeExtract a
        schedule thread do resume out
    cancelAction = do
      update promises id cases
        Right v      -> Right v
        Left waiters -> waiters |> data.Map.delete thread |> Left
    cancelAction
  readPromise' :
    ∀ a.
      Remote.Promise a
      -> Thread.Id
      -> (a ->{Remote, Scope s} Any)
      -> '{Scope s} ()
  readPromise' promise thread resume = readPromise promise thread cases
    Left _  -> bug "bug: promise not found"
    Right v -> resume v
  readPromiseNow : ∀ a. Remote.Promise a -> Either Failure (Optional a)
  readPromiseNow promise =
    id = Promise.id promise
    match get promises id with
      None                   -> Left (unknownPromise id)
      Some (Left _)          -> Right None
      Some (Right (Left f))  -> Left f
      Some (Right (Right a)) -> Right (Some (unsafeExtract a))
  readPromiseNow' : ∀ a. Remote.Promise a -> Optional a
  readPromiseNow' promise = match readPromiseNow promise with
    Left _  -> bug "bug: promise not found"
    Right a -> a
  deletePromise : ∀ a. Remote.Promise a -> ()
  deletePromise promise =
    id = Promise.id promise
    match get promises id with
      None -> ()
      Some (Right _) -> delete promises id
      Some (Left waiters) ->
        v = Left (promiseDeleted id)
        waiters
          |> toList
          |> (foreach cases (t, resume) -> schedule t (resume v))
        delete promises id
  threads : mutable.Ref {Scope s} (Map Thread.Id (ThreadState s))
  threads = Scope.ref empty
  spawnThread : ∀ a. '{Remote, Scope s} a -> Thread.Id
  spawnThread prog =
    thread = Thread.Id.Id newId()
    completed = newPromise()
    insert threads thread (ThreadState Running completed [])
    schedule thread do Any prog()
    thread
  failNoThread : Optional (ThreadState s) -> ThreadState s
  failNoThread = getOrBug "Bug: thread not found"
  threadCompleted : Thread.Id -> Remote.Promise (Optional (Either Failure Any))
  threadCompleted thread =
    get threads thread |> failNoThread |> ThreadState.completed
  threadStatus : Thread.Id -> ThreadStatus s
  threadStatus thread =
    get threads thread |> failNoThread |> ThreadState.status
  threadFinalizers : Thread.Id -> [Outcome ->{Remote, Scope s} ()]
  threadFinalizers thread = get threads thread |> failNoThread |> finalizers
  rootThread = spawnThread prog
  scopeThread :
    ∀ a. '{Remote} a -> Thread.Id -> (Either Failure a ->{Remote} Any) -> ()
  scopeThread thunk thread resume =
    scopeThread = spawnThread thunk
    onScopeComplete = cases
      None     -> Any allowCancel!
      Some res -> resume (mapRight unsafeExtract res)
    _ = readPromise' (threadCompleted scopeThread) thread onScopeComplete
    update threads thread (status.set (Scoped (Function.id scopeThread)))
  blockThread :
    Thread.Id -> resume -> (Thread.Id -> resume -> '{Scope s} ()) -> ()
  blockThread thread resume blockingAction =
    scheduleCancellation = do schedule thread do Any allowCancel!
    match threadStatus thread with
      CancellationRequested -> scheduleCancellation()
      _                     ->
        stopBlocking = blockingAction thread resume
        cancelAction = do
          stopBlocking()
          scheduleCancellation()
        update threads thread (status.set (Blocking cancelAction))
  cancelThread : Thread.Id -> Thread.Id -> '{Remote} Any -> ()
  cancelThread childThread thread resume =
    cancel thread = match get threads thread with
      None                                  -> None
      Some (ThreadState status completed _) ->
        match status with
          CancellationRequested -> ()
          Running               -> ()
          Blocking cancelAction -> cancelAction()
          Scoped subThread      -> ignore (cancel subThread)
        update threads thread (status.set CancellationRequested)
        Some completed
    match cancel childThread with
      None -> schedule thread resume
      Some childCompleted ->
        ignore (readPromise' childCompleted thread do resume())
  terminateThread : Thread.Id -> Optional (Either Failure Any) -> ()
  terminateThread thread result = match threadFinalizers thread with
    []   ->
      completed' = threadCompleted thread
      ignore (writePromise completed' result)
      when (thread !== rootThread) do
        deletePromise completed'
        delete threads thread
    fins ->
      update threads thread (finalizers.set [])
      let
        (outcome, onComplete) =
          match result with
            Some (Left failure) -> (Outcome.Failed failure, _ -> fail failure)
            Some (Right result) -> (Completed, _ -> result)
            None                -> (Cancelled, _ -> Any allowCancel!)
        finalizer = spawnThread do List.map (f -> Remote.try do f outcome) fins
        ignore (readPromise (threadCompleted finalizer) thread onComplete)
  go : '{Scope s} Either Failure a
  go =
    do
      match Map.breakOffMin (read runQueue) with
        None ->
          deadlock =
            Left (Failure (typeLink Cancelled) "Deadlock!" (Any rootThread))
          readPromiseNow' (threadCompleted rootThread)
            |> Optional.flatten
            |> Optional.getOrElse deadlock
            |> mapRight unsafeExtract
        Some (((scheduledAt, _), (thread, prog)), runQueueNow) ->
          write runQueue runQueueNow
          write clock scheduledAt
          update
            threads thread (status.modify cases
              Blocking _ -> Running
              Scoped _   -> Running
              s          -> s)
          handle handleRest prog
          with cases
            { here! -> resume }                          ->
              schedule thread do resume here
              go()
            { region! -> resume }                        ->
              schedule thread do resume here
              go()
            { tryNear loc1 loc2 -> resume }              ->
              schedule thread do resume (Right loc1)
              go()
            { tryFar loc1 loc2 -> resume }               ->
              schedule thread do resume (Right loc1)
              go()
            { monotonic! -> resume }                     ->
              out = microseconds (toInt (read clock))
              schedule thread do resume out
              go()
            { now! -> resume }                           ->
              use Instant +
              out = epoch + microseconds (toInt (read clock))
              schedule thread do resume out
              go()
            { Remote.sleepMicroseconds time -> resume }  ->
              blockThread thread resume (sleep time)
              go()
            { Remote.randomBytes n -> resume }           ->
              out = rng do Random.bytes n
              schedule thread do resume out
              go()
            { detached a -> resume }                     ->
              out = newRef a
              schedule thread do resume out
              go()
            { tryReadForCas ref -> resume }              ->
              id = Ref.id ref
              out = match get refs id with
                None              -> Left (unknownRef id)
                Some (version, a) -> Right (Ticket version, unsafeExtract a)
              schedule thread do resume out
              go()
            { Ref.tryWrite ref a -> resume }             ->
              use Nat +
              id = Ref.id ref
              out = match get refs id with
                None              -> Left (unknownRef id)
                Some (version, _) ->
                  insert refs id (version + 1, Any a)
                  Right()
              schedule thread do resume out
              go()
            { tryCas ref ticket a -> resume }            ->
              (Ticket seenAt) = ticket
              use Nat +
              id = Ref.id ref
              out = match get refs id with
                None              -> Left (unknownRef id)
                Some (version, _)
                  | version == seenAt ->
                    insert refs id (version + 1, Any a)
                    Right true
                  | otherwise         -> Right false
              schedule thread do resume out
              go()
            { Ref.tryDelete ref -> resume }              ->
              id = Ref.id ref
              delete refs id
              schedule thread do resume Right()
              go()
            { detached! -> resume }                      ->
              promise = newPromise()
              schedule thread do resume promise
              go()
            { Promise.tryWrite promise value -> resume } ->
              out = writePromise promise value
              schedule thread do resume out
              go()
            { tryReadNow promise -> resume }             ->
              out = readPromiseNow promise
              schedule thread do resume out
              go()
            { Remote.Promise.tryRead promise -> resume } ->
              blockThread thread resume (readPromise promise)
              go()
            { Promise.tryDelete promise -> resume }      ->
              deletePromise promise
              schedule thread do resume Right()
              go()
            { tryCancel childThread -> resume }          ->
              resume' = do resume Right()
              cancelThread (Thread.id childThread) thread resume'
              go()
            { tryDetachAt loc thunk -> resume }          ->
              childThread = spawnThread do handleRest do reraise! thunk
              out = Thread childThread (locationId here)
              schedule thread do resume (Right out)
              go()
            { addFinalizer finalizer -> resume }         ->
              update threads thread (finalizers.modify ((+:) finalizer))
              schedule thread resume
              go()
            { tryScope thunk -> resume }                 ->
              scopeThread thunk thread resume
              go()
            { allowCancel! -> resume }                   ->
              match threadStatus thread with
                CancellationRequested -> terminateThread thread None
                _                     -> schedule thread resume
              go()
            { fail f -> _ }                              ->
              terminateThread thread (Some (Left f))
              go()
            { a }                                        ->
              terminateThread thread (Some (Right a))
              go()
  go()

Remote.run.pure.ThreadState.completed :
  ThreadState s -> Remote.Promise (Optional (Either Failure Any))
Remote.run.pure.ThreadState.completed = cases
  ThreadState _ completed _ -> completed

Remote.run.pure.ThreadState.completed.modify :
  (Remote.Promise (Optional (Either Failure Any))
  ->{g} Remote.Promise (Optional (Either Failure Any)))
  -> ThreadState s
  ->{g} ThreadState s
Remote.run.pure.ThreadState.completed.modify f = cases
  ThreadState status completed finalizers ->
    ThreadState status (f completed) finalizers

Remote.run.pure.ThreadState.completed.set :
  Remote.Promise (Optional (Either Failure Any))
  -> ThreadState s
  -> ThreadState s
Remote.run.pure.ThreadState.completed.set completed1 = cases
  ThreadState status _ finalizers -> ThreadState status completed1 finalizers

Remote.run.pure.ThreadState.finalizers :
  ThreadState s -> [Outcome ->{Remote, Scope s} ()]
Remote.run.pure.ThreadState.finalizers = cases
  ThreadState _ _ finalizers -> finalizers

Remote.run.pure.ThreadState.finalizers.modify :
  ([Outcome ->{Remote, Scope s} ()] ->{g} [Outcome ->{Remote, Scope s} ()])
  -> ThreadState s
  ->{g} ThreadState s
Remote.run.pure.ThreadState.finalizers.modify f = cases
  ThreadState status completed finalizers ->
    ThreadState status completed (f finalizers)

Remote.run.pure.ThreadState.finalizers.set :
  [Outcome ->{Remote, Scope s} ()] -> ThreadState s -> ThreadState s
Remote.run.pure.ThreadState.finalizers.set finalizers1 = cases
  ThreadState status completed _ -> ThreadState status completed finalizers1

Remote.run.pure.ThreadState.status : ThreadState s -> ThreadStatus s
Remote.run.pure.ThreadState.status = cases ThreadState status _ _ -> status

Remote.run.pure.ThreadState.status.modify :
  (ThreadStatus s ->{g} ThreadStatus s) -> ThreadState s ->{g} ThreadState s
Remote.run.pure.ThreadState.status.modify f = cases
  ThreadState status completed finalizers ->
    ThreadState (f status) completed finalizers

Remote.run.pure.ThreadState.status.set :
  ThreadStatus s -> ThreadState s -> ThreadState s
Remote.run.pure.ThreadState.status.set status1 = cases
  ThreadState _ completed finalizers ->
    ThreadState status1 completed finalizers

Remote.run.threaded :
  (∀ g x. ctx -> run.Interrupt -> '{g} x ->{IO, Exception, Abort, Remote} x)
  ->{IO, Random} (∀ a. ctx -> run.Interrupt -> '{Remote} a ->{IO, Exception} a)
Remote.run.threaded handleRest =
  use Abort abort
  use Exception raise
  use List +:
  use Location locationId
  use Nat + ==
  use Outcome Failed
  use Remote scope
  use run Interrupt
  use run.Interrupt interruptibly
  use systemfw_concurrent_2_2_0 Map
  use systemfw_concurrent_2_2_0.Map alter delete empty lookup put
  rng : '{Random} a ->{IO} a
  rng = io()
  newId : '{IO} UID
  newId = do rng UID.random
  here : Location {}
  here = Location (LocationId newId()) []
  refs : Map Ref.Id (Nat, Any)
  refs = empty()
  promises : Map Promise.Id (concurrent.Promise (Either Failure Any))
  promises = empty()
  threads : Map Thread.Id Interrupt
  threads = empty()
  toResult : '{IO, Exception, Abort} a ->{IO} Optional (Either Failure a)
  toResult thunk =
    handle tryEval.impl (Abort.toOptional do catch thunk)
    with cases
      { raise todoOrBug -> _ } -> Some (Left todoOrBug)
      { x }                    -> x
  runFinalizers :
    ctx -> mutable.Ref {IO} [Outcome ->{Remote} ()] -> Outcome ->{IO} ()
  runFinalizers ctx finalizers outcome =
    _ =
      """
          Each finalizer runs in its own scope, uninterruptibly, and without
          propagating failure

      """
    run : (Outcome ->{Remote} ()) ->{IO} ()
    run finalizer =
      r =
        toResult do
          go ctx Interrupt.new() (IO.ref []) do scope do finalizer outcome
      unison_base_3_22_0.ignore (r : Optional (Either Failure ()))
    mutable.Ref.read finalizers |> List.foreach run
  go :
    ctx
    -> Interrupt
    -> mutable.Ref {IO} [Outcome ->{Remote} ()]
    -> '{g, Remote} a
    ->{g, IO, Exception, Abort} a
  go ctx interrupt finalizers p =
    handle handleRest ctx interrupt p
    with cases
      { here! -> resume } -> go ctx interrupt finalizers do resume here
      { region! -> resume } -> go ctx interrupt finalizers do resume here
      { tryNear loc1 _ -> resume } ->
        go ctx interrupt finalizers do resume (Right loc1)
      { tryFar loc1 _ -> resume } ->
        go ctx interrupt finalizers do resume (Right loc1)
      { now! -> resume } ->
        t = realtime()
        go ctx interrupt finalizers do resume t
      { monotonic! -> resume } ->
        t = Clock.monotonic()
        go ctx interrupt finalizers do resume t
      { Remote.sleepMicroseconds n -> resume } ->
        interruptibly interrupt do concurrent.sleepMicroseconds n
        go ctx interrupt finalizers resume
      { Remote.randomBytes n -> resume } ->
        out = rng do Random.bytes n
        go ctx interrupt finalizers do resume out
      { detached a -> resume } ->
        id = Ref.Id.Id newId()
        ref = Ref id (locationId here)
        put refs id (0, Any a)
        go ctx interrupt finalizers do resume ref
      { tryReadForCas ref -> resume } ->
        id = Ref.id ref
        out = match lookup refs id with
          None              -> Left (unknownRef id)
          Some (version, a) -> Right (Ticket version, unsafeExtract a)
        go ctx interrupt finalizers do resume out
      { Ref.tryWrite ref a -> resume } ->
        id = Ref.id ref
        out = alter refs id cases
          None              -> (NoChange, Left (unknownRef id))
          Some (version, _) -> (Put (version + 1, Any a), Right())
        go ctx interrupt finalizers do resume out
      { tryCas ref ticket a -> resume } ->
        (Ticket seenAt) = ticket
        id = Ref.id ref
        out = alter refs id cases
          None              -> (NoChange, Left (unknownRef id))
          Some (version, _)
            | version == seenAt -> (Put (version + 1, Any a), Right true)
            | otherwise         -> (NoChange, Right false)
        go ctx interrupt finalizers do resume out
      { Ref.tryDelete ref -> resume } ->
        delete refs (Ref.id ref)
        go ctx interrupt finalizers do resume Right()
      { detached! -> resume } ->
        id = Promise.Id.Id newId()
        promise = Promise.new()
        out = Promise id (locationId here)
        put promises id promise
        go ctx interrupt finalizers do resume out
      { Promise.tryWrite promise res -> resume } ->
        id = Promise.id promise
        out =
          match lookup promises id with
            None -> Left (unknownPromise id)
            Some promise ->
              Right (concurrent.Promise.write promise (Right (Any res)))
        go ctx interrupt finalizers do resume out
      { tryReadNow promise -> resume } ->
        id = Promise.id promise
        out = match lookup promises id with
          None         -> Left (unknownPromise id)
          Some promise ->
            match concurrent.Promise.tryRead promise with
              None           -> Right None
              Some (Left e)  -> Left e
              Some (Right a) -> Right (Some (unsafeExtract a))
        go ctx interrupt finalizers do resume out
      { Remote.Promise.tryRead promise -> resume } ->
        id = Promise.id promise
        out = match lookup promises id with
          None         -> Left (unknownPromise id)
          Some promise ->
            out = interruptibly interrupt do concurrent.Promise.read promise
            Either.mapRight unsafeExtract out
        go ctx interrupt finalizers do resume out
      { Promise.tryDelete promise -> resume } ->
        id = Promise.id promise
        notifyWaiters =
          alter promises id cases
            None -> (NoChange, do ())
            Some promise ->
              action =
                do concurrent.Promise.write_ promise (Left (promiseDeleted id))
              (Delete, action)
        notifyWaiters()
        go ctx interrupt finalizers do resume Right()
      { tryCancel thread -> resume } ->
        match lookup threads (Thread.id thread) with
          None -> ()
          Some interrupt ->
            run.Interrupt.interruptAndAwaitFinalization interrupt
        go ctx interrupt finalizers do resume Right()
      { tryDetachAt loc prog -> resume } ->
        id = Thread.Id.Id newId()
        thread = Thread id (locationId loc)
        childInterrupt = Interrupt.new()
        put threads id childInterrupt
        _ =
          concurrent.fork do
            scopedFinalizers = IO.ref []
            runProg =
              do go ctx childInterrupt scopedFinalizers (coerceAbilities prog)
            outcome = match toResult runProg with
              Some (Left failure) -> Failed failure
              Some (Right result) -> Completed
              None                -> Cancelled
            runFinalizers ctx scopedFinalizers outcome
            delete threads id
            run.Interrupt.signalFinalization childInterrupt
        go ctx interrupt finalizers do resume (Right thread)
      { addFinalizer f -> resume } ->
        ref.atomically finalizers (finalizers -> (f +: finalizers, ()))
        go ctx interrupt finalizers resume
      { tryScope prog -> resume } ->
        scopedFinalizers = IO.ref []
        runProg = toResult do go ctx interrupt scopedFinalizers prog
        match runProg with
          Some (Left failure) ->
            runFinalizers ctx scopedFinalizers (Failed failure)
            go ctx interrupt finalizers do resume (Left failure)
          Some (Right result) ->
            runFinalizers ctx scopedFinalizers Completed
            go ctx interrupt finalizers do resume (Right result)
          None                ->
            runFinalizers ctx scopedFinalizers Cancelled
            abort
      { allowCancel! -> resume } ->
        if run.Interrupt.checkInterrupt interrupt then abort
        else go ctx interrupt finalizers resume
      { Remote.fail failure -> _ } -> raise failure
      { a } -> a
  runner : ctx -> Interrupt -> '{Remote} a ->{IO, Exception} a
  runner ctx interrupt p =
    toDefault! (do raise cancelled) do go ctx interrupt (IO.ref []) do scope p
  runner

Remote.run.threaded.doc : Doc
Remote.run.threaded.doc =
  {{
  `Remote.local.run handler prog` runs {type Remote} actions locally,
  performing real effects and concurrency. To call `Remote.local.run` directly
  you have to pass a function to handle all the abilities used by any
  {type Location} you used in `prog`. Note that these aren't tracked for the
  type system, so you will have to call {coerceAbilities} on your handler
  function before passing it in, and `Remote.local.run` will fail at runtime if
  you forgot to handle something.

  This is a low-level building block, the
  [cloud client](https://share.unison-lang.org/@unison/cloud) contains higher
  level helpers to run Unison Cloud computations locally, and as a general
  guideline, should be preferred.
  }}

Remote.scope : '{Remote} a ->{Remote} a
Remote.scope = Remote.reraise << tryScope

Remote.sleep : Remote.Duration ->{Remote} ()
Remote.sleep t = Remote.sleepMicroseconds (toMicros t)

Remote.sleep.doc : Doc
Remote.sleep.doc =
  use Nat +
  use Remote sleep
  {{
  `` sleep duration `` will sleep the running task for the given
  {type Remote.Duration}.

  ```
  slow = do
    sleep (Remote.Duration.seconds 1)
    1 + 1
  fast = do
    sleep (millis 2)
    2 + 2
  docs.run do race slow fast |> Either.unwrap
  ```

  Note, the argument {type Remote.Duration} is a different type than the
  {type time.Duration} found in base. You can convert the {type Remote} version
  to its `base` counterpart with {toBaseDuration} or get its value in raw
  microseconds with {toMicros}.
  }}

Remote.Task.doc : Doc
Remote.Task.doc =
  use Remote fork
  {{
  A {type Task} represents the future result of a computation running at an
  abstract {type Location} in the {type Remote} context. It's a powerful
  primitive in the {type Remote} library that allows you to create and manage
  concurrent computations.

  Tasks are created with functions like {fork} and {forkAt} and unwrapped with
  functions like {await}. {{
  docAside
    {{
    This means you shouldn't have to manually create tasks with the {type Task}
    data constructor in the process of writing distributed programs.
    }} }}

  ```
  forkedTasks : '{Remote} Nat
  forkedTasks = do
    use Nat +
    use unison_base_3_22_0 ignore
    task1 = fork here! do
      ignore "do some work"
      1 + 1
    task2 = fork here! do
      ignore "do some other work"
      2 + 2
    await task1 + await task2
  docs.run forkedTasks
  ```

  A few important properties of tasks:

  * Tasks are asynchronous.
    * They begin executing immediately in the background, allowing the program
      to continue with other work.
  * Tasks can create other tasks.
    * A task can spawn many child tasks, but the successful completion of the
      parent task is not dependent upon all of its children returning values.
  * Tasks are cancellable
    * Cancelling a parent task triggers the cancellation of its children. This
      ensures that child tasks are not left running and consuming resources
      when they are no longer needed.

  **See also**

  * {forkAt} for spawning a task at a specific location
  * {await} for waiting for a task to complete
  * {Remote.cancel} for cancelling a task
  * {Task.pure} for creating a task that immediately completes with a value
  }}

Remote.Task.promise : Task a -> Remote.Promise (Either Failure a)
Remote.Task.promise = cases Task _ p -> p

Remote.Task.pure : a ->{Remote} Task a
Remote.Task.pure a = forkAt here! do a

Remote.Task.pure.doc : Doc
Remote.Task.pure.doc =
  use Task pure
  {{
  `` pure a `` creates a task that returns `a` on {await}.

  That is, {{ docExample 1 do a -> await (pure a) === a }}
  }}

Remote.Task.resultLocation : Task a -> Location {}
Remote.Task.resultLocation = cases
  Task _ (Promise _ locId) -> Location locId []

Remote.Task.thread : Task a -> Thread
Remote.Task.thread = cases Task th _ -> th

Remote.Task.workLocation : Task a -> Location {}
Remote.Task.workLocation = cases Task (Thread _ locId) _ -> Location locId []

Remote.test.assertions.allowCancel : Exists assertions.Test
Remote.test.assertions.allowCancel =
  use Nat >
  label = "allowCancel allows an interrupt"
  program = do
    reraise! do
      finalN = Promise.empty()
      forkedTask = forkAt region! do
        currentN = Ref.new 0
        addFinalizer do Remote.Promise.write_ finalN (Remote.Ref.read currentN)
        go n =
          use Nat <
          ensuring do n < maxNat
          n' = Nat.increment n
          Remote.Ref.write currentN n'
          allowCancel!
          go n'
        go 0
      Remote.cancel forkedTask
      finalNValue = Promise.readNow finalN
      ensuring do Optional.exists (n -> n > 0) finalNValue
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.assertUnknownPromiseFailure :
  Either Failure a ->{Exception} ()
Remote.test.assertions.assertUnknownPromiseFailure = cases
  Left (Failure t _ _)| t === typeLink UnknownPromise  -> ()
  other -> test.raiseFailure "Expected UnknownPromise failure" other

Remote.test.assertions.awaitAwaitsFinalization : Exists assertions.Test
Remote.test.assertions.awaitAwaitsFinalization =
  label = "await awaits finalization"
  program = do
    reraise! do
      finalizerCompleted = Ref.new false
      forkedTask = forkAt region! do
        addFinalizer do
          Remote.sleep (Remote.Duration.seconds 2)
          Remote.Ref.write finalizerCompleted true
      await forkedTask
      ensuring do Remote.Ref.read finalizerCompleted
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.basicCancellation : Exists assertions.Test
Remote.test.assertions.basicCancellation =
  label = "basic cancellation"
  program = do
    e = longRunningTask()
    Remote.cancel e
    Remote.try do isComplete e
  check isCompleteResponse =
    assertUnknownPromiseFailure (Either.toException isCompleteResponse)
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.basicProgram : Exists assertions.Test
Remote.test.assertions.basicProgram =
  use Nat ==
  label = "basic program that doesn't use Remote"
  program = do
    use Nat +
    x = 1
    y = 2
    x + y
  check r = ensuring do Either.toException r == 3
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.bugInFork : Exists assertions.Test
Remote.test.assertions.bugInFork =
  label = "bug in fork is returned as a normal failure"
  program =
    do
      reraise! do
        res = (forkAt region! do bug "blargh") |> tryAwait
        ensuring do
          match res with
            Left (Failure errType msg any)| errType === typeLink RuntimeFailure
              && msg === "builtin.bug"
              && any === Any "blargh"  ->
              true
            _ -> false
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.bugInTryScope : Exists assertions.Test
Remote.test.assertions.bugInTryScope =
  label = "bug in tryScope is returned as a normal failure"
  program =
    do
      reraise! do
        res = tryScope do bug "blargh"
        ensuring do
          match res with
            Left (Failure errType msg any)| errType === typeLink RuntimeFailure
              && msg === "builtin.bug"
              && any === Any "blargh"  ->
              true
            _ -> false
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.cancelAfterAwait : Exists assertions.Test
Remote.test.assertions.cancelAfterAwait =
  label = "cancel after await"
  program = do
    forkedThread = forkAt region! do Remote.sleep (millis 100)
    await forkedThread
    Remote.cancel forkedThread
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.cancelAwait : Exists assertions.Test
Remote.test.assertions.cancelAwait =
  use Duration - < >=
  label = "cancel await"
  program = do
    reraise! do
      start = monotonic!
      awaitingTask = forkAt region! do longRunningTask() |> await
      Remote.cancel awaitingTask
      stop = monotonic!
      elapsed = stop - start
      ensuring do elapsed >= time.Duration.zero
      ensuring do elapsed < time.Duration.seconds +10
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.cancelAwaitsFinalization : Exists assertions.Test
Remote.test.assertions.cancelAwaitsFinalization =
  label = "cancel awaits finalization"
  program = do
    reraise! do
      finalizerCompleted = Ref.new false
      forkedTask = forkAt region! do
        addFinalizer do
          Remote.sleep (Remote.Duration.seconds 2)
          Remote.Ref.write finalizerCompleted true
        sleepForever()
      Remote.cancel forkedTask
      ensuring do Remote.Ref.read finalizerCompleted
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.childrenGetCanceled : Exists assertions.Test
Remote.test.assertions.childrenGetCanceled =
  label = "children get canceled"
  program = do
    parent : Task (Task Void)
    parent = forkAt region! neverEndingTask
    child = awaitRetain parent
    Remote.cancel parent
    Remote.try do isComplete child
  check isCompleteResponse =
    assertUnknownPromiseFailure (Either.toException isCompleteResponse)
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.forkAndAwait : Exists assertions.Test
Remote.test.assertions.forkAndAwait =
  use Nat ==
  label = "Remote ops: forkAt, near, far, await"
  program = do
    use Nat +
    startNode = here!
    t1 = forkAt (Remote.near region! startNode) do 1 + 2
    t2 = forkAt (Remote.far region! startNode) do 3 + 4
    await t1 + await t2
  check r = ensuring do Either.toException r == 10
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.forkFinalizersRuntimeException : Exists assertions.Test
Remote.test.assertions.forkFinalizersRuntimeException =
  label = "fork finalizers run when there is a runtime exception"
  program = do
    reraise! do
      finalizerCompleted = Ref.new false
      res =
        tryAwait <| (forkAt region! do
          addFinalizer do Remote.Ref.write finalizerCompleted true
          bug "blargh")
      ensuring do Remote.Ref.read finalizerCompleted
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.forkHereIsHere : Exists assertions.Test
Remote.test.assertions.forkHereIsHere =
  label = "forking here leaves you in the same location"
  program = do
    startNode = here!
    forkedNode = (forkAt startNode do here!) |> await
    (startNode, forkedNode)
  check r = ensuring do
    (startNode, forkedNode) = Either.toException r
    startNode === forkedNode
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.idAllocationInFinalizers : Exists assertions.Test
Remote.test.assertions.idAllocationInFinalizers =
  use Promise empty
  use Ref delete
  use Remote.Promise read write_
  label =
    "finalizers should run with unique ID generation (they shouldn't reuse a seed)"
  program = do
    reraise! do
      ref1Promise = empty()
      ref2Promise = empty()
      addFinalizer do
        read ref1Promise |> delete
        read ref2Promise |> delete
      Remote.scope do
        addFinalizer do
          write_ ref1Promise (detached 1)
          write_ ref2Promise (detached 2)
      ref1 = read ref1Promise
      ref2 = read ref2Promise
      ensuring do toDebugText ref1 !== toDebugText ref2
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.impure : '{IO, Exception} [Exists assertions.Test]
Remote.test.assertions.impure =
  do
    [ bugInFork
    , bugInTryScope
    , forkFinalizersRuntimeException
    , scopeFinalizersRuntimeException
    , nowIsReasonable realtime()
    ]

Remote.test.assertions.nowIsReasonable : Instant -> Exists assertions.Test
Remote.test.assertions.nowIsReasonable clientTime =
  label = "Remote.time.now should be reasonable"
  program = do
    t1 = now!
    Remote.sleep (Remote.Duration.seconds 1)
    t2 = now!
    (t1, t2)
  check r =
    use Duration >
    use Instant <
    isNowish t =
      time.Duration.seconds +60 > Duration.abs (between clientTime t)
    let
      (t1, t2) = Either.toException r
      ensuring do t1 < t2
      ensuring do isNowish t1
      ensuring do isNowish t2
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.pure : [Exists assertions.Test]
Remote.test.assertions.pure =
  [ basicCancellation
  , basicProgram
  , assertions.allowCancel
  , assertions.cancelAfterAwait
  , assertions.cancelAwait
  , cancelAwaitsFinalization
  , awaitAwaitsFinalization
  , childrenGetCanceled
  , forkAndAwait
  , forkHereIsHere
  , idAllocationInFinalizers
  , randomBytesAreReasonable
  , refDeleteNonexistent
  , remoteCancellation
  , scopeRunsSyncFinalization
  , scopePropagatesInterruption
  ]

Remote.test.assertions.pure.doc : Doc
Remote.test.assertions.pure.doc =
  {{
  A collection of [Tests]({type assertions.Test}) that should pass for any
  {type Remote} handler.
  }}

Remote.test.assertions.randomBytesAreReasonable : Exists assertions.Test
Remote.test.assertions.randomBytesAreReasonable =
  use Bytes size
  label = "The output of randomBytes should be reasonable"
  program = do
    use Remote randomBytes
    b1 = randomBytes 16
    b2 = randomBytes 32
    (b1, b2)
  check r =
    (b1, b2) = Either.toException r
    ensuring do size b1 === 16
    ensuring do size b2 === 32
    ensuring do Bytes.take 16 b2 !== b1
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.refDeleteNonexistent : Exists assertions.Test
Remote.test.assertions.refDeleteNonexistent =
  label = "delete on a nonexistent ref doesn't fail"
  program = do
    use Ref delete
    ref = Ref.new 0
    delete ref
    delete ref
    Remote.sleep (Remote.Duration.seconds 2)
    delete ref
  assertions.Test.Test label program Either.toException |> Exists.wrap

Remote.test.assertions.remoteCancellation : Exists assertions.Test
Remote.test.assertions.remoteCancellation =
  label = "remote cancellation"
  program = do
    t1 = forkAt (Remote.far region! here!) do Remote.sleepMicroseconds 60000000
    Remote.cancel t1
    isComplete t1
  check isCompleteResponse = assertUnknownPromiseFailure isCompleteResponse
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.run :
  (∀ r. '{Remote} r ->{g, Exception} r)
  -> [Exists assertions.Test]
  ->{g} [Result]
Remote.test.assertions.run runRemote tests =
  run : assertions.Test a ->{g} Result
  run = cases
    assertions.Test.Test label program check ->
      runLabeled label do check (catch do runRemote program)
  List.map (Exists.apply run) tests

Remote.test.assertions.runAllLocally : '{IO, Exception} [Result]
Remote.test.assertions.runAllLocally =
  do assertions.run test.run (assertions.pure List.++ impure())

test> Remote.test.assertions.runAllPure =
  assertions.run (docs.run >> Either.toException) assertions.pure

Remote.test.assertions.scopeFinalizersRuntimeException : Exists assertions.Test
Remote.test.assertions.scopeFinalizersRuntimeException =
  label = "scope finalizers run when there is a runtime exception"
  program = do
    reraise! do
      finalizerCompleted = Ref.new false
      res = tryScope do
        addFinalizer do Remote.Ref.write finalizerCompleted true
        bug "blargh"
      ensuring do Remote.Ref.read finalizerCompleted
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.scopePropagatesInterruption : Exists assertions.Test
Remote.test.assertions.scopePropagatesInterruption =
  label = "Remote.scope propagates interruption"
  program = do
    use Promise empty
    use Remote detachAt
    use Remote.Promise read
    started = empty()
    done = empty()
    task = do
      use Remote.Promise write_
      addFinalizer (outcome -> write_ done outcome)
      write_ started ()
      _ = tryScope do Remote.sleep (Remote.Duration.seconds 3)
      ()
    thread = detachAt here! task
    _ = read started
    _ = detachAt here! do Thread.cancel thread
    outcome = read done
    reraise! do test.ensureEqual outcome Cancelled
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.assertions.scopeRunsSyncFinalization : Exists assertions.Test
Remote.test.assertions.scopeRunsSyncFinalization =
  label = "scope runs finalization synchronously"
  program = do
    reraise! do
      finalizerCompleted = Ref.new false
      Remote.scope do
        addFinalizer do Remote.Ref.write finalizerCompleted true
        Remote.sleep (Remote.Duration.seconds 2)
      ensuring do Remote.Ref.read finalizerCompleted
  check r = Either.toException r
  assertions.Test.Test label program check |> Exists.wrap

Remote.test.longRunningTask : '{Remote} Task ()
Remote.test.longRunningTask =
  do forkAt region! do Remote.sleep (Remote.Duration.seconds 59)

Remote.test.neverEndingTask : '{Remote} Task Void
Remote.test.neverEndingTask = do forkAt region! sleepForever

Remote.test.real.allowCancel : '{Remote} ()
Remote.test.real.allowCancel = do
  eval testPool do
    use Remote sleep
    use TestLog log
    log "Forking"
    t = Remote.fork testPool do
      Each.run do
        i = Each.range 1 5000
        allowCancel!
        log (Nat.toText i)
    sleep (millis 2)
    Remote.cancel t
    log "Sleeping to observe cancellation"
    sleep (Remote.Duration.seconds 2)

Remote.test.real.cancelAfterAwait : '{Remote} ()
Remote.test.real.cancelAfterAwait = do
  eval testPool do
    use TestLog log
    t = Remote.fork testPool do
      log "Started task"
      Remote.sleep (Remote.Duration.seconds 1)
    _ = tryAwait t
    log "Task complete"
    Remote.cancel t
    log "cancelled task"
    log "end"

Remote.test.real.cancelAwait : '{Remote} ()
Remote.test.real.cancelAwait = do
  eval testPool do
    use Remote fork sleep
    use Remote.Duration seconds
    use TestLog log
    t = fork testPool do
      log "Awaiting a long task"
      lt = fork testPool do
        Each.run do
          i = Each.range 1 50
          sleep (seconds 1)
          log (Nat.toText i)
      await lt
    sleep (seconds 4)
    log "Cancelling"
    Remote.cancel t
    log "Sleeping to observe cancellation"
    sleep (seconds 2)

Remote.test.real.cancelAwaitsFinalizers : '{Remote} ()
Remote.test.real.cancelAwaitsFinalizers = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    action = do
      log "Waiting"
      Each.run do
        i = Each.range 1 50
        sleep (millis 200)
        log (Nat.toText i)
    whenDone = do
      log "thread canceling..."
      sleep (seconds 3)
      log "thread finalized"
    log "Starting"
    t = Remote.fork testPool do
      finalizer do eval testPool whenDone
      action()
    sleep (seconds 2)
    log "Cancellation initiated"
    Remote.cancel t
    log "Cancellation completed"
    log "Sleeping to observe cancellation"
    sleep (seconds 2)

Remote.test.real.cancelNotification : '{Remote} Either Failure ()
Remote.test.real.cancelNotification = do
  eval testPool do
    use Remote fork sleep
    use Remote.Duration seconds
    use TestLog log
    log "Sleep then timeout"
    t1 = fork testPool do
      finalizer do eval testPool do log "finalized"
      sleep (seconds 50)
    timeout = fork testPool do
      sleep (seconds 2)
      Remote.cancel t1
    Remote.try do await t1

Remote.test.real.closedScope : '{Remote} Either Failure Text
Remote.test.real.closedScope = do
  eval testPool do
    use TestLog log
    log "Spawning"
    t = Remote.scope do
      Remote.fork testPool do
        Remote.sleep (Remote.Duration.seconds 2)
        log "done"
        "done"
    log "Scope ended"
    Remote.try do await t

Remote.test.real.delayedForkAwait : '{Remote} Text
Remote.test.real.delayedForkAwait = do
  eval testPool do
    use TestLog log
    t = Remote.fork testPool do
      Each.run do
        i = Each.range 1 4
        Remote.sleep (Remote.Duration.seconds 1)
        log (Nat.toText i)
    log "Counter started"
    await t
    "done"

Remote.test.real.detachAt : '{Remote} ()
Remote.test.real.detachAt = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "Spawning"
    _ = Remote.scope do
      Remote.detachAt testPool do
        sleep (seconds 2)
        "done"
    log "Scope ended"
    sleep (seconds 3)

Remote.test.real.detachedScope : '{Remote} ()
Remote.test.real.detachedScope = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "Spawning"
    _ = Remote.scope do
      Remote.detachAt testPool do
        sleep (seconds 2)
        log "done"
    log "Scope ended"
    sleep (seconds 3)

Remote.test.real.idAllocationFinalizers : '{Remote} ()
Remote.test.real.idAllocationFinalizers = do
  eval testPool do
    Remote.scope do
      use Ref delete id
      use TestLog log
      finalizer do
        eval testPool do
          r = detached 0
          log (toDebugText (id r))
          delete r
      finalizer do
        eval testPool do
          r = detached 0
          log (toDebugText (id r))
          delete r

Remote.test.real.immediateForkAwait : '{Remote} Text
Remote.test.real.immediateForkAwait = do
  eval testPool do
    TestLog.log "About to sleep"
    Remote.sleep (Remote.Duration.seconds 2)
    "done"

Remote.test.real.promiseRead : '{Remote} Text
Remote.test.real.promiseRead = do
  p = detached!
  Remote.Promise.write_ p "done"
  Remote.Promise.read p

Remote.test.real.refConcurrency : '{Remote} Nat
Remote.test.real.refConcurrency = do
  eval testPool do
    use Nat +
    use Remote fork
    state = detached 0
    updating = do
      Each.run do
        i = Each.range 0 1000
        Remote.sleepMicroseconds 5
        Ref.update state (i -> i + 1)
    TestLog.log "Racing updates"
    t1 = fork testPool updating
    t2 = fork testPool updating
    await t1
    await t2
    Remote.Ref.read state

Remote.test.real.refReadWrite : '{Remote} Nat
Remote.test.real.refReadWrite = do
  eval testPool do
    use Nat +
    use TestLog log
    state = detached 0
    t = Remote.fork testPool do
      Each.run do
        i = Each.range 1 4
        Remote.sleep (Remote.Duration.seconds 1)
        log (Nat.toText i)
        Ref.update state (i -> i + 1)
    log "Counter started"
    await t
    Remote.Ref.read state

Remote.test.real.scopedCancelAndOwn : '{Remote} Text
Remote.test.real.scopedCancelAndOwn = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    t = Remote.fork testPool do
      Each.run do
        i = Each.range 1 50
        sleep (seconds 1)
        log (Nat.toText i)
    log "Counter started"
    eval testPool do
      own t
      sleep (seconds 4)
      log "Cancelling counter now"
    log "Sleeping to observe cancellation"
    sleep (seconds 2)
    "done"

Remote.test.real.scopedCancelNotification : '{Remote} Either Failure ()
Remote.test.real.scopedCancelNotification = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    TestLog.log "Sleep then timeout"
    t1 = Remote.fork testPool do sleep (seconds 50)
    eval testPool do
      own t1
      sleep (seconds 2)
    Remote.try do await t1

Remote.test.real.sharedStateMultipleInterpreters : '{IO, Exception} Nat
Remote.test.real.sharedStateMultipleInterpreters = do
  splitmix base.IO.randomNat() do
    use Interrupt new
    use Remote Ref
    a : '{Remote} Ref Nat
    a = do detached 0
    b : Ref Nat ->{Remote} ()
    b r = Remote.Ref.write r 42
    c : Ref Nat ->{Remote} Nat
    c r = Remote.Ref.read r
    runner = threaded do _ x -> coerceAbilities x ()
    r = runner () new() a
    w = runner () new() do b r
    runner () new() do c r

Remote.test.real.taskBug : '{Remote} Either Failure t
Remote.test.real.taskBug = do
  eval testPool do
    t = Remote.fork testPool do bug "bug"
    tryAwait t

Remote.test.real.taskCancel : '{Remote} Text
Remote.test.real.taskCancel = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    t = Remote.fork testPool do
      Each.run do
        i = Each.range 1 50
        sleep (seconds 1)
        log (Nat.toText i)
    log "Counter started"
    sleep (seconds 4)
    log "Cancelling counter now"
    Remote.cancel t
    log "Sleeping to observe cancellation"
    sleep (seconds 2)
    "done"

Remote.test.run : '{Remote} a ->{IO, Exception} a
Remote.test.run p =
  use run Interrupt
  handleScratch : ∀ g a. '{g, Scratch} a ->{g, IO} a
  handleScratch = local.handler () ()
  go :
    ∀ a. Interrupt -> '{TestLog, Scratch} a ->{IO, Exception, Abort, Remote} a
  go interrupt p =
    handle handleScratch p
    with cases
      { a }                         -> a
      { TestLog.log msg -> resume } ->
        printLine msg
        go interrupt resume
  poly : Interrupt -> '{g} x ->{IO, Exception, Abort, Remote} x
  poly i p = coerceAbilities (go i) (coerceAbilities p)
  splitmix base.IO.randomNat() do threaded (_ i -> poly i) () Interrupt.new() p

Remote.test.runLabeled : Text -> '{g, Exception} () ->{g} Result
Remote.test.runLabeled label test =
  use Text ++
  handle test()
  with cases
    { () }                     -> Ok label
    { Exception.raise e -> _ } -> Result.Fail (label ++ ": " ++ toDebugText e)

Remote.test.runLabeled.doc : Doc
Remote.test.runLabeled.doc =
  {{
  `` runLabeled label test `` runs `test` and returns `` Ok label `` if it
  succeeds, or {Result.Fail} with the label and a textual representation of the
  {type Failure} if {Exception.raise} is called.
  }}

Remote.test.simulate : Nat -> '{Remote} a -> ([Text], Either Failure a)
Remote.test.simulate seed p =
  splitmix seed do
    Scope.run do
      use Hashed toBytes
      use List :+
      use Scope ref
      use modify deprecated
      use mutable.Ref read
      stdout = ref ([] : [Text])
      state = ref (data.Map.empty : data.Map Bytes Any)
      go : ∀ a. '{TestLog, Scratch} a -> a
      go p =
        handle p()
        with cases
          { a } -> a
          { TestLog.log msg -> resume } ->
            deprecated stdout (msgs -> msgs :+ msg)
            go resume
          { hashKey key -> resume } ->
            hashed = Hashed (Hash (blake2b_256 key))
            go do resume hashed
          { touch _ -> resume } -> go do resume()
          { lookupHashed hashed -> resume } ->
            v =
              read state
                |> Map.get (toBytes hashed)
                |> Optional.map unsafeExtract
            go do resume v
          { Scratch.exists hashed -> resume } ->
            res = read state |> Map.contains (toBytes hashed)
            go do resume res
          { saveHashed hashed a -> resume } ->
            deprecated state (Map.insert (toBytes hashed) (Any a))
            go do resume()
      coerceGo : ('{TestLog, Scratch} x ->{Scope s} x) -> '{g} x ->{Scope s} x
      coerceGo f x = coerceAbilities f (coerceAbilities x)
      res = run.pure (coerceGo go) p
      (read stdout, res)

Remote.test.simulated.allowCancel : '{Remote} ()
Remote.test.simulated.allowCancel = do
  eval testPool do
    use Each range run
    use Remote fork
    use TestLog log
    log "start"
    t1 = fork testPool do
      run do
        i = range 1 50
        log (Nat.toText i)
        allowCancel!
      log "thread done"
    t2 = fork testPool do
      run do
        _ = range 1 5
        log "spin"
      log "About to cancel"
      Remote.cancel t1
    Remote.sleep (micros 20)
    log "end"

Remote.test.simulated.allowCancelAfterScope : '{Remote} ()
Remote.test.simulated.allowCancelAfterScope = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      Remote.scope do
        finalizer do
          sleep (seconds 20)
          eval testPool do log "uncancelable section done"
      allowCancel!
      log "shouldn't happen"
    sleep (seconds 10)
    Remote.cancel t1
    log "done"

Remote.test.simulated.cancelPreSleep : '{Remote} ()
Remote.test.simulated.cancelPreSleep = do
  eval testPool do
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      log "thread started"
      note =
        """
          I've checked that with the default seed this is indeed canceled
          before the sleep, and not during, which is what this example is
          trying to test.

        """
      Remote.sleep (Remote.Duration.seconds 20)
      log "shouldn't happen"
    Remote.cancel t1
    log "end"

Remote.test.simulated.cancelPromiseRead : '{Remote} ()
Remote.test.simulated.cancelPromiseRead = do
  eval testPool do
    use TestLog log
    log "start"
    p : Remote.Promise ()
    p = detached!
    t1 = Remote.fork testPool do
      log "thread started"
      Remote.Promise.read p
      log "shouldn't happen"
    Remote.sleep (Remote.Duration.seconds 20)
    Remote.cancel t1
    _ = Remote.Promise.write p ()
    log "end"

Remote.test.simulated.cancelSleep : '{Remote} ()
Remote.test.simulated.cancelSleep = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      log "thread started"
      note =
        """
          I've checked that with the default seed this is indeed canceled
          during the sleep, and not before, which is what this example is
          trying to test.

        """
      sleep (seconds 200)
      log "shouldn't happen"
    sleep (seconds 20)
    Remote.cancel t1
    log "end"

Remote.test.simulated.cancelSleepAfterForkScope : '{Remote} ()
Remote.test.simulated.cancelSleepAfterForkScope = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      finalizer do
        sleep (seconds 20)
        eval testPool do log "thread finalized"
      sleep (seconds 20)
      log "shouldn't happen"
    sleep (seconds 10)
    Remote.cancel t1
    log "done"

Remote.test.simulated.cancelSleepAfterScope : '{Remote} ()
Remote.test.simulated.cancelSleepAfterScope = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      Remote.scope do
        finalizer do
          sleep (seconds 20)
          eval testPool do log "uncancelable section done"
      sleep (seconds 20)
      log "shouldn't happen"
    sleep (seconds 10)
    Remote.cancel t1
    log "done"

Remote.test.simulated.failShortCircuits : '{Remote} ()
Remote.test.simulated.failShortCircuits = do
  eval testPool do
    use TestLog log
    log "start"
    Remote.fail (failure "boom" ())
    log "skipped"

Remote.test.simulated.nestedScopes : '{Remote} ()
Remote.test.simulated.nestedScopes = do
  eval testPool do
    use Remote scope sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      finalizer do eval testPool do log "thread finalized"
      sleep (seconds 20)
      scope do
        finalizer do eval testPool do log "outer Remote.scope done"
        sleep (seconds 20)
        scope do
          finalizer do eval testPool do log "inner Remote.scope done"
          sleep (seconds 20)
          eval testPool do log "shouldn't happen"
        eval testPool do log "shouldn't happen"
    sleep (seconds 50)
    Remote.cancel t1
    log "end"

Remote.test.simulated.promiseReadDelete : '{Remote} Either Failure a
Remote.test.simulated.promiseReadDelete = do
  eval testPool do
    use Remote.Duration seconds
    use TestLog log
    log "start"
    p = detached!
    _ = Remote.fork testPool do
      Remote.sleep (seconds 1)
      log "deleting promise"
      Promise.delete p
    Remote.try do Remote.timeout (seconds 3) do Remote.Promise.read p

Remote.test.simulated.promiseRendezVous : '{Remote} Text
Remote.test.simulated.promiseRendezVous = do
  eval testPool do
    use TestLog log
    log "start"
    out = detached!
    t1 = Remote.fork testPool do
      Remote.sleep (Remote.Duration.seconds 20)
      eval testPool do log "thread done"
      Remote.Promise.write out ()
    log "spawned thread"
    Remote.Promise.read out
    log "end"
    "end"

Remote.test.simulated.runtimeRunsDetachedThreads : '{Remote} Nat
Remote.test.simulated.runtimeRunsDetachedThreads = do
  eval testPool do
    use TestLog log
    log "start"
    t1 = Remote.detachAt testPool do
      Remote.sleep (Remote.Duration.seconds 20)
      log "thread done"
    log "spawned thread"
    log "end"
    42

Remote.test.simulated.scopeCancellation : '{Remote} ()
Remote.test.simulated.scopeCancellation = do
  eval testPool do
    use TestLog log
    log "start"
    p : Remote.Promise ()
    p = detached!
    t1 = Remote.fork testPool do
      addFinalizer cases
        Cancelled -> eval testPool do log "thread finalized"
        _         -> eval testPool do log "wrong case!"
      log "thread started"
      Remote.Promise.read p
      log "shouldn't happen"
    Remote.sleep (Remote.Duration.seconds 20)
    Remote.cancel t1
    _ = Remote.Promise.write p ()
    log "end"

Remote.test.simulated.scopeCompletion : '{Remote} ()
Remote.test.simulated.scopeCompletion = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    p : Remote.Promise ()
    p = detached!
    t1 = Remote.fork testPool do
      addFinalizer cases
        Completed -> eval testPool do log "thread finalized"
        _         -> eval testPool do log "wrong case!"
      log "thread started"
      Remote.Promise.read p
    sleep (seconds 20)
    _ = Remote.Promise.write p ()
    sleep (seconds 20)
    log "end"

Remote.test.simulated.simpleFork : '{Remote} Nat
Remote.test.simulated.simpleFork = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      sleep (seconds 20)
      log "thread done"
    log "spawned thread"
    sleep (seconds 30)
    log "end"
    42

Remote.test.simulated.uncancelableAfterScope : '{Remote} ()
Remote.test.simulated.uncancelableAfterScope = do
  eval testPool do
    use Remote sleep
    use Remote.Duration seconds
    use TestLog log
    log "start"
    t1 = Remote.fork testPool do
      Remote.scope do
        finalizer do
          sleep (seconds 20)
          eval testPool do log "uncancelable section done"
      log "implicitly uncancelable op"
    sleep (seconds 10)
    Remote.cancel t1
    log "done"

Remote.test.sleepForever : '{Remote} Void
Remote.test.sleepForever = do
  Remote.sleepMicroseconds 10000000000
  Remote.test.sleepForever()

Remote.test.testPool : Location {TestLog, Scratch}
Remote.test.testPool = Location (LocationId (UID 0xs)) []

Remote.Thread.cancel : Thread ->{Remote} ()
Remote.Thread.cancel t = Remote.reraise (tryCancel t)

Remote.Thread.id : Thread -> Thread.Id
Remote.Thread.id = cases Thread id _ -> id

Remote.time.monotonic!.doc : Doc
Remote.time.monotonic!.doc =
  {{
  @inlineSignature{monotonic!} is used for measuring finite time intervals in
  the {type Remote} context. If two calls to {monotonic!} are made at the same
  location, the second call will always return a {type time.Duration} greater
  than or equal to the first call.

  ```
  monotonicExample : '{Remote} time.Duration
  monotonicExample = do
    use Duration -
    first = monotonic!
    Remote.sleep (Remote.Duration.seconds 1)
    second = monotonic!
    second - first
  docs.run monotonicExample
  ```

  If you're interested in finding the current time, use {now!}.
  }}

Remote.time.now!.doc : Doc
Remote.time.now!.doc =
  {{
  `` now! `` returns the current time at the current location in the cloud as
  an {type Instant}.

  ```
  nowExample : '{Remote} Instant
  nowExample = do now!
  docs.run nowExample
  ```

  For measuring the __duration__ of a computation, you might be interested in
  {monotonic!}.
  }}

Remote.timeout : Remote.Duration -> '{Remote} a ->{Remote} a
Remote.timeout t thunk =
  timer = do
    Remote.sleep t
    failure.timeout t
  Remote.reraise (race timer thunk)

Remote.timeout.doc : Doc
Remote.timeout.doc =
  use Nat +
  use Remote sleep timeout
  use Remote.Duration seconds
  use docs run
  {{
  `` timeout duration computation `` runs the given {type Remote} computation
  for the given {type Remote.Duration} and returns the result. If the
  computation does not complete within the given duration, the computation will
  be cancelled and a {type Failure} will be returned.

  ```
  timeoutSucceed : '{Remote} Nat
  timeoutSucceed = do
    timeout (seconds 2) do
      sleep (millis 100)
      1 + 1
  run timeoutSucceed
  ```

  A timeout failure looks like this:

  ```
  timeoutFail : '{Remote} Nat
  timeoutFail = do
    timeout (millis 100) do
      sleep (seconds 2)
      1 + 1
  run timeoutFail
  ```
  }}

Remote.try : '{g, Exception, Remote} a ->{g, Remote} Either Failure a
Remote.try p =
  use Remote randomBytes sleepMicroseconds try
  use Remote.Promise tryRead
  handle p()
  with cases
    { a }                              -> Right a
    { Exception.raise e -> _ }         -> Left e
    { Remote.fail e -> _ }             -> Left e
    { detached v -> resume }           ->
      out = detached v
      try do resume out
    { tryDetachAt l t -> resume }      ->
      out = tryDetachAt l t
      try do resume out
    { Ref.tryDelete r -> resume }      ->
      out = Ref.tryDelete r
      try do resume out
    { tryScope r -> resume }           ->
      out = tryScope r
      try do resume out
    { here! -> resume }                ->
      out = here!
      try do resume out
    { region! -> resume }              ->
      out = region!
      try do resume out
    { Ref.tryWrite r v -> resume }     ->
      out = Ref.tryWrite r v
      try do resume out
    { Promise.tryDelete p -> resume }  ->
      out = Promise.tryDelete p
      try do resume out
    { monotonic! -> resume }           ->
      out = monotonic!
      try do resume out
    { Promise.tryWrite p v -> resume } ->
      out = Promise.tryWrite p v
      try do resume out
    { tryNear l1 l2 -> resume }        ->
      out = tryNear l1 l2
      try do resume out
    { tryFar l1 l2 -> resume }         ->
      out = tryFar l1 l2
      try do resume out
    { tryReadForCas r -> resume }      ->
      out = tryReadForCas r
      try do resume out
    { tryCas r t v -> resume }         ->
      out = tryCas r t v
      try do resume out
    { tryCancel t -> resume }          ->
      out = tryCancel t
      try do resume out
    { randomBytes n -> resume }        ->
      out = randomBytes n
      try do resume out
    { sleepMicroseconds n -> resume }  ->
      sleepMicroseconds n
      try do resume()
    { allowCancel! -> resume }         ->
      allowCancel!
      try do resume()
    { detached! -> resume }            ->
      out = detached!
      try do resume out
    { addFinalizer f -> resume }       ->
      addFinalizer f
      try do resume()
    { tryRead p -> resume }            ->
      out = tryRead p
      try do resume out
    { now! -> resume }                 ->
      out = now!
      try do resume out
    { tryReadNow p -> resume }         ->
      out = tryReadNow p
      try do resume out

Remote.tryAwait : Task a ->{Remote} Either Failure a
Remote.tryAwait t = Remote.try do await t

Remote.tryOr : a -> '{g, Remote} a ->{g, Remote} a
Remote.tryOr default a = match Remote.try a with
  Left _  -> default
  Right a -> a

Remote.UID.fromNat : Nat -> UID
Remote.UID.fromNat n = n |> toBytesBigEndian |> UID

Remote.UID.random : '{Random} UID
Remote.UID.random _ = UID (Random.bytes 16)

Remote.Value.at : Location g -> a -> Remote.Value a
Remote.Value.at loc a = Value (cba -> await (forkAt loc do cba a))

Remote.Value.at.doc : Doc
Remote.Value.at.doc =
  use Value at
  {{
  `` at loc a `` produces an in-memory value that will be computed on at `loc`.

  For example, {{ docExample 2 do f loc a -> Value.map f (at loc a) }} will
  apply `f` at the location `loc`.

  This can be used in conjunction with {Value.flatMap} to switch locations
  midway through a chain of transformations.
  }}

Remote.Value.cache : Task x -> Remote.Value a ->{Remote} Remote.Value a
Remote.Value.cache lifetime r =
  use Value map
  go a =
    t = Task.pure a
    Remote.link lifetime t
    t
  ta = Remote.fork here! do Value.get (map go r)
  map await (fromTask ta)

Remote.Value.cache.doc : Doc
Remote.Value.cache.doc =
  {{
  `` cache lifetime v `` caches the result of forcing `v` at the location of
  `v`, without sending the result to the current ocation.

  The `lifetime` task controls how long the value is cached. When it completes,
  the cached value is deleted. After the cached value is deleted, the returned
  {type Remote.Value} will begin failing on {Value.get}.
  }}

Remote.Value.delay : '{Remote} a -> Remote.Value a
Remote.Value.delay a =
  go : (a ->{Remote} r) ->{Remote} r
  go cb = cb a()
  Value go

Remote.Value.delay.doc : Doc
Remote.Value.delay.doc =
  use Nat +
  use Value delay get
  {{
  Create a {type Remote.Value} from a delayed computation.

  In `` delay a `` will force `a` on each {get}. For example, in

  @typecheck ```
  a = delay do 1 + 1
  r1 = get a
  r2 = get a
  ```

  `` 1 + 1 `` will be computed twice.

  Also see {fromTask}.
  }}

Remote.Value.delayAt :
  Location {g} -> '{g, Exception, Remote} t -> Remote.Value t
Remote.Value.delayAt loc a =
  Value
    (cba -> let
      t = forkAt loc do cba a()
      await t)

Remote.Value.delayAt.doc : Doc
Remote.Value.delayAt.doc =
  use Nat pow
  use Value get
  {{
  `` delayAt loc a `` creates a {type Remote.Value} at the location `loc` from
  a delayed computation `a`.

  The computation will be forced on every call to {get}, so in this example ``
  pow 2 8 `` will be evaluated twice:

  @typecheck ```
  a = delayAt here! do pow 2 8
  r1 = get a
  r2 = get a
  ```

  Also see: {Value.delay}, {Value.at}
  }}

Remote.Value.doc : Doc
Remote.Value.doc =
  use Value get map
  {{
  A remote value of type `a`, useful for building distributed data structures
  that can be computed on "where the data lives".

      @signatures{get, Value.pure, Value.delay, Value.at, map}

  Most operations on the value (for instance {map}) are lazy and pure, with
  nothing actually happening until the value is forced via {get}. When forced,
  any pending transformations on the value that have been built up lazily will
  be applied at the location where the value is located.
  }}

Remote.Value.flatMap :
  (i ->{Exception, Remote} Remote.Value a) -> Remote.Value i -> Remote.Value a
Remote.Value.flatMap f = cases
  Value va ->
    Value
      (cbb ->
        va
          (a -> let
            fa = do f a
            let
              (Value k) = reraise! fa
              k cbb))

Remote.Value.flatMap.doc : Doc
Remote.Value.flatMap.doc =
  use Nat +
  use Value at flatMap map
  {{
  `` flatMap f v `` lazily applies `f` to `v` at the location of `v`, useful
  for "moving the computation to the data".

  This is similar to {map} but can be used to move a value elsewhere for
  subsequent operations. For instance, in

  {{
  docExampleBlock 3 do v loc f -> v |> flatMap (x -> at loc (x + 10)) |> map f
  }}

  the `` x + 10 `` will be computed at `v`, but subsequent operations like `f`
  will happen at `loc`, because of the ``at loc``.
  }}

Remote.Value.forkMemoAt :
  Location {Scratch, g}
  -> '{g, Exception, Remote, Scratch} r
  ->{Remote} Remote.Value r
Remote.Value.forkMemoAt loc s =
  v = memoAt loc s
  _ = forkAt loc do Value.get v
  v

Remote.Value.forkMemoAt.doc : Doc
Remote.Value.forkMemoAt.doc =
  {{
  `` forkMemoAt loc r `` forks a task `r` at `loc` in the background, returning
  a memoized {type Remote.Value}.

  This is just like {memoAt}, except that it eagerly starts `r` running the
  background, rather than waiting until the returned {type Remote.Value} is
  forced with a {Value.get}.
  }}

Remote.Value.fromTask : Task a -> Remote.Value a
Remote.Value.fromTask ta =
  Value.map awaitRetain (Value.at (workLocation ta) ta)

Remote.Value.fromTask.doc : Doc
Remote.Value.fromTask.doc =
  use Value get
  {{
  `` fromTask t `` creates a {type Remote.Value} from a task `t`.

  This will call {awaitRetain} on {get}. If you're sure the {type Remote.Value}
  will only be forced once, you can do {fromTaskOnce} instead and the task will
  be deleted on {get}.
  }}

Remote.Value.fromTaskOnce : Task a -> Remote.Value a
Remote.Value.fromTaskOnce ta = Value.map await (Value.at (workLocation ta) ta)

Remote.Value.fromTaskOnce.doc : Doc
Remote.Value.fromTaskOnce.doc =
  {{
  `` fromTaskOnce t `` creates a {type Remote.Value} from a task `t`.

  This will call {await} on {Value.get}, which deletes the task. If the task
  being passed here is one you're planning to {await} elsewhere, you can use
  {fromTask} instead.
  }}

Remote.Value.get : Remote.Value a ->{Remote} a
Remote.Value.get = cases Value va -> va Function.id

Remote.Value.get.doc : Doc
Remote.Value.get.doc =
  use Value get
  {{
  Obtain the `a` denoted by this {type Remote.Value}.

  A few properties:

  * {{ docExample 1 do a -> get (Value.pure a) === a }}
  * {{ docExample 2 do loc a -> get (Value.at loc a) === a }}
  * {{ docExample 2 do v f -> f (get v) === get (Value.map f v) }}
  }}

Remote.Value.join : Remote.Value (Remote.Value a) -> Remote.Value a
Remote.Value.join v = Value.flatMap Function.id v

Remote.Value.map :
  (i ->{Exception, Remote} o) -> Remote.Value i -> Remote.Value o
Remote.Value.map f = cases
  Value va ->
    Value
      (cbb ->
        va
          (a -> let
            t = do f a
            cbb (reraise! t)))

Remote.Value.map.doc : Doc
Remote.Value.map.doc =
  use Value get map
  {{
  `` map f v `` lazily applies `f` to `v` at the location of `v`, useful for
  "moving the computation to the data".

  This function is pure and does nothing until the value is forced via
  {#bj09qlvsa3}.

  By using {map} and {#bj09qlvsa3} appropriately, you can affect where
  computations happen and what values will be sent over the network. For
  instance, both these expressions produce the same result, but they have
  different runtime behavior:

  1. {{ docExample 2 do v f -> get (map f v) }}
  2. {{ docExample 2 do v f -> f (get v) }} and

  If location __Alice__ does (1) for `v` located at __Bob__, it will send the
  function `f` to Bob. Bob will apply `f` locally and send back the result.
  This is "moving the computation to the data".

  If Alice instead does (2), the function `f` won't be sent to Bob. Instead,
  Bob will send the result of `v` back to Alice, who will apply `f` to it
  locally. This is "moving the data to the computation".

  Often computations occupy less space than the data they operate on, so moving
  the computation to the data reduces network traffic. But this isn't the only
  factor to keep in mind: locations have limited compute capacity. If multiple
  computations require the same data and can be run in parallel, we can
  sometimes achieve more parallelism (and lower overall runtime) by copying the
  data to the locations of the computations.
  }}

Remote.Value.memo : Location {g, Scratch} -> Remote.Value a -> Remote.Value a
Remote.Value.memo region v = Value.delay do
  eval region do
    h = Hashed (memoHash v)
    match lookupHashed h with
      Some a -> a
      None   ->
        a = Value.get v
        saveHashed h a
        a

Remote.Value.memo.impl.memoHash : a -> Hash
Remote.Value.memo.impl.memoHash a =
  h = crypto.hash Blake2b_256 (MemoTagged a)
  Hash h

Remote.Value.memo.impl.saveHash : a -> Hash
Remote.Value.memo.impl.saveHash a =
  h = crypto.hash Blake2b_256 (SaveTagged a)
  Hash h

Remote.Value.memo' : Location {Scratch, g} -> '{Remote} a -> Remote.Value a
Remote.Value.memo' region a = memo region (Value.delay a)

Remote.Value.memoAt :
  Location {Scratch, g} -> '{g, Exception, Remote, Scratch} r -> Remote.Value r
Remote.Value.memoAt loc s = memo loc (delayAt loc s)

Remote.Value.memoAt.doc : Doc
Remote.Value.memoAt.doc =
  use Value get
  {{
  `` memoAt loc r `` produces a memoized {type Remote.Value} at the given
  `loc`.

  For instance, in this code, `` sqrt 2.0 `` will only be evaluated once
  (unless the result is evicted from the cache, in which case it will be
  recomputed and cached again).

  @typecheck ```
  v = memoAt (assume here!) do sqrt 2.0
  r1 = get v
  r2 = get v
  ```

  This function doesn't start the computation running in the background, so it
  doesn't add any parallelism. Use {forkMemoAt} if you'd like to start the
  computation `r` running in the background immediately when this function is
  called.
  }}

Remote.Value.pure : a -> Remote.Value a
Remote.Value.pure a = Value (cb -> cb a)

Remote.Value.pure.doc : Doc
Remote.Value.pure.doc =
  {{
  Create an in-memory {type Remote.Value}.

  Satisfies the property: {{
  docExample 1 do a -> Value.get (Value.pure a) === a }}
  }}

Remote.Value.toRAM : Remote.Value a ->{Remote} Remote.Value a
Remote.Value.toRAM v = Value.pure (Value.get v)

Scratch.assume : Location g -> Location {g, Scratch}
Scratch.assume loc = cast (tags.add location.tag loc)

Scratch.Hashed.toBytes : Hashed a -> Bytes
Scratch.Hashed.toBytes = cases Hashed (Hash bs) -> bs

Scratch.Hashed.toBytes.doc : Doc
Scratch.Hashed.toBytes.doc =
  use Hashed toBytes
  {{
  `` toBytes e `` extracts the {type Bytes} of the hash inside `e`.

  Satisfies the property that
  {{ docExample 1 do bs -> toBytes (Hashed (Hash bs)) === bs }}.

  ```
  toBytes (Hashed (Hash 0xs2093489023904884))
  ```
  }}

Scratch.local.handler : '{IO} (∀ g a. ctx -> '{g, Scratch} a ->{g, IO} a)
Scratch.local.handler = do
  use Hashed toBytes
  use systemfw_concurrent_2_2_0.Map lookup
  state : systemfw_concurrent_2_2_0.Map (ctx, Bytes) Any
  state = systemfw_concurrent_2_2_0.Map.empty()
  go : ctx -> '{g, Scratch} a ->{g, IO} a
  go ctx p =
    handle p()
    with cases
      { hashKey key -> resume }           ->
        hashed = Hashed (Hash (blake2b_256 key))
        go ctx do resume hashed
      { touch _ -> resume }               -> go ctx do resume()
      { lookupHashed hashed -> resume }   ->
        v = lookup state (ctx, toBytes hashed) |> Optional.map unsafeExtract
        go ctx do resume v
      { Scratch.exists hashed -> resume } ->
        res = lookup state (ctx, toBytes hashed) |> isSome
        go ctx do resume res
      { saveHashed hashed a -> resume }   ->
        systemfw_concurrent_2_2_0.Map.put state (ctx, toBytes hashed) (Any a)
        go ctx do resume()
      { a }                               -> a
  go

Scratch.location.tag : Text
Scratch.location.tag = "scratch"

Scratch.lookup : k a ->{Scratch} Optional a
Scratch.lookup key = lookupHashed (hashKey key)

Scratch.RAM : '{g, Scratch} a ->{g} a
Scratch.RAM s = handle s() with RAM.handler

Scratch.RAM.handler : Request {Scratch} a -> a
Scratch.RAM.handler =
  use Map get
  go : data.Map Hash Any -> Request {Scratch} x -> x
  go m = cases
    { a } -> a
    { hashKey k -> resume } ->
      h = Hashed (Hash (crypto.hash Blake2b_256 k))
      handle resume h with go m
    { lookupHashed (Hashed h) -> resume } ->
      match get h m with
        None   -> handle resume None with go m
        Some v -> handle resume (Some (unsafeExtract v)) with go m
    { saveHashed (Hashed h) a -> resume } ->
      match get h m with
        None   -> handle resume() with go (Map.insert h (Any a) m)
        Some _ -> handle resume() with go m
    { touch _ -> resume } -> handle resume() with go m
    { Scratch.exists (Hashed h) -> resume } ->
      handle resume (Map.contains h m) with go m
  go data.Map.empty

Scratch.replicateFrom :
  '{Remote} Location {Scratch, g} -> Nat -> a ->{Remote} Remote.Value a
Scratch.replicateFrom loc replicationOdds a =
  use Nat ==
  use Remote fork
  src = loc()
  t = forkAt src do save a
  h = await t
  Value.delay do match await (fork src do lookupHashed h) with
    Some a -> a
    None   ->
      t = forkAt src do lookupHashed h
      match await t with
        None   -> Remote.fail (unknownHash "Scratch.replicateFrom" h)
        Some a ->
          n = randomly do Random.natIn 0 replicationOdds
          if n == 0 then
            _ = fork src do saveHashed h a
            ()
          else ()
          a

Scratch.replicateFrom.doc : Doc
Scratch.replicateFrom.doc =
  {{
  `` replicateFrom loc k `` returns a function that saves values to
  {type Scratch} storage, initially at the {type Location} returned by `loc`,
  but replicating to other locations on a {Scratch.lookup} miss with
  probability `1/k`.

  This tends to replicate values based on demand, and will to replicate keys
  commonly accessed together onto the same location.
  }}

Scratch.restore! : Hashed a ->{Abort, Scratch} a
Scratch.restore! e = match lookupHashed e with
  None   -> Abort.abort
  Some a -> a

Scratch.restoreOr : a -> Hashed a ->{Scratch} a
Scratch.restoreOr default e = Optional.getOrElse default (lookupHashed e)

Scratch.save : a ->{Scratch} Hashed a
Scratch.save a = saveKeyed (data.Id.Id a) a

Scratch.saveKeyed : k a -> a ->{Scratch} Hashed a
Scratch.saveKeyed key a =
  h = hashKey key
  saveHashed h a
  h

ServiceAssignment.assignedAt : ServiceAssignment -> Instant
ServiceAssignment.assignedAt = cases
  ServiceAssignment _ _ _ _ _ _ _ _ assignedAt _ -> assignedAt

ServiceAssignment.assignedAt.modify :
  (Instant ->{g} Instant) -> ServiceAssignment ->{g} ServiceAssignment
ServiceAssignment.assignedAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      (f assignedAt)
      historyAt

ServiceAssignment.assignedAt.set :
  Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.assignedAt.set assignedAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    _
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt1
      historyAt

ServiceAssignment.assignedBy : ServiceAssignment -> UserInfo
ServiceAssignment.assignedBy = cases
  ServiceAssignment _ _ _ _ _ _ _ assignedBy _ _ -> assignedBy

ServiceAssignment.assignedBy.modify :
  (UserInfo ->{g} UserInfo) -> ServiceAssignment ->{g} ServiceAssignment
ServiceAssignment.assignedBy.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      (f assignedBy)
      assignedAt
      historyAt

ServiceAssignment.assignedBy.set :
  UserInfo -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.assignedBy.set assignedBy1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    _
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy1
      assignedAt
      historyAt

ServiceAssignment.deployedAt : ServiceAssignment -> Instant
ServiceAssignment.deployedAt = cases
  ServiceAssignment _ _ deployedAt _ _ _ _ _ _ _ -> deployedAt

ServiceAssignment.deployedAt.modify :
  (Instant ->{g} Instant) -> ServiceAssignment ->{g} ServiceAssignment
ServiceAssignment.deployedAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      (f deployedAt)
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.deployedAt.set :
  Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.deployedAt.set deployedAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    _
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt1
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.deployedBy : ServiceAssignment -> UserInfo
ServiceAssignment.deployedBy = cases
  ServiceAssignment _ deployedBy _ _ _ _ _ _ _ _ -> deployedBy

ServiceAssignment.deployedBy.modify :
  (UserInfo ->{g} UserInfo) -> ServiceAssignment ->{g} ServiceAssignment
ServiceAssignment.deployedBy.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      (f deployedBy)
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.deployedBy.set :
  UserInfo -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.deployedBy.set deployedBy1 = cases
  ServiceAssignment
    hash
    _
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy1
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.doc : Doc
ServiceAssignment.doc =
  {{
  A {type ServiceAssignment} represents an entry in the history of a
  {type ServiceName}. It contains information about the deployment,
  undeployment, exposure, and unexposure of a service, as well as the user who
  assigned the service name and the time at which it was assigned.

  # Fields

    * {ServiceAssignment.hash} - The hash of the service that was assigned, as
      a {type ServiceHash.Untyped}.
    * {ServiceAssignment.deployedBy} - Information about the user who deployed
      the service, as a {type UserInfo}.
    * {ServiceAssignment.deployedAt} - The time at which the service was
      deployed.
    * {ServiceAssignment.undeployedAt} - The time at which the service was
      undeployed, if it has been undeployed.
    * {ServiceAssignment.exposedAt} - The time at which the service was
      exposed, if it has been exposed.
    * {ServiceAssignment.unexposedAt} - The time at which the service was
      unexposed, if it has been unexposed.
    * {ServiceAssignment.tags} - A list of tags that are associated with the
      service.
    * {assignedBy} - Information about the user who assigned the service, as a
      {type UserInfo}.
    * {assignedAt} - The time at which the service was assigned.
    * {historyAt} - The time at which this history entry was created.
  }}

ServiceAssignment.exposedAt : ServiceAssignment -> Optional Instant
ServiceAssignment.exposedAt = cases
  ServiceAssignment _ _ _ _ exposedAt _ _ _ _ _ -> exposedAt

ServiceAssignment.exposedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> ServiceAssignment
  ->{g} ServiceAssignment
ServiceAssignment.exposedAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      (f exposedAt)
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.exposedAt.set :
  Optional Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.exposedAt.set exposedAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    _
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt1
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.fromJson : '{Decoder} ServiceAssignment
ServiceAssignment.fromJson =
  do
    use Decoder optional
    use object at!
    hash = at! "hash" ServiceHash.fromJson
    deployedBy = at! "deployedBy" UserInfo.fromJson
    deployedAt = at! "deployedAt" instant
    undeployedAt = at! "undeployedAt" (optional instant)
    exposedAt = at! "exposedAt" (optional instant)
    unexposedAt = at! "unexposedAt" (optional instant)
    tags = at! "tags" (Decoder.array Decoder.text)
    assignedBy = at! "assignedBy" UserInfo.fromJson
    assignedAt = at! "assignedAt" instant
    unassignedAt = at! "unassignedAt" (optional instant)
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      unassignedAt

ServiceAssignment.fromJson.doc : Doc
ServiceAssignment.fromJson.doc =
  use UserInfo fromJson
  {{
  A {type Decoder} for parsing JSON string into a {type ServiceAssignment}.

  This decoder expects a JSON object with the following properties:

  * `hash` - The hash of the service. A string.
  * `deployedBy` - A {type UserInfo} object representing the user who deployed
    the service. See {fromJson}.
  * `deployedAt` - The time the service was deployed. A string in ISO 8601
    format.
  * `undeployedAt` - The time the service was undeployed. A string in ISO 8601
    format.
  * `exposedAt` (optional) - The time the service was exposed. A string in ISO
    8601 format.
  * `unexposedAt` (optional) - The time the service was unexposed. A string in
    ISO 8601 format.
  * `tags` - The tags associated with the service. An array of strings.
  * `assignedBy` - A {type UserInfo} object representing the user who assigned
    the service to an environment. See {fromJson}.
  * `assignedAt` - The time the service was assigned to an environment. A
    string in ISO 8601 format.
  * `unassignedAt` (optional) - The time the service was unassigned from an
    environment. A string in ISO 8601 format.
  }}

ServiceAssignment.hash : ServiceAssignment -> ServiceHash.Untyped
ServiceAssignment.hash = cases ServiceAssignment hash _ _ _ _ _ _ _ _ _ -> hash

ServiceAssignment.hash.modify :
  (ServiceHash.Untyped ->{g} ServiceHash.Untyped)
  -> ServiceAssignment
  ->{g} ServiceAssignment
ServiceAssignment.hash.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      (f hash)
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.hash.set :
  ServiceHash.Untyped -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.hash.set hash1 = cases
  ServiceAssignment
    _
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash1
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.historyAt : ServiceAssignment -> Optional Instant
ServiceAssignment.historyAt = cases
  ServiceAssignment _ _ _ _ _ _ _ _ _ historyAt -> historyAt

ServiceAssignment.historyAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> ServiceAssignment
  ->{g} ServiceAssignment
ServiceAssignment.historyAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      (f historyAt)

ServiceAssignment.historyAt.set :
  Optional Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.historyAt.set historyAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    _ ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt1

ServiceAssignment.tags : ServiceAssignment -> [Text]
ServiceAssignment.tags = cases ServiceAssignment _ _ _ _ _ _ tags _ _ _ -> tags

ServiceAssignment.tags.modify :
  ([Text] ->{g} [Text]) -> ServiceAssignment ->{g} ServiceAssignment
ServiceAssignment.tags.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      (f tags)
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.tags.set : [Text] -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.tags.set tags1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    _
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt
      tags1
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.undeployedAt : ServiceAssignment -> Optional Instant
ServiceAssignment.undeployedAt = cases
  ServiceAssignment _ _ _ undeployedAt _ _ _ _ _ _ -> undeployedAt

ServiceAssignment.undeployedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> ServiceAssignment
  ->{g} ServiceAssignment
ServiceAssignment.undeployedAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      (f undeployedAt)
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.undeployedAt.set :
  Optional Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.undeployedAt.set undeployedAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    _
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt1
      exposedAt
      unexposedAt
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.unexposedAt : ServiceAssignment -> Optional Instant
ServiceAssignment.unexposedAt = cases
  ServiceAssignment _ _ _ _ _ unexposedAt _ _ _ _ -> unexposedAt

ServiceAssignment.unexposedAt.modify :
  (Optional Instant ->{g} Optional Instant)
  -> ServiceAssignment
  ->{g} ServiceAssignment
ServiceAssignment.unexposedAt.modify f = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    unexposedAt
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      (f unexposedAt)
      tags
      assignedBy
      assignedAt
      historyAt

ServiceAssignment.unexposedAt.set :
  Optional Instant -> ServiceAssignment -> ServiceAssignment
ServiceAssignment.unexposedAt.set unexposedAt1 = cases
  ServiceAssignment
    hash
    deployedBy
    deployedAt
    undeployedAt
    exposedAt
    _
    tags
    assignedBy
    assignedAt
    historyAt ->
    ServiceAssignment
      hash
      deployedBy
      deployedAt
      undeployedAt
      exposedAt
      unexposedAt1
      tags
      assignedBy
      assignedAt
      historyAt

ServiceHash.allTags : '{IO, Exception} [Text]
ServiceHash.allTags =
  do ServiceHash.allTags.withConfig Cloud.ClientConfig.default()

ServiceHash.allTags.withConfig : Cloud.ClientConfig ->{IO, Exception} [Text]
ServiceHash.allTags.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "deployments" / "tags") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching service tags"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array Decoder.text)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceHash.allTags.withConfig.doc : Doc
ServiceHash.allTags.withConfig.doc =
  {{
  Get the list of all tags which have been assigned to any {type ServiceHash}
  }}

ServiceHash.byTag : Text ->{IO, Exception} [DeploymentInfo]
ServiceHash.byTag tag =
  ServiceHash.byTag.withConfig Cloud.ClientConfig.default() tag

ServiceHash.byTag.withConfig :
  Cloud.ClientConfig -> Text ->{IO, Exception} [DeploymentInfo]
ServiceHash.byTag.withConfig config tag =
  use Path /
  use Text ++
  base =
    httpUri config (root / "v1" / "deployments" / "tags" / tag) RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching service hashes with tag " ++ tag
  expectApiSuccess context res
  Decoder.run
    (Decoder.array DeploymentInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceHash.byTag.withConfig.doc : Doc
ServiceHash.byTag.withConfig.doc =
  {{
  Get the list of {type ServiceHash}s which have been assigned a given tag
  }}

ServiceHash.doc : Doc
ServiceHash.doc =
  {{
  A [ServiceHash a b]({type ServiceHash}) is an immutable identifier for a
  specific version of a Unison Cloud service that takes an input of type `a`
  and returns an output of type `b`.

  This identifier is derived from the service's source code (along with some
  other metadata), so it effectively points to a snapshot of the service's code
  at a point in time.
  }}

ServiceHash.fromJson : '{Decoder} ServiceHash.Untyped
ServiceHash.fromJson = do ServiceHash.Untyped.Untyped Decoder.text()

ServiceHash.list : '{IO, Exception} [DeploymentInfo]
ServiceHash.list = do ServiceHash.list.withConfig Cloud.ClientConfig.default()

ServiceHash.list.withConfig :
  Cloud.ClientConfig ->{IO, Exception} [DeploymentInfo]
ServiceHash.list.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "deployments") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching deployments"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array DeploymentInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceHash.list.withConfig.doc : Doc
ServiceHash.list.withConfig.doc =
  {{
  Get the list of {type ServiceHash}s which have been assigned to any
  {type ServiceName}
  }}

ServiceHash.tag : Text -> ServiceHash.Untyped ->{IO, Exception} ()
ServiceHash.tag tag hash =
  ServiceHash.tag.withConfig Cloud.ClientConfig.default() tag hash

ServiceHash.tag.withConfig :
  Cloud.ClientConfig -> Text -> ServiceHash.Untyped ->{IO, Exception} ()
ServiceHash.tag.withConfig config tag = cases
  ServiceHash.Untyped.Untyped hash ->
    use Path /
    use Text ++
    base =
      httpUri
        config
        (root / "v1" / "deployments" / hash / "tags" / tag)
        RawQuery.empty
    req =
      HttpRequest.post base Body.empty
        |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "tagging service with hash " ++ hash ++ " with tag " ++ tag
    expectApiSuccess context res

ServiceHash.tag.withConfig.doc : Doc
ServiceHash.tag.withConfig.doc = {{ Assign a tag to a {type ServiceHash} }}

ServiceHash.tags : ServiceHash.Untyped ->{IO, Exception} [Text]
ServiceHash.tags hash =
  ServiceHash.tags.withConfig Cloud.ClientConfig.default() hash

ServiceHash.tags.withConfig :
  Cloud.ClientConfig -> ServiceHash.Untyped ->{IO, Exception} [Text]
ServiceHash.tags.withConfig config = cases
  ServiceHash.Untyped.Untyped hash ->
    use Path /
    use Text ++
    base =
      httpUri
        config (root / "v1" / "deployments" / hash / "tags") RawQuery.empty
    req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "fetching tags for service with hash " ++ hash
    expectApiSuccess context res
    Decoder.run
      (Decoder.array Decoder.text)
      (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceHash.tags.withConfig.doc : Doc
ServiceHash.tags.withConfig.doc =
  {{
  Get the list of tags which have been assigned to a given {type ServiceHash}
  }}

ServiceHash.toText : ServiceHash a b -> Text
ServiceHash.toText = cases ServiceHash base64 -> base64

ServiceHash.untag : Text -> ServiceHash.Untyped ->{IO, Exception} ()
ServiceHash.untag tag hash =
  ServiceHash.untag.withConfig Cloud.ClientConfig.default() tag hash

ServiceHash.untag.withConfig :
  Cloud.ClientConfig -> Text -> ServiceHash.Untyped ->{IO, Exception} ()
ServiceHash.untag.withConfig config tag = cases
  ServiceHash.Untyped.Untyped hash ->
    use Path /
    use Text ++
    base =
      httpUri
        config
        (root / "v1" / "deployments" / hash / "tags" / tag)
        RawQuery.empty
    req = HttpRequest.delete base |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "removing tag '" ++ tag ++ "' from service with hash " ++ hash
    expectApiSuccess context res

ServiceHash.untag.withConfig.doc : Doc
ServiceHash.untag.withConfig.doc = {{ Remove a tag from a {type ServiceHash} }}

ServiceHash.untagged : '{IO, Exception} [DeploymentInfo]
ServiceHash.untagged =
  do ServiceHash.untagged.withConfig Cloud.ClientConfig.default()

ServiceHash.untagged.withConfig :
  Cloud.ClientConfig ->{IO, Exception} [DeploymentInfo]
ServiceHash.untagged.withConfig config =
  use Path /
  base =
    httpUri config (root / "v1" / "deployments" / "untagged") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching untagged deployments"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array DeploymentInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceHash.untagged.withConfig.doc : Doc
ServiceHash.untagged.withConfig.doc =
  {{ Get the list of {type ServiceHash}s which have not been assigned a tag }}

ServiceName.allTags : '{IO, Exception} [Text]
ServiceName.allTags =
  do ServiceName.allTags.withConfig Cloud.ClientConfig.default()

ServiceName.allTags.withConfig : Cloud.ClientConfig ->{IO, Exception} [Text]
ServiceName.allTags.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "services" / "tags") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching all service tags"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array Decoder.text)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.allTags.withConfig.doc : Doc
ServiceName.allTags.withConfig.doc =
  {{
  Get the list of all tags which have been assigned to any {type ServiceName}
  }}

ServiceName.assign :
  ServiceName a b -> ServiceHash a b ->{Exception, Cloud} URI
ServiceName.assign service serviceHash =
  Either.toException (ServiceName.assign.impl service serviceHash)

ServiceName.byTag : Text ->{IO, Exception} [ServiceNameInfo]
ServiceName.byTag tag =
  ServiceName.byTag.withConfig Cloud.ClientConfig.default() tag

ServiceName.byTag.withConfig :
  Cloud.ClientConfig -> Text ->{IO, Exception} [ServiceNameInfo]
ServiceName.byTag.withConfig config tag =
  use Path /
  use Text ++
  base =
    httpUri config (root / "v1" / "services" / "tags" / tag) RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching services with tag " ++ tag
  expectApiSuccess context res
  Decoder.run
    (Decoder.array ServiceNameInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.byTag.withConfig.doc : Doc
ServiceName.byTag.withConfig.doc =
  {{
  Get the list of {type ServiceName}s which have been assigned a given tag
  }}

ServiceName.delete : ServiceName a b ->{Exception, Cloud} ()
ServiceName.delete service =
  Either.toException (ServiceName.delete.impl service)

ServiceName.doc : Doc
ServiceName.doc =
  {{
  An identifier for a Unison Cloud service that takes an input of type `a` and
  returns an output of type `b`.

  While a {type ServiceHash} is an immutable identifier for a specific version
  of a service, a {type ServiceName} is a mutable identifier that can be
  updated to point to a different version of the service. So you could have a
  {type ServiceName} representing your chatbot service and after a new
  deployment passes tests and health checks, you could update the chatbot
  {type ServiceName} to point to the newly-deployed {type ServiceHash}.

  For those familiar with [git](https://git-scm.com/), a {type ServiceHash} is
  like a git commit hash, and a {type ServiceName} is like a git branch that
  can be updated to point to a different {type ServiceHash}.
  }}

ServiceName.fromJson : '{Decoder} ServiceName.Untyped
ServiceName.fromJson = do
  use Decoder text
  use object at!
  id = at! "id" text |> ServiceName.Id.Id
  name = at! "name" text
  ServiceName.Untyped.Untyped id name

ServiceName.history : ServiceName.Id ->{IO, Exception} [ServiceAssignment]
ServiceName.history id = history.withConfig Cloud.ClientConfig.default() id

ServiceName.history.withConfig :
  Cloud.ClientConfig -> ServiceName.Id ->{IO, Exception} [ServiceAssignment]
ServiceName.history.withConfig config = cases
  ServiceName.Id.Id id ->
    use Path /
    use Text ++
    base =
      httpUri
        config (root / "v1" / "services" / id / "deployments") RawQuery.empty
    req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "fetching deployments for service " ++ id
    expectApiSuccess context res
    Decoder.run
      (Decoder.array ServiceAssignment.fromJson)
      (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.history.withConfig.doc : Doc
ServiceName.history.withConfig.doc =
  {{
  Get the list of {type ServiceHash}s which have been assigned to this
  {type ServiceName}
  }}

ServiceName.id : ServiceName a b -> ServiceName.Id
ServiceName.id = cases ServiceName i _ -> i

ServiceName.Id.toText : ServiceName.Id -> Text
ServiceName.Id.toText = cases ServiceName.Id.Id t -> t

ServiceName.list : '{IO, Exception} [ServiceNameInfo]
ServiceName.list = do ServiceName.list.withConfig Cloud.ClientConfig.default()

ServiceName.list.withConfig :
  Cloud.ClientConfig ->{IO, Exception} [ServiceNameInfo]
ServiceName.list.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "services") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res : HttpResponse
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "listing service names"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array ServiceNameInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.name : ServiceName a b -> Text
ServiceName.name = cases ServiceName _ name -> name

ServiceName.named : Text ->{Exception, Cloud} ServiceName a b
ServiceName.named name = Either.toException (ServiceName.create.impl name)

ServiceName.tag : Text -> ServiceName.Id ->{IO, Exception} ()
ServiceName.tag tag id =
  ServiceName.tag.withConfig Cloud.ClientConfig.default() tag id

ServiceName.tag.withConfig :
  Cloud.ClientConfig -> Text -> ServiceName.Id ->{IO, Exception} ()
ServiceName.tag.withConfig config tag = cases
  ServiceName.Id.Id id ->
    use Path /
    use Text ++
    base =
      httpUri
        config (root / "v1" / "services" / id / "tags" / tag) RawQuery.empty
    req =
      HttpRequest.post base Body.empty
        |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "tagging service name " ++ id
    expectApiSuccess context res

ServiceName.tag.withConfig.doc : Doc
ServiceName.tag.withConfig.doc = {{ Assign a tag to a {type ServiceName} }}

ServiceName.tags : ServiceName.Id ->{IO, Exception} [Text]
ServiceName.tags id =
  ServiceName.tags.withConfig Cloud.ClientConfig.default() id

ServiceName.tags.withConfig :
  Cloud.ClientConfig -> ServiceName.Id ->{IO, Exception} [Text]
ServiceName.tags.withConfig config = cases
  ServiceName.Id.Id id ->
    use Path /
    use Text ++
    base =
      httpUri config (root / "v1" / "services" / id / "tags") RawQuery.empty
    req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "fetching tags for service " ++ id
    expectApiSuccess context res
    Decoder.run
      (Decoder.array Decoder.text)
      (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.tags.withConfig.doc : Doc
ServiceName.tags.withConfig.doc =
  {{
  Get the list of tags which have been assigned to a given {type ServiceName}
  }}

ServiceName.toText : ServiceName a b -> Text
ServiceName.toText = cases
  ServiceName id name ->
    ServiceName.Id.toText id Text.++ " (" Text.++ name Text.++ ")"

ServiceName.unassign : ServiceName a b ->{Exception, Cloud} ()
ServiceName.unassign service =
  Either.toException (ServiceName.unassign.impl service)

ServiceName.untag : Text -> ServiceName.Id ->{IO, Exception} ()
ServiceName.untag tag id =
  ServiceName.untag.withConfig Cloud.ClientConfig.default() tag id

ServiceName.untag.withConfig :
  Cloud.ClientConfig -> Text -> ServiceName.Id ->{IO, Exception} ()
ServiceName.untag.withConfig config tag = cases
  ServiceName.Id.Id id ->
    use Path /
    use Text ++
    base =
      httpUri
        config (root / "v1" / "services" / id / "tags" / tag) RawQuery.empty
    req = HttpRequest.delete base |> addAuthHeader (ClientConfig.token config)
    res = handle request req with Http.configuredHandler (httpConfig config)
    context = "removing '" ++ tag ++ "' tag from service " ++ id
    expectApiSuccess context res

ServiceName.untag.withConfig.doc : Doc
ServiceName.untag.withConfig.doc = {{ Remove a tag from a {type ServiceName} }}

ServiceName.untagged : '{IO, Exception} [ServiceNameInfo]
ServiceName.untagged =
  do ServiceName.untagged.withConfig Cloud.ClientConfig.default()

ServiceName.untagged.withConfig :
  Cloud.ClientConfig ->{IO, Exception} [ServiceNameInfo]
ServiceName.untagged.withConfig config =
  use Path /
  base = httpUri config (root / "v1" / "services" / "untagged") RawQuery.empty
  req = HttpRequest.get base |> addAuthHeader (ClientConfig.token config)
  res = handle request req with Http.configuredHandler (httpConfig config)
  context = "fetching untagged services"
  expectApiSuccess context res
  Decoder.run
    (Decoder.array ServiceNameInfo.fromJson)
    (HttpResponse.body res |> Body.toBytes |> fromUtf8)

ServiceName.untagged.withConfig.doc : Doc
ServiceName.untagged.withConfig.doc =
  {{ Get the list of {type ServiceName}s which have not been assigned a tag }}

ServiceNameInfo.currentDeployment : ServiceNameInfo -> Optional DeploymentInfo
ServiceNameInfo.currentDeployment = cases
  ServiceNameInfo _ currentDeployment _ -> currentDeployment

ServiceNameInfo.currentDeployment.modify :
  (Optional DeploymentInfo ->{g} Optional DeploymentInfo)
  -> ServiceNameInfo
  ->{g} ServiceNameInfo
ServiceNameInfo.currentDeployment.modify f = cases
  ServiceNameInfo service currentDeployment owner ->
    ServiceNameInfo service (f currentDeployment) owner

ServiceNameInfo.currentDeployment.set :
  Optional DeploymentInfo -> ServiceNameInfo -> ServiceNameInfo
ServiceNameInfo.currentDeployment.set currentDeployment1 = cases
  ServiceNameInfo service _ owner ->
    ServiceNameInfo service currentDeployment1 owner

ServiceNameInfo.fromJson : '{Decoder} ServiceNameInfo
ServiceNameInfo.fromJson = do
  use Decoder text
  use object at!
  name = at! "name" text
  id = at! "id" text |> ServiceName.Id.Id
  service = ServiceName.Untyped.Untyped id name
  currentDeployment = optionalAt! "latestServiceDeploy" DeploymentInfo.fromJson
  owner = at! "owner" UserInfo.fromJson
  ServiceNameInfo service currentDeployment owner

ServiceNameInfo.owner : ServiceNameInfo -> UserInfo
ServiceNameInfo.owner = cases ServiceNameInfo _ _ owner -> owner

ServiceNameInfo.owner.modify :
  (UserInfo ->{g} UserInfo) -> ServiceNameInfo ->{g} ServiceNameInfo
ServiceNameInfo.owner.modify f = cases
  ServiceNameInfo service currentDeployment owner ->
    ServiceNameInfo service currentDeployment (f owner)

ServiceNameInfo.owner.set : UserInfo -> ServiceNameInfo -> ServiceNameInfo
ServiceNameInfo.owner.set owner1 = cases
  ServiceNameInfo service currentDeployment _ ->
    ServiceNameInfo service currentDeployment owner1

ServiceNameInfo.service : ServiceNameInfo -> ServiceName.Untyped
ServiceNameInfo.service = cases ServiceNameInfo service _ _ -> service

ServiceNameInfo.service.modify :
  (ServiceName.Untyped ->{g} ServiceName.Untyped)
  -> ServiceNameInfo
  ->{g} ServiceNameInfo
ServiceNameInfo.service.modify f = cases
  ServiceNameInfo service currentDeployment owner ->
    ServiceNameInfo (f service) currentDeployment owner

ServiceNameInfo.service.set :
  ServiceName.Untyped -> ServiceNameInfo -> ServiceNameInfo
ServiceNameInfo.service.set service1 = cases
  ServiceNameInfo _ currentDeployment owner ->
    ServiceNameInfo service1 currentDeployment owner

Services.basePath : Headers -> Path
Services.basePath headers =
  match getValues "Unison-Cloud-Base-Path" headers with
    [path] ->
      match catch do Path.fromText path with
        Right path -> path
        Left _     -> root
    _      -> root

Services.basePath.doc : Doc
Services.basePath.doc =
  use Path toText
  {{
  Returns the base path for a Unison Cloud service, given the headers of an
  incoming request to that service.

  For a request to a {type ServiceHash}, this will have the form:

  ```
  toText (Path ["h", "BsvWyedHZS29od8ijrYjInauEQTNkQIRHX5iVcOpcsY"])
  ```

  For a request to a {type ServiceName}, this will have the form:

  ```
  toText (Path ["s", "my-echo-service"])
  ```

  For a request to a custom domain that has been connected to a Unison Cloud
  {type ServiceName}, this will be the empty path, since the base path for
  these URIs do not have any path segments:

  ```
  toText root
  ```

  If a valid {type Path} cannot be parsed, this returns the empty path ( {root}
  ). In practice the {type Path} should never fail to parse, so this seems more
  ergonomic than the alternative of forcing the consumer to deal with an
  unexpected error case via {type Exception} or {type Optional}.
  }}

Services.call : ServiceHash a b -> a ->{Services, Remote} b
Services.call service input = Remote.reraise (tryCall service input)

Services.callName : ServiceName a b -> a ->{Services, Remote} b
Services.callName service input = Remote.reraise (tryCallByName service input)

Services.doc : Doc
Services.doc =
  {{
  Operations for calling Unison Cloud services.

  * {Services.call} calls a service by its {type ServiceHash}.
  * {callName} calls a service by its {type ServiceName}.
  * {resolve} resolves a {type ServiceName} to the {type ServiceHash} that it
    currently points to.
  }}

Services.resolve : ServiceName a b ->{Services, Remote} ServiceHash a b
Services.resolve service = Remote.reraise (tryResolve service)

Storage.Batch.awaitRead : Read v ->{Exception, Batch} v
Storage.Batch.awaitRead = cases
  t@(Read.Read _ table k) ->
    match tryAwaitRead t with
      Some v -> v
      None ->
        abortWith
          "Key not found in table" (("table", table), ("key", unsafeExtract k))

Storage.Batch.doc : Doc
Storage.Batch.doc =
  {{
  {type Batch} performs a bulk read from the database, returning the values for
  the keys requested in one round-trip.

  The {type Batch} ability follows a fork-await pattern. Build a batch by
  "forking" the keys you want to read, and then await the result.

  {{ batchSimple }}

  {{
  docCallout
    (Some {{ ⚠️ }})
    {{
    Each result is bound to a value with {awaitRead}, but the __first__ call to
    {awaitRead} is what triggers the batch request to the database.
    }} }}

  The {type Batch} ability can be run non-transactionally against a database
  with {batchRead} or transactionally with {transact}.

  {{ semantics }}

  {{ failures }}
  }}

Storage.Batch.doc.examples.batchAwaitIdempotent : Doc
Storage.Batch.doc.examples.batchAwaitIdempotent =
  use Transaction.write tx
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "table"
    transact db do
      tx table 1 "🍓"
      read1 = forkRead table 1
      result1 = awaitRead read1
      tx table 1 "🍏"
      read2 = forkRead table 1
      result2 = awaitRead read2
      result1' = awaitRead read1
      (result1, result2, result1')
  ```
  }}

Storage.Batch.doc.examples.batchFailure : Doc
Storage.Batch.doc.examples.batchFailure =
  use Storage write
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "myTable"
    write db table "🍋" true
    write db table "🥝" true
    batchRead db do
      read1 = forkRead table "🍋"
      read2 = forkRead table "🥝"
      read3 = forkRead table "🌮"
      (awaitRead read1, awaitRead read2, awaitRead read3)
  ```
  }}

Storage.Batch.doc.examples.batchForkWrite : Doc
Storage.Batch.doc.examples.batchForkWrite =
  use Transaction.write tx
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "table"
    transact db do
      tx table 1 "🍎"
      read = forkRead table 1
      tx table 1 "🍊"
      awaitRead read
  ```
  }}

Storage.Batch.doc.examples.batchOptional : Doc
Storage.Batch.doc.examples.batchOptional =
  use Storage write
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "myTable"
    write db table "🍋" true
    write db table "🥝" true
    batchRead db do
      read1 = forkRead table "🍋"
      read2 = forkRead table "🥝"
      read3 = forkRead table "🍤"
      (tryAwaitRead read1, tryAwaitRead read2, tryAwaitRead read3)
  ```
  }}

Storage.Batch.doc.examples.batchScope : Doc
Storage.Batch.doc.examples.batchScope =
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "table"
    read = transact db do
      Transaction.write.tx table "key" 1
      forkRead table "key"
    transact db do awaitRead read
  ```
  }}

Storage.Batch.doc.examples.batchSimple : Doc
Storage.Batch.doc.examples.batchSimple =
  use Storage write
  use Table Table
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table1 = Table "table1"
    table2 = Table "table2"
    write db table1 1 "🌹"
    write db table1 2 "🪻"
    write db table2 "3" false
    batchRead db do
      read1 : Read Text
      read1 = forkRead table1 1
      read2 : Read Text
      read2 = forkRead table1 2
      read3 : Read Boolean
      read3 = forkRead table2 "3"
      (awaitRead read1, awaitRead read2, awaitRead read3)
  ```
  }}

Storage.Batch.doc.examples.batchTransaction : Doc
Storage.Batch.doc.examples.batchTransaction =
  use Transaction.write tx
  {{
  ```
  Storage.doc.example do
    db = Database (Database.Id.Id "id") "db"
    table = Table.Table "table"
    transact db do
      tx table 1 "🥭"
      tx table 2 "🍇"
      read1 : Read Text
      read1 = forkRead table 1
      read2 : Read Text
      read2 = forkRead table 2
      (awaitRead read1, awaitRead read2)
  ```
  }}

Storage.Batch.doc.failures : Doc
Storage.Batch.doc.failures =
  {{
  # Failures in batched reads

    If a single read in the {batchRead} or {transact} block fails with an
    {type Exception}, the entire batch will fail

    {{ batchFailure }}

    You can handle optional values in batched reads on a per-request basis by
    using {tryAwaitRead} instead of {awaitRead}.

    {{ batchOptional }}
  }}

Storage.Batch.doc.semantics : Doc
Storage.Batch.doc.semantics =
  {{
  # Batched reads semantics

    The first call to {awaitRead} or {tryAwaitRead} after forking multiple
    reads will trigger the database read, so forking a read and immediately
    awaiting it will not be any more efficient than an individual read.

    **Batched reads are scoped**

    you cannot await a {type Read} that was not forked within the same
    enclosing {batchRead} or {transact} block.

    {{ batchScope }}

    **Writes and batched reads in the same transaction**

    If you read and write to the same key in a single database transaction, a
    forked read which __has not been awaited__ will return a value that
    reflects the latest write to that key.

    {{ batchForkWrite }}

    **Read idempotency in transactions**

    In a transaction, once a {type Read} has been awaited, subsequent calls to
    await with the same {type Read} value will return the same value, even if
    the value at the key later changes in the same transaction.

    {{ batchAwaitIdempotent }}
  }}

Storage.batchRead : Database -> '{Exception, Batch} a ->{Exception, Storage} a
Storage.batchRead db p = Either.toException <| tryBatchRead db p

Storage.catchAbort : '{g, Exception} a ->{g, Exception} Optional a
Storage.catchAbort p =
  abortErr = typeLink AbortTransaction
  match catch p with
    Left (Failure typ _ _) | typ === abortErr -> None
    Left err               -> Exception.raise err
    Right a                -> Some a

Storage.delete : Database -> Table k v -> k ->{Remote} ()
Storage.delete db tbl k =
  toRemote do transact db do Transaction.delete.tx tbl k

Storage.delete.tx : Table k v -> k ->{Transaction} ()
Storage.delete.tx = Transaction.delete.tx

Storage.delete.tx.doc : Doc
Storage.delete.tx.doc = {{ Alias for {Transaction.delete.tx}. }}

Storage.doc : Doc
Storage.doc =
  use Database named
  {{
  {{
  docCallout
    None
    {{
    As a general guideline, use {type OrderedTable} rather than raw
    {type Table} to store your data, since {type OrderedTable} also allows
    iteration and range queries.
    }} }}

  {type Storage} is an ability for interacting with the Cloud's durable storage
  resources.

  Its core function is {transact}, which allows you to perform updates to a
  {type Database}:

      @signature{transact}

  The {type Transaction} argument represents the reads and/or updates to be
  performed on the {type Database}, which can contain one or more typed
  {type Table} values:

      @signatures{Transaction.read, Transaction.write.tx, Transaction.delete.tx}

  There's no serialization code to write to or read from a {type Table}, and
  you can store any sort of value in a {type Table}, including functions or
  other {type Table} values!

  Interactions within a {transact} on a {type Database} are transactional, more
  specifically:

  * Reads get a consistent view of all tables accessed. For instance, if you
    read multiple keys, the values read will all be as of the same point in
    time, as if run on a consistent snapshot of the database.
  * If an exception occurs during the transaction block (or if you explicitly
    call {Transaction.abort}), the database will be left in its original state.

  Here's an example of a transaction which reads a value from a table and
  writes a modified value back, aborting if the value doesn't exist.

      @source{readWriteInteraction}

  {{
  docCallout
    (Some {{ 🧐 }})
    {{
    **Limits and performance concerns**

    Individual table entries are currently limited in size to approximately 350
    kilobytes __compressed.__ This will get lifted in the future.

    A single transaction that touches lots of table entries will also tend to
    have slower performance, and may hit transaction size limits. If you hit
    these limits, look for ways to break your workload into multiple smaller
    transactions.
    }} }}

  # Creating databases and tables

    To initialize a database, use `` named "myData" `` with a descriptive name.
    Database creation is idempotent, so the same user can call `` named ``
    multiple times with the same {type Text} argument without worrying about
    creating duplicate databases.

    In order to run your database in the Cloud, you should assign it to a cloud
    {type Environment} with {Database.assign}. Assigning a database to an
    environment is a kind of access control mechanism. Only services which are
    deployed with the same {type Environment} as the {type Database} can access
    the data stored there.

        @source{initializeStorage}

    A {type Table} is a typed key-value store. As mentioned above, they can
    store any sort of key or value type. For instance, you could have a
    {type Table} whose keys are of type {type LocalDate} and whose values are
    of {type Json}.

    Tables are given a {type Text} name when initially bound to a Unison term,
    and the same user can reference `` Table.Table "myTableName" `` multiple
    times in different services to manipulate shared table data, provided the
    services have access to the same {type Database}.

    A {type Table} is extremely lightweight and doesn't need to be created
    ahead of time, though you may wish to declare one or more top-level
    {type Table} definitions, give them type signatures and add these to your
    codebase, for instance:

        @source{dateTable}

  # Batched reads from Storage

    If you are reading multiple values from a {type Database} and want to avoid
    the overhead of database round-trips for each individual read, use the
    {type Batch} ability for bulk reads.

    The Batch ability follows a fork-await pattern. Build a batch by "forking"
    the keys you want to read, and then await the result.

        @signatures{forkRead, awaitRead, tryAwaitRead}

    With this API, you can create a batch that spans tables of different types
    that will only make one bulk request to the database.

    {{ batchSimple }}

    The handler to run the {type Batch} ability non-transactionally is
    {batchRead}.

        @signature{batchRead}

    ## Transactions and batched reads

       Batched reads that interact with other database requests transactionally
       are also supported:

       {{ batchTransaction }}

       The {type Batch} ability is handled in the existing {transact} handler.

       [Read more about the semantics of batched reads in the Batch docs.]({Batch.doc})

  # Building other durable data types

    The building blocks introduced above can also be used for making more
    sophisticated durable data types. For example, see {type OrderedTable} for
    a sorted map with range queries.
  }}

Storage.doc.example :
  '{Exception, Storage, Remote, Random, Scratch} a -> Either Failure a
Storage.doc.example a =
  logic : '{Random, Scope s} Either Failure a
  logic =
    do
      use Hashed toBytes
      use Map get insert
      use Optional getOrElse
      use Scope ref
      use data Map
      use data.Map empty
      use mutable Ref
      use mutable.Ref read
      state : Ref {Scope s} (Map Database (Map (Text, Any) Any))
      state = ref empty
      scratch : Ref {Scope s} (Map Bytes Any)
      scratch = ref empty
      hash : ∀ a. a -> Bytes
      hash = blake2b_256
      rng = rng.scope()
      go : '{Storage, Scratch} x ->{Scope s} x
      go stuff =
        rng do
          handle stuff()
          with cases
            { x } -> x
            { hashKey key -> resume } ->
              hashed = Hashed (Hash (hash key))
              go do resume hashed
            { touch _ -> resume } -> go resume
            { lookupHashed hashed -> resume } ->
              v =
                read scratch
                  |> get (toBytes hashed)
                  |> Optional.map unsafeExtract
              go do resume v
            { Scratch.exists hashed -> resume } ->
              res = read scratch |> Map.contains (toBytes hashed)
              go do resume res
            { saveHashed hashed a -> resume } ->
              modify.deprecated scratch (insert (toBytes hashed) (Any a))
              go do resume()
            { tryTransact db t -> resume } ->
              dbSnapshot = get db (read state) |> getOrElse empty
              match catch do implTransaction dbSnapshot t with
                Left e                 -> go do resume (Left e)
                Right (dbSnapshot', r) ->
                  mutable.Ref.write state (insert db dbSnapshot' (read state))
                  go do resume (Right r)
            { tryBatchRead db p -> resume } ->
              dbSnapshot = get db (read state) |> getOrElse empty
              match catch do implTransaction dbSnapshot p with
                Left e       -> go do resume (Left e)
                Right (_, r) -> go do resume (Right r)
      coerceGo : ('{Storage, Scratch} x ->{Scope s} x) -> '{g} x ->{Scope s} x
      coerceGo f x = coerceAbilities f (coerceAbilities x)
      run.pure (coerceGo go) do (Remote.fork here! do toRemote a) |> await
  splitmix 234823 do Scope.run logic

Storage.doc.example.doc : Doc
Storage.doc.example.doc =
  use OrderedTable.write tx
  {{
  A pure interpreter of {type Remote}, {type Scratch}, {type Storage}, handy
  for examples in documentation and simple testing:

  ```
  Storage.doc.example do
    tbl : OrderedTable Text Text
    tbl = OrderedTable.named exampleDb "favorite-foods" Universal.ordering
    populateTable = transact exampleDb do
      tx tbl "Alice" "🍦"
      tx tbl "Bob" "🍍"
    Remote.parMap (OrderedTable.read tbl) ["Alice", "Bob"]
  ```

  Note that this doesn't handle {type Http} or {type Log}. Also, if using it
  for testing, note that it doesn't do actual parallelism.

  **Also see:**

  * {Cloud.run.local} an actually parallel local runner supporting all the
    abilities of the cloud.
  * {{ docLink (docEmbedTermLink do exampleWithDb) }} if your example code
  }}

test> Storage.doc.example.test = test.verify do
  use OrderedTable.write tx
  rs =
    Either.toException <| (Storage.doc.example do
      tbl : OrderedTable Text Text
      tbl = OrderedTable.named exampleDb "favorite-foods" Universal.ordering
      populateTable = transact exampleDb do
        tx tbl "Alice" "🍦"
        tx tbl "Bob" "🍍"
      Remote.parMap (OrderedTable.read tbl) ["Alice", "Bob"])
  test.ensureEqual rs ["🍦", "🍍"]

Storage.doc.exampleDb : Database
Storage.doc.exampleDb =
  Database (Database.Id.Id "<testing-db-92394028394>") "db"

Storage.doc.exampleDb.doc : Doc
Storage.doc.exampleDb.doc =
  use Storage.doc example
  {{
  A simple example {type Database} useful for local testing:

  ```
  example do
    c = Cell.named exampleDb "number" 42
    Cell.modifyGet c Nat.increment
  ```

  **Also see:** {{ docLink (docEmbedTermLink do example) }} and {{
  docLink (docEmbedTermLink do exampleWithDb) }}
  }}

Storage.doc.examples.dateTable : Table LocalDate Json
Storage.doc.examples.dateTable = Table.Table "dateTable"

Storage.doc.examples.initializeStorage : '{Exception, Cloud} ()
Storage.doc.examples.initializeStorage = do
  database = Database.named "myDatabase"
  environment = Environment.named "environment"
  Database.assign database environment

Storage.doc.examples.readWriteInteraction :
  Database -> '{Exception, Storage} ()
Storage.doc.examples.readWriteInteraction database = do
  table : Table Text Boolean
  table = Table.Table "myTable"
  transact database do
    active = Transaction.read table "Bob"
    Transaction.write.tx table "Bob" (Boolean.not active)

Storage.doc.exampleWithDb :
  (Database ->{Exception, Storage, Remote, Random, Scratch} a)
  -> Either Failure a
Storage.doc.exampleWithDb f =
  splitmix 234823 do Storage.doc.example do f exampleDb

Storage.doc.exampleWithDb.doc : Doc
Storage.doc.exampleWithDb.doc =
  use Nat +
  use Transaction read
  use Transaction.write tx
  {{
  A pure interpreter of {type Remote}, {type Scratch}, {type Storage}, handy
  for examples in documentation and simple testing:

  ```
  logic db = transact db do
    tbl : Table Text Nat
    tbl = Table.Table "counts"
    tx tbl "Alice" 1
    tx tbl "Bob" 1
    read tbl "Alice" + read tbl "Bob"
  exampleWithDb logic
  ```

  Note that this doesn't handle {type Http} or {type Log}. Also, if using it
  for testing, note that it doesn't do actual parallelism.

  **Also see:**

  * {Cloud.run.local} an actually parallel local runner supporting all the
    abilities of the cloud.
  * {{ docLink (docEmbedTermLink do Storage.doc.example) }} if your example
    code makes up its own {type Database} and doesn't need one passed in.
  }}

Storage.modifyGet : Database -> Table k t -> k -> t -> (t -> t) ->{Remote} t
Storage.modifyGet db tbl k default f =
  toRemote do transact db do Storage.modifyGet.tx tbl k default f

Storage.modifyGet.doc : Doc
Storage.modifyGet.doc =
  {{ Like {Storage.modifyGet.tx} but also runs the {type Transaction}. }}

Storage.modifyGet.tx : Table k v -> k -> v -> (v ->{g} v) ->{g, Transaction} v
Storage.modifyGet.tx tbl k default f =
  v0 = Storage.tryRead.tx tbl k |> Optional.getOrElse default
  v = f v0
  Transaction.write.tx tbl k v
  v

Storage.modifyGet.tx.doc : Doc
Storage.modifyGet.tx.doc =
  {{
  `` Storage.modifyGet.tx tbl k default f `` modifies the key `k` of `tbl`
  using the function `f`. If the key is not found, the new result written to
  `k` will be `f default`.

  Returns the new value for the key `k`.

  **Also see:** {Storage.modifyGet} which does the same but runs the
  {type Transaction}.
  }}

Storage.read : Database -> Table k v -> k ->{Remote} v
Storage.read db tbl k = toRemote do transact db do Storage.read.tx tbl k

Storage.read.doc : Doc
Storage.read.doc =
  {{
  Lookup a key in a given {type Table} in a {type Database}, throwing a
  {Transaction.abort} exception if the key is not found.

  * Use {Storage.tryRead} if you'd prefer to return {None} if the key is not
    found.
  * Use {Storage.read.tx} to return the result within a {type Transaction} if
    this is part of a transactional update or a consistent read of multiple
    table keys.
  }}

Storage.readOr.tx : Table k v -> v -> k ->{Transaction} v
Storage.readOr.tx tbl default k = match Storage.tryRead.tx tbl k with
  None   -> default
  Some a -> a

Storage.readOr.tx.doc : Doc
Storage.readOr.tx.doc =
  {{
  `` readOr.tx tbl default key `` reads a key from a {type Table}, returning
  `default` if the key isn't found.

  ```
  logic db = transact db do
    tbl : Table Text Nat
    tbl = Table.Table "tbl"
    Transaction.write.tx tbl "a" 10
    (readOr.tx tbl 99 "c", readOr.tx tbl 0 "a")
  exampleWithDb logic
  ```
  }}

Storage.run.local : '{g, Exception, Storage} a ->{g, Exception, Random} a
Storage.run.local p =
  use Map get
  use Optional getOrElse
  use data Map
  use data.Map empty
  go :
    Map Database (Map (Text, Any) Any)
    -> '{g, Exception, Storage} x
    ->{g, Exception, Random} x
  go dbs p =
    handle p()
    with cases
      { a } -> a
      { tryTransact db t -> resume } ->
        dbSnapshot = get db dbs |> getOrElse empty
        match catch do implTransaction dbSnapshot t with
          Left e -> go dbs do resume (Left e)
          Right (dbSnapshot', r) ->
            go (Map.insert db dbSnapshot' dbs) do resume (Right r)
      { tryBatchRead db p -> resume } ->
        dbSnapshot = get db dbs |> getOrElse empty
        match catch do implTransaction dbSnapshot p with
          Left e       -> go dbs do resume (Left e)
          Right (_, r) -> go dbs do resume (Right r)
  go empty p

Storage.run.local.doc : Doc
Storage.run.local.doc =
  {{
  A simple in-memory interpreter of {type Storage}. Is purely sequential as
  well and has no parallelism. Useful for testing.

  **Also see:**

  * {Storage.doc.example} which is more convenient to use for doc examples.
  * {Cloud.run.local} for local testing of {type Storage} computation with real
    concurrency.
  }}

Storage.run.local.implTransaction :
  data.Map (Text, Any) Any
  -> '{Transaction, Exception, Random, Batch} a
  ->{Exception, Random} (data.Map (Text, Any) Any, a)
Storage.run.local.implTransaction db p =
  use data Map
  go :
    Nat
    -> [Map (Text, Any) Any]
    -> Map Read.Id (Optional Nat)
    -> '{Transaction, Exception, Random, Batch} a
    ->{Exception, Random} (Map (Text, Any) Any, a)
  go t history batchReads p =
    use List :+ unsafeAt
    use Map get insert
    use Nat +
    use Optional map
    use Read Read
    use Table Table
    db = history |> unsafeAt t
    record f = history :+ f db
    t' = t + 1
    handle p()
    with cases
      { a } -> (db, a)
      { Transaction.write.tx (Table table) k v -> resume } ->
        history' = record <| insert (table, Any k) (Any v)
        go t' history' batchReads resume
      { Transaction.delete.tx (Table table) k -> resume } ->
        history' = record <| data.Map.delete (table, Any k)
        go t' history' batchReads resume
      { Transaction.tryRead.tx (Table table) k -> resume } ->
        r = db |> get (table, Any k) |> map unsafeExtract
        go t history batchReads do resume r
      { forkRead (Table table) k -> resume } ->
        readId = Read.Id.Id (Random.bytes 256)
        batchReads' = batchReads |> insert readId None
        read = Read readId table (Any k)
        go t history batchReads' do resume read
      { tryAwaitRead (Read id table k) -> resume } ->
        readAt t = history |> unsafeAt t |> get (table, k) |> map unsafeExtract
        match get id batchReads with
          None ->
            Exception.raise
              (failure
                "Awaiting an invalid Batch.Read, you can only await reads forked in the same call to batchRead or transact"
                id)
          Some (Some t) ->
            r = readAt t
            go t history batchReads do resume r
          Some None ->
            batchReads' = batchReads |> insert id (Some t)
            r = readAt t
            go t history batchReads' do resume r
  split! do go 0 [db] data.Map.empty p

Storage.Table.doc : Doc
Storage.Table.doc =
  {{
  A typed key-value mapping used by the {type Storage} ability. See
  {type Storage}) for more details.

  The {type Table} API is a low-level API primarily used to build other cloud
  storage types, consider {type OrderedTable} for your application logic.
  }}

Storage.Table.name : Table k v -> Text
Storage.Table.name = cases Table.Table name -> name

Storage.transact :
  Database
  -> '{Transaction, Exception, Random, Batch} a
  ->{Exception, Storage} a
Storage.transact storage p = Either.toException (tryTransact storage p)

Storage.transact.random.deprecated :
  Database
  -> '{Transaction, Exception, Random} a
  ->{Exception, Storage, Random} a
Storage.transact.random.deprecated db a =
  rng = Random.split()
  transact db do rng a

Storage.transact.random.deprecated.doc : Doc
Storage.transact.random.deprecated.doc =
  {{
  Like {transact} but allows use of randomness within the {type Transaction}.

  If the transaction is retried, the same random numbers will be generated each
  time.
  }}

Storage.transact.stream :
  Database
  -> '{Transaction, Exception, Random, Batch, Stream a} r
  -> '{Remote, Stream a} r
Storage.transact.stream db in =
  do match toRemote do transact db do Stream.uncons in with
    Left r          -> r
    Right (e, rest) ->
      emit e
      Storage.transact.stream db rest ()

Storage.transact.stream.doc : Doc
Storage.transact.stream.doc =
  {{
  Runs transactions in a {type Stream}, such that each element is emitted in
  its own transaction
  }}

Storage.Transaction.abort : '{Exception} r
Storage.Transaction.abort = do abortWith "aborted transaction" ()

Storage.Transaction.abort.doc : Doc
Storage.Transaction.abort.doc =
  {{
  Raises the canonical {type Failure} to abort a {type Transaction} computation
  }}

Storage.Transaction.abortWith : Text -> a ->{Exception} r
Storage.Transaction.abortWith msg details =
  Exception.raise (Failure (typeLink AbortTransaction) msg (Any details))

Storage.Transaction.abortWith.doc : Doc
Storage.Transaction.abortWith.doc =
  {{ Abort a {type Transaction} with a given message and error details. }}

Storage.Transaction.read : Table k v -> k ->{Transaction, Exception} v
Storage.Transaction.read table k =
  match Transaction.tryRead.tx table k with
    Some v -> v
    None ->
      abortWith
        "key not found in table" (("table", Table.name table), ("key", k))

Storage.Transaction.read.doc : Doc
Storage.Transaction.read.doc =
  {{
  Reads a value from a {type Table} given a key, aborting if there is no value
  for that key.

  **Also see**:

  * @inlineSignature{Transaction.tryRead.tx}
  * @inlineSignature{catchAbort}
  }}

Storage.tryRead : Database -> Table k v -> k ->{Remote} Optional v
Storage.tryRead db tbl k =
  toRemote do transact db do Transaction.tryRead.tx tbl k

Storage.tryRead.doc : Doc
Storage.tryRead.doc =
  {{ Like {Storage.read}, but returns {None} if the key is not found. }}

Storage.tryRead.tx : Table k v -> k ->{Transaction} Optional v
Storage.tryRead.tx = Transaction.tryRead.tx

Storage.tryRead.tx.doc : Doc
Storage.tryRead.tx.doc =
  {{
  An alias for {Transaction.tryRead.tx}. Unlike {Transaction.read} this returns
  {None} if the key is not found in the table.
  }}

Storage.write : Database -> Table k v -> k -> v ->{Remote} ()
Storage.write db tbl k v =
  toRemote do transact db do Transaction.write.tx tbl k v

Storage.write.doc : Doc
Storage.write.doc =
  {{
  Write a key of a {type Table}. If a previous value exists for that key, it
  will be overwritten.

  Immediately after a {Storage.write}, a {Storage.read} or {Transaction.read}
  of the same key will return the value written.

  **Also see:** {Storage.read}, {Transaction.write.tx}
  }}

up.base.crypto.blake2b_256 : a -> Bytes
up.base.crypto.blake2b_256 a = crypto.hash Blake2b_256 a

up.base.crypto.blake2b_256.doc : Doc
up.base.crypto.blake2b_256.doc =
  {{
  Computes the hash of any value, using the {Blake2b_256} hash algorithm.

  ```
  blake2b_256 [42, 43]
  ```

  ```
  blake2b_256 "🌈"
  ```
  }}

up.base.data.List.parMap : (a ->{IO} b) -> [a] ->{IO} [b]
up.base.data.List.parMap f =
  use List map
  map (a -> runForked do f a) >> map concurrent.Promise.read

up.base.IO.concurrent.runForked : '{IO} a ->{IO} concurrent.Promise a
up.base.IO.concurrent.runForked k =
  p = Promise.new()
  unison_base_3_22_0.ignore
    (concurrent.fork do concurrent.Promise.write_ p k())
  p

up.base.IO.randomNat : '{IO} Nat
up.base.IO.randomNat = do match decodeNat64be (IO.randomBytes 8) with
  Some (n, _) -> n
  None        -> bug "requested 8 random bytes, but did not receive enough"

up.base.Nat.floorPowerOf2 : Nat -> Nat
up.base.Nat.floorPowerOf2 n =
  use Nat - ==
  if n == 0 then 0 else Nat.shiftLeft 1 (64 - Nat.leadingZeros n - 1)

test> up.base.Nat.floorPowerOf2.tests = test.verify do
  use test ensureEqual
  Each.repeat 1000
  ensureEqual (floorPowerOf2 0) 0
  ensureEqual (floorPowerOf2 1) 1
  ensureEqual (floorPowerOf2 2) 2
  ensureEqual (floorPowerOf2 3) 2
  ensureEqual (floorPowerOf2 4) 4
  ensureEqual (floorPowerOf2 5) 4
  ensureEqual (floorPowerOf2 16) 16
  ensureEqual (floorPowerOf2 31) 16
  ensureEqual (floorPowerOf2 32) 32

up.base.Ordering.product :
  (a ->{g3} a ->{g2} Ordering)
  -> (b ->{g1} b ->{g} Ordering)
  -> (a, b)
  -> (a, b)
  ->{g3, g2, g1, g} Ordering
up.base.Ordering.product o1 o2 = cases
  (a1, b1), (a2, b2) ->
    match o1 a1 a2 with
      Less    -> Less
      Greater -> Greater
      Equal   -> o2 b1 b2

test> up.base.Ordering.product.tests = test.verify do
  use Random natIn
  use Universal ordering
  Each.repeat 100
  p1 = (natIn 0 5, natIn 0 5)
  p2 = (natIn 0 5, natIn 0 5)
  Ordering.product ordering ordering p1 p2 |> test.ensureEqual (ordering p1 p2)

-- builtin up.base.Pattern.captureAs :
--   a -> lib.unison_base_3_22_0.Pattern a -> lib.unison_base_3_22_0.Pattern a

up.base.Random.rng.io : '{IO, Random} (∀ a g. '{g, Random} a ->{g, IO} a)
up.base.Random.rng.io = do
  ref = IO.ref (RNG split!)
  go : '{g, Random} a ->{g, IO} a
  go thunk =
    rng = ref.atomically ref RNG.split
    RNG.run rng thunk
  go

up.base.Random.rng.mix : RNG -> RNG -> RNG
up.base.Random.rng.mix l r =
  use Bytes ++
  use Nat * + / == xor
  use RNG run split
  use Random bytes nat!
  go : RNG -> RNG -> '{g, Random} a ->{g} a
  go l r p =
    (thisL, nextL) = split l
    (thisR, nextR) = split r
    handle p()
    with cases
      { nat! -> resume } ->
        getNat rng = run rng do nat!
        out = xor (getNat thisL) (getNat thisR)
        go nextL nextR do resume out
      { bytes n -> resume } ->
        k = n / 64
        n' = 64 * (if Nat.mod n 64 == 0 then k else k + 1)
        Bytes.xor out l r = match (decodeNat64be l, decodeNat64be r) with
          (Some (x, l), Some (y, r)) ->
            out' = out ++ encodeNat64be (xor x y)
            Bytes.xor out' l r
          _                          -> out
        getBytes rng = run rng do bytes n'
        out =
          Bytes.xor Bytes.empty (getBytes thisL) (getBytes thisR)
            |> Bytes.take n
        go nextL nextR do resume out
      { split! -> resume } -> go nextL nextR do resume (go thisL thisR)
      { x } -> x
  RNG (go l r)

up.base.Random.rng.mix.tests.bytes : '{IO} [Result]
up.base.Random.rng.mix.tests.bytes = do
  use List map
  use base.IO randomNat
  use test ensureEqual
  a = fromSplitmix randomNat()
  b = fromSplitmix randomNat()
  c = fromSplitmix randomNat()
  d = fromSplitmix randomNat()
  rng = mix (mix a b) (mix c d)
  sizes = [0, 1, 2, 63, 120, 128, 128, 1024, 1050]
  results = RNG.run rng do map Random.bytes sizes
  test.verify do
    ensureEqual (map Bytes.size results) sizes
    ensureEqual (results |> List.distinct |> List.size) (List.size results)

up.base.Random.rng.mix.tests.nat : '{IO} [Result]
up.base.Random.rng.mix.tests.nat =
  do
    use List size
    use Random nat!
    use base.IO randomNat
    a = fromSplitmix randomNat()
    b = fromSplitmix randomNat()
    c = fromSplitmix randomNat()
    d = fromSplitmix randomNat()
    rng = mix (mix a b) (mix c d)
    results = RNG.run rng do [nat!, nat!, nat!, nat!]
    test.verify do
      test.ensureEqual (results |> List.distinct |> size) (size results)

up.base.Random.rng.scope :
  '{Random, Scope s} (∀ a g. '{g, Random} a ->{g, Scope s} a)
up.base.Random.rng.scope = do
  ref = Scope.ref (RNG split!)
  go : '{g, Random} a ->{g, Scope s} a
  go thunk =
    oldRng = mutable.Ref.read ref
    let
      (newState, out) = RNG.split oldRng
      mutable.Ref.write ref newState
      RNG.run out thunk
  go

up.concurrent.Map.foldLeft :
  (a -> b ->{g} a) -> a -> systemfw_concurrent_2_2_0.Map k b ->{g, IO} a
up.concurrent.Map.foldLeft f a = cases
  Map map -> data.Map.foldLeft f a (mutable.Ref.read map)

up.http.HttpRequest.asCurl : HttpRequest ->{Exception} Text
up.http.HttpRequest.asCurl = cases
  HttpRequest m v u (Headers headers) (Body body) ->
    use Text ++
    method = "-X " ++ Method.toText m
    url = "'" ++ URI.toText u ++ "'"
    headers' =
      headers
        |> Map.toList
        |> (List.flatMap cases
             (k, vs) -> List.map (v -> "-H '" ++ k ++ ": " ++ v ++ "'") vs)
        |> Text.join " "
    body' =
      if Bytes.size body === 0 then "" else "--data-binary " ++ fromUtf8 body
    "curl " ++ method ++ " " ++ headers' ++ " " ++ url ++ " " ++ body'

up.json.Decoder.instant : '{Decoder} Instant
up.json.Decoder.instant = do match Instant.fromIso8601 Decoder.text() with
  None         -> Decoder.fail "time not in ISO 8601 format"
  Some instant -> instant

up.json.Decoder.instant.doc : Doc
up.json.Decoder.instant.doc =
  {{
  A {type Decoder} for parsing a JSON string in ISO 8601 format into an
  {type Instant}.
  }}

up.json.Decoder.uri : '{Decoder} URI
up.json.Decoder.uri = do match parseOptional Decoder.text() with
  None     -> Decoder.fail ("invalid uri format: " Text.++ Decoder.text())
  Some uri -> uri

up.json.Decoder.uri.doc : Doc
up.json.Decoder.uri.doc =
  {{
  A {type Decoder} for parsing a JSON string into a {type URI}.

  # Example

    ```
    tryRun Decoder.uri "https://example.com"
    ```
  }}

UserInfo.avatarUrl : UserInfo -> URI
UserInfo.avatarUrl = cases UserInfo.UserInfo avatarUrl _ _ _ -> avatarUrl

UserInfo.avatarUrl.modify : (URI ->{g} URI) -> UserInfo ->{g} UserInfo
UserInfo.avatarUrl.modify f = cases
  UserInfo.UserInfo avatarUrl email handl id ->
    UserInfo.UserInfo (f avatarUrl) email handl id

UserInfo.avatarUrl.set : URI -> UserInfo -> UserInfo
UserInfo.avatarUrl.set avatarUrl1 = cases
  UserInfo.UserInfo _ email handl id ->
    UserInfo.UserInfo avatarUrl1 email handl id

UserInfo.doc : Doc
UserInfo.doc =
  {{
  A {type UserInfo} contains information about a user. It has fields for the
  user's avatar URL, email, handle, and ID. It's used in e.g.
  {type DeploymentInfo}, {type ServiceAssignment}, and {type ServiceNameInfo}.

  # Fields

    * {avatarUrl} - The URL of the user's avatar.
    * {email} - The user's email address.
    * {handl} - The user's handle.
    * {UserInfo.id} - The user's ID.
  }}

UserInfo.email : UserInfo -> Text
UserInfo.email = cases UserInfo.UserInfo _ email _ _ -> email

UserInfo.email.modify : (Text ->{g} Text) -> UserInfo ->{g} UserInfo
UserInfo.email.modify f = cases
  UserInfo.UserInfo avatarUrl email handl id ->
    UserInfo.UserInfo avatarUrl (f email) handl id

UserInfo.email.set : Text -> UserInfo -> UserInfo
UserInfo.email.set email1 = cases
  UserInfo.UserInfo avatarUrl _ handl id ->
    UserInfo.UserInfo avatarUrl email1 handl id

UserInfo.fromJson : '{Decoder} UserInfo
UserInfo.fromJson = do
  use Decoder text
  use object at!
  avatar = at! "avatarUrl" Decoder.uri
  email = at! "email" text
  handl = at! "handle" text
  id = at! "id" text
  UserInfo.UserInfo avatar email handl id

UserInfo.fromJson.doc : Doc
UserInfo.fromJson.doc =
  {{
  A {type Decoder} for parsing JSON string into a {type UserInfo}.

  This decoder expects a JSON object with the following properties:

  * `avatarUrl` - The URL of the user's avatar. A string.
  * `email` - The email address of the user. A string.
  * `handle` - The handle of the user. A string.
  * `id` - The ID of the user. A string.
  }}

UserInfo.handl : UserInfo -> Text
UserInfo.handl = cases UserInfo.UserInfo _ _ handl _ -> handl

UserInfo.handl.modify : (Text ->{g} Text) -> UserInfo ->{g} UserInfo
UserInfo.handl.modify f = cases
  UserInfo.UserInfo avatarUrl email handl id ->
    UserInfo.UserInfo avatarUrl email (f handl) id

UserInfo.handl.set : Text -> UserInfo -> UserInfo
UserInfo.handl.set handl1 = cases
  UserInfo.UserInfo avatarUrl email _ id ->
    UserInfo.UserInfo avatarUrl email handl1 id

UserInfo.id : UserInfo -> Text
UserInfo.id = cases UserInfo.UserInfo _ _ _ id -> id

UserInfo.id.modify : (Text ->{g} Text) -> UserInfo ->{g} UserInfo
UserInfo.id.modify f = cases
  UserInfo.UserInfo avatarUrl email handl id ->
    UserInfo.UserInfo avatarUrl email handl (f id)

UserInfo.id.set : Text -> UserInfo -> UserInfo
UserInfo.id.set id1 = cases
  UserInfo.UserInfo avatarUrl email handl _ ->
    UserInfo.UserInfo avatarUrl email handl id1

websockets.HttpWebSocket.doc : Doc
websockets.HttpWebSocket.doc =
  {{
  An ability for initiating WebSocket client connections within {type Remote}
  }}

websockets.HttpWebSocket.webSocket :
  HttpRequest ->{Remote, websockets.HttpWebSocket} websockets.WebSocket
websockets.HttpWebSocket.webSocket request =
  Remote.reraise (websockets.HttpWebSocket.tryWebSocket request)

websockets.HttpWebSocket.webSocket.doc : Doc
websockets.HttpWebSocket.webSocket.doc =
  {{ Attempt to initiate a WebSocket connection. }}

websockets.WebSocket.doc : Doc
websockets.WebSocket.doc =
  {{
  A {type websockets.WebSocket} represents a web socket connection to the
  cloud. It is used to send and receive messages over the web socket.

  # Operations

    The following operations are available for interacting with the web socket
    using the {type WebSockets} and handle errors using the {type Remote}
    ability:

    * {WebSockets.close} - Close the WebSocket.
    * {WebSockets.send} - Send a message over the WebSocket.
    * {WebSockets.receive} - Receive a message from the WebSocket.

    These operations use the {type WebSockets} ability only, and return a
    {type Failure} if an error occurs:

    * {tryClose} - Try to close the WebSocket
    * {trySend} - Try to send a message over the WebSocket
    * {tryReceive} - Try to receive a message from the WebSocket
  }}

websockets.WebSockets.close : websockets.WebSocket ->{Exception, WebSockets} ()
websockets.WebSockets.close ws = Either.toException (tryClose ws)

websockets.WebSockets.receive :
  websockets.WebSocket ->{Exception, WebSockets} Message
websockets.WebSockets.receive ws = Either.toException (tryReceive ws)

websockets.WebSockets.send :
  websockets.WebSocket -> Message ->{Exception, WebSockets} ()
websockets.WebSockets.send ws msg = Either.toException (trySend ws msg)
